id,year,title,event_type,pdf_name,abstract,paper_text
1,1987,Self-Organization of Associative Database and Its Applications,,1-self-organization-of-associative-database-and-its-applications.pdf,Abstract Missing,"767

SELF-ORGANIZATION OF ASSOCIATIVE DATABASE
AND ITS APPLICATIONS
Hisashi Suzuki and Suguru Arimoto
Osaka University, Toyonaka, Osaka 560, Japan
ABSTRACT
An efficient method of self-organizing associative databases is proposed together with
applications to robot eyesight systems. The proposed databases can associate any input
with some output. In the first half part of discussion, an algorithm of self-organization is
proposed. From an aspect of hardware, it produces a new style of neural network. In the
latter half part, an applicability to handwritten letter recognition and that to an autonomous
mobile robot system are demonstrated.

INTRODUCTION
Let a mapping f : X -+ Y be given. Here, X is a finite or infinite set, and Y is another
finite or infinite set. A learning machine observes any set of pairs (x, y) sampled randomly
from X x Y. (X x Y means the Cartesian product of X and Y.) And, it computes some
estimate j : X -+ Y of f to make small, the estimation error in some measure.
Usually we say that: the faster the decrease of estimation error with increase of the number of samples, the better the learning machine. However, such expression on performance
is incomplete. Since, it lacks consideration on the candidates of J of j assumed preliminarily. Then, how should we find out good learning machines? To clarify this conception,
let us discuss for a while on some types of learning machines. And, let us advance the
understanding of the self-organization of associative database .
. Parameter Type
An ordinary type of learning machine assumes an equation relating x's and y's with
parameters being indefinite, namely, a structure of f. It is equivalent to define implicitly a
set F of candidates of
(F is some subset of mappings from X to Y.) And, it computes
values of the parameters based on the observed samples. We call such type a parameter
type.
For a learning machine defined well, if F 3 f, j approaches f as the number of samples
increases. In the alternative case, however, some estimation error remains eternally. Thus,
a problem of designing a learning machine returns to find out a proper structure of f in this
sense.
On the other hand, the assumed structure of f is demanded to be as compact as possible
to achieve a fast learning. In other words, the number of parameters should be small. Since,
if the parameters are few, some j can be uniquely determined even though the observed
samples are few. However, this demand of being proper contradicts to that of being compact.
Consequently, in the parameter type, the better the compactness of the assumed structure
that is proper, the better the learning machine. This is the most elementary conception
when we design learning machines .

1.

. Universality and Ordinary Neural Networks
Now suppose that a sufficient knowledge on f is given though J itself is unknown. In
this case, it is comparatively easy to find out proper and compact structures of J. In the
alternative case, however, it is sometimes difficult. A possible solution is to give up the
compactness and assume an almighty structure that can cover various 1's. A combination
of some orthogonal bases of the infinite dimension is such a structure. Neural networks 1 ,2
are its approximations obtained by truncating finitely the dimension for implementation.

? American Institute of Physics 1988

768
A main topic in designing neural networks is to establish such desirable structures of 1.
This work includes developing practical procedures that compute values of coefficients from
the observed samples. Such discussions are :flourishing since 1980 while many efficient methods have been proposed. Recently, even hardware units computing coefficients in parallel
for speed-up are sold, e.g., ANZA, Mark III, Odyssey and E-1.
Nevertheless, in neural networks, there always exists a danger of some error remaining
eternally in estimating /. Precisely speaking, suppose that a combination of the bases of a
finite number can define a structure of 1 essentially. In other words, suppose that F 3 /, or
1 is located near F. In such case, the estimation error is none or negligible. However, if 1
is distant from F, the estimation error never becomes negligible. Indeed, many researches
report that the following situation appears when 1 is too complex. Once the estimation
error converges to some value (> 0) as the number of samples increases, it decreases hardly
even though the dimension is heighten. This property sometimes is a considerable defect of
neural networks .
. Recursi ve Type
The recursive type is founded on another methodology of learning that should be as
follows. At the initial stage of no sample, the set Fa (instead of notation F) of candidates
of I equals to the set of all mappings from X to Y. After observing the first sample
(Xl, Yl) E X x Y, Fa is reduced to Fi so that I(xt) = Yl for any I E F. After observing
the second sample (X2' Y2) E X x Y, Fl is further reduced to F2 so that i(xt) = Yl and
I(X2) = Y2 for any I E F. Thus, the candidate set F becomes gradually small as observation
of samples proceeds. The after observing i-samples, which we write
is one of the most
likelihood estimation of 1 selected in fi;. Hence, contrarily to the parameter type, the
recursive type guarantees surely that j approaches to 1 as the number of samples increases.
The recursive type, if observes a sample (x"" yd, rewrites values 1,-l(X),S to I,(x)'s for
some x's correlated to the sample. Hence, this type has an architecture composed of a rule
for rewriting and a free memory space. Such architecture forms naturally a kind of database
that builds up management systems of data in a self-organizing way. However, this database
differs from ordinary ones in the following sense. It does not only record the samples already
observed, but computes some estimation of l(x) for any x E X. We call such database an
associative database.
The first subject in constructing associative databases is how we establish the rule for
rewri ting. For this purpose, we adap t a measure called the dissimilari ty. Here, a dissimilari ty
means a mapping d : X x X -+ {reals > O} such that for any (x, x) E X x X, d(x, x) > 0
whenever l(x) t /(x). However, it is not necessarily defined with a single formula. It is
definable with, for example, a collection of rules written in forms of ""if? .. then?? .. ""
The dissimilarity d defines a structure of 1 locally in X x Y. Hence, even though
the knowledge on f is imperfect, we can re:flect it on d in some heuristic way. Hence,
contrarily to neural networks, it is possible to accelerate the speed of learning by establishing
d well. Especially, we can easily find out simple d's for those l's which process analogically
information like a human. (See the applications in this paper.) And, for such /'s, the
recursive type shows strongly its effectiveness.
We denote a sequence of observed samples by (Xl, Yd, (X2' Y2),???. One of the simplest
constructions of associative databases after observing i-samples (i = 1,2,.,,) is as follows.

i

i""

I,

Algorithm 1. At the initial stage, let So be the empty set. For every i =
1,2"" .. , let i,-l(x) for any x E X equal some y* such that (x*,y*) E S,-l and

d(x, x*) =

min
(%,y)ES.-t

d(x, x) .

Furthermore, add (x"" y,) to S;-l to produce Sa, i.e., S, = S,_l U {(x""

(1)

y,n.

769

Another version improved to economize the memory is as follows.

Algorithm 2, At the initial stage, let So be composed of an arbitrary element
in X x Y. For every i = 1,2"""", let ii-lex) for any x E X equal some y. such
that (x?, y.) E Si-l and
d(x, x?) =

min

d(x, x) .

(i,i)ES.-l

Furthermore, if ii-l(Xi) # Yi then let Si = Si-l, or add (Xi, Yi) to Si-l to
produce Si, i.e., Si = Si-l U {(Xi, Yi)}'
In either construction, ii approaches to f as i increases. However, the computation time
grows proportionally to the size of Si. The second subject in constructing associative
databases is what addressing rule we should employ to economize the computation time. In
the subsequent chapters, a construction of associative database for this purpose is proposed.
It manages data in a form of binary tree.

SELF-ORGANIZATION OF ASSOCIATIVE DATABASE
Given a sample sequence (Xl, Yl), (X2' Y2), .. "" the algorithm for constructing associative
database is as follows.

Algorithm 3,'

Step I(Initialization): Let (x[root], y[root]) = (Xl, Yd. Here, x[.] and y[.] are
variables assigned for respective nodes to memorize data.. Furthermore, let t = 1.
Step 2: Increase t by 1, and put x, in. After reset a pointer n to the root, repeat
the following until n arrives at some terminal node, i.e., leaf.
Notations nand
d(xt, x[n)), let n

n mean the descendant nodes of n.
=n. Otherwise, let n =n.

If d(x"" r[n)) ~

Step 3: Display yIn] as the related information. Next, put y, in. If yIn] = y"" back
to step 2. Otherwise, first establish new descendant nodes n and n. Secondly,
let

(x[n], yIn))
(x[n], yIn))

(x[n], yIn)),
(Xt, y,).

(2)
(3)

Finally, back to step 2. Here, the loop of step 2-3 can be stopped at any time
and also can be continued.
Now, suppose that gate elements, namely, artificial ""synapses"" that play the role of branching by d are prepared. Then, we obtain a new style of neural network with gate elements
being randomly connected by this algorithm.

LETTER RECOGNITION
Recen tly, the vertical slitting method for recognizing typographic English letters3 , the
elastic matching method for recognizing hand written discrete English letters4 , the global
training and fuzzy logic search method for recognizing Chinese characters written in square
styleS, etc. are published. The self-organization of associative database realizes the recognition of handwritten continuous English letters.

770

9 /wn""

NOV

~ ~ ~ -xk :La.t

~~ ~ ~~~

dw1lo'

~~~~~of~~

~~~ 4,-?~~4Fig. 1. Source document.
2~~---------------'

lOO~---------------'

H

o

o
Fig. 2. Windowing.

1000

2000

3000

4000

Number of samples

o

1000

2000

3000

4000

NUAlber of sampl es

Fig. 3. An experiment result.

An image scanner takes a document image (Fig. 1). The letter recognizer uses a parallelogram window that at least can cover the maximal letter (Fig. 2), and processes the
sequence of letters while shifting the window. That is, the recognizer scans a word in a
slant direction. And, it places the window so that its left vicinity may be on the first black
point detected. Then, the window catches a letter and some part of the succeeding letter.
If recognition of the head letter is performed, its end position, namely, the boundary line
between two letters becomes known. Hence, by starting the scanning from this boundary
and repeating the above operations, the recognizer accomplishes recursively the task. Thus
the major problem comes to identifying the head letter in the window.
Considering it, we define the following.
? Regard window images as x's, and define X accordingly.
? For a (x, x) E X x X, denote by B a black point in the left area from the boundary on
window image X. Project each B onto window image x. Then, measure the Euclidean
distance 6 between fj and a black point B on x being the closest to B. Let d(x, x) be
the summation of 6's for all black points B's on x divided by the number of B's.
? Regard couples of the ""reading"" and the position of boundary as y's, and define Y
accordingly.
An operator teaches the recognizer in interaction the relation between window image and
reading& boundary with algorithm 3. Precisely, if the recalled reading is incorrect, the
operator teaches a correct reading via the console. Moreover, if the boundary position is
incorrect, he teaches a correct position via the mouse.
Fig. 1 shows partially a document image used in this experiment. Fig. 3 shows the
change of the number of nodes and that of the recognition rate defined as the relative
frequency of correct answers in the past 1000 trials. Speciiications of the window are height
= 20dot, width = 10dot, and slant angular = 68deg. In this example, the levels of tree
were distributed in 6-19 at time 4000 and the recognition rate converged to about 74%.
Experimentally, the recognition rate converges to about 60-85% in most cases, and to 95% at
a rare case. However, it does not attain 100% since, e.g., ""c"" and ""e"" are not distinguishable
because of excessive lluctuation in writing. If the consistency of the x, y-relation is not
assured like this, the number of nodes increases endlessly (d. Fig. 3). Hence, it is clever to
stop the learning when the recognition rate attains some upper limit. To improve further
the recognition rate, we must consider the spelling of words. It is one of future subjects.

771

OBSTACLE AVOIDING MOVEMENT
Various systems of camera type autonomous mobile robot are reported flourishingly6-1O.
The system made up by the authors (Fig. 4) also belongs to this category. Now, in mathematical methodologies, we solve usually the problem of obstacle avoiding movement as
a cost minimization problem under some cost criterion established artificially. Contrarily,
the self-organization of associative database reproduces faithfully the cost criterion of an
operator. Therefore, motion of the robot after learning becomes very natural.
Now, the length, width and height of the robot are all about O.7m, and the weight is
about 30kg. The visual angle of camera is about 55deg. The robot has the following three
factors of motion. It turns less than ?30deg, advances less than 1m, and controls speed less
than 3km/h. The experiment was done on the passageway of wid th 2.5m inside a building
which the authors' laboratories exist in (Fig. 5). Because of an experimental intention, we
arrange boxes, smoking stands, gas cylinders, stools, handcarts, etc. on the passage way at
random. We let the robot take an image through the camera, recall a similar image, and
trace the route preliminarily recorded on it. For this purpose, we define the following.
? Let the camera face 28deg downward to take an image, and process it through a low
pass filter. Scanning vertically the filtered image from the bottom to the top, search
the first point C where the luminance changes excessively. Then, su bstitu te all points
from the bottom to C for white, and all points from C to the top for black (Fig. 6).
(If no obstacle exists just in front of the robot, the white area shows the ''free'' area
where the robot can move around.) Regard binary 32 x 32dot images processed thus
as x's, and define X accordingly.
? For every (x, x) E X x X, let d(x, x) be the number of black points on the exclusive-or
image between x and X.
? Regard as y's the images obtained by drawing routes on images x's, and define Y
accordingly.
The robot superimposes, on the current camera image x, the route recalled for x, and
inquires the operator instructions. The operator judges subjectively whether the suggested
route is appropriate or not. In the negative answer, he draws a desirable route on x with the
mouse to teach a new y to the robot. This opera.tion defines implicitly a sample sequence
of (x, y) reflecting the cost criterion of the operator.

.::l"" !
-

IibUBe

_. -

22

11

Roan

12

{-

13

Stationary uni t

Fig. 4. Configuration of
autonomous mobile robot system.

~

I

,

23

24

North
14

rmbi Ie unit (robot)

-

Roan

y

t

Fig. 5. Experimental
environment.

772

Wall

Camera image

Preprocessing

A

::: !fa

?

Preprocessing

0

O

Course
suggest ion

??

..

Search

A

Fig. 6. Processing for
obstacle avoiding movement.

x

Fig. 1. Processing for
position identification.
We define the satisfaction rate by the relative frequency of acceptable suggestions of
route in the past 100 trials. In a typical experiment, the change of satisfaction rate showed
a similar tendency to Fig. 3, and it attains about 95% around time 800. Here, notice that
the rest 5% does not mean directly the percentage of collision. (In practice, we prevent the
collision by adopting some supplementary measure.) At time 800, the number of nodes was
145, and the levels of tree were distributed in 6-17.
The proposed method reflects delicately various characters of operator. For example, a
robot trained by an operator 0 moves slowly with enough space against obstacles while one
trained by another operator 0' brushes quickly against obstacles. This fact gives us a hint
on a method of printing ""characters"" into machines.
POSITION IDENTIFICATION
The robot can identify its position by recalling a similar landscape with the position data
to a camera image. For this purpose, in principle, it suffices to regard camera images and
position data as x's and y's, respectively. However, the memory capacity is finite in actual
compu ters. Hence, we cannot but compress the camera images at a slight loss of information.
Such compression is admittable as long as the precision of position identification is in an
acceptable area. Thus, the major problem comes to find out some suitable compression
method.
In the experimental environment (Fig. 5), juts are on the passageway at intervals of
3.6m, and each section between adjacent juts has at most one door. The robot identifies
roughly from a surrounding landscape which section itself places in. And, it uses temporarily
a triangular surveying technique if an exact measure is necessary. To realize the former task,
we define the following .
? Turn the camera to take a panorama image of 360deg. Scanning horizontally the
center line, substitute the points where the luminance excessively changes for black
and the other points for white (Fig. 1). Regard binary 360dot line images processed
thus as x's, and define X accordingly.
? For every (x, x) E X x X, project each black point A on x onto x. And, measure the
Euclidean distance 6 between A and a black point A on x being the closest to A. Let
the summation of 6 be S. Similarly, calculate S by exchanging the roles of x and X.
Denoting the numbers of A's and A's respectively by nand n, define

773

d(x, x) =

~(~
+ ~).
2 n
n

(4)

? Regard positive integers labeled on sections as y's (cf. Fig. 5), and define Y accordingly.
In the learning mode, the robot checks exactly its position with a counter that is reset periodically by the operator. The robot runs arbitrarily on the passageways within 18m area
and learns the relation between landscapes and position data. (Position identification beyond 18m area is achieved by crossing plural databases one another.) This task is automatic
excepting the periodic reset of counter, namely, it is a kind of learning without teacher.
We define the identification rate by the relative frequency of correct recalls of position
data in the past 100 trials. In a typical example, it converged to about 83% around time
400. At time 400, the number of levels was 202, and the levels oftree were distributed in 522. Since the identification failures of 17% can be rejected by considering the trajectory, no
pro blem arises in practical use. In order to improve the identification rate, the compression
ratio of camera images must be loosened. Such possibility depends on improvement of the
hardware in the future.
Fig. 8 shows an example of actual motion of the robot based on the database for obstacle
avoiding movement and that for position identification. This example corresponds to a case
of moving from 14 to 23 in Fig. 5. Here, the time interval per frame is about 40sec.

,~. .~ (
;~""i..
~

""

""

.

..I

I

?
?

""

I'
.
'.1
t

;

i

-:
, . . , 'II

Fig. 8. Actual motion of the robot.

774

CONCLUSION
A method of self-organizing associative databases was proposed with the application to
robot eyesight systems. The machine decomposes a global structure unknown into a set of
local structures known and learns universally any input-output response. This framework
of problem implies a wide application area other than the examples shown in this paper.
A defect of the algorithm 3 of self-organization is that the tree is balanced well only
for a subclass of structures of f. A subject imposed us is to widen the class. A probable
solution is to abolish the addressing rule depending directly on values of d and, instead, to
establish another rule depending on the distribution function of values of d. It is now under
investigation.

REFERENCES
1. Hopfield, J. J. and D. W. Tank, ""Computing with Neural Circuit: A Model/'

Science 233 (1986), pp. 625-633.
2. Rumelhart, D. E. et al., ""Learning Representations by Back-Propagating Errors,"" Nature 323 (1986), pp. 533-536.

3. Hull, J. J., ""Hypothesis Generation in a Computational Model for Visual Word
Recognition,"" IEEE Expert, Fall (1986), pp. 63-70.
4. Kurtzberg, J. M., ""Feature Analysis for Symbol Recognition by Elastic Matching,"" IBM J. Res. Develop. 31-1 (1987), pp. 91-95.

5. Wang, Q. R. and C. Y. Suen, ""Large Tree Classifier with Heuristic Search and
Global Training,"" IEEE Trans. Pattern. Anal. & Mach. Intell. PAMI 9-1
(1987) pp. 91-102.
6. Brooks, R. A. et al, ""Self Calibration of Motion and Stereo Vision for Mobile
Robots,"" 4th Int. Symp. of Robotics Research (1987), pp. 267-276.
7. Goto, Y. and A. Stentz, ""The CMU System for Mobile Robot Navigation,"" 1987
IEEE Int. Conf. on Robotics & Automation (1987), pp. 99-105.
8. Madarasz, R. et al., ""The Design of an Autonomous Vehicle for the Disabled,""
IEEE Jour. of Robotics & Automation RA 2-3 (1986), pp. 117-125.
9. Triendl, E. and D. J. Kriegman, ""Stereo Vision and Navigation within Buildings,"" 1987 IEEE Int. Conf. on Robotics & Automation (1987), pp. 1725-1730.
10. Turk, M. A. et al., ""Video Road-Following for the Autonomous Land Vehicle,""
1987 IEEE Int. Conf. on Robotics & Automation (1987), pp. 273-279.

"
10,1987,A Mean Field Theory of Layer IV of Visual Cortex and Its Application to Artificial Neural Networks,,10-a-mean-field-theory-of-layer-iv-of-visual-cortex-and-its-application-to-artificial-neural-networks.pdf,Abstract Missing,"683

A MEAN FIELD THEORY OF LAYER IV OF VISUAL CORTEX
AND ITS APPLICATION TO ARTIFICIAL NEURAL NETWORKS*
Christopher L. Scofield
Center for Neural Science and Physics Department
Brown University
Providence, Rhode Island 02912
and
Nestor, Inc., 1 Richmond Square, Providence, Rhode Island,
02906.
ABSTRACT
A single cell theory for the development of selectivity and
ocular dominance in visual cortex has been presented previously
by Bienenstock, Cooper and Munrol. This has been extended to a
network applicable to layer IV of visual cortex 2 . In this paper
we present a mean field approximation that captures in a fairly
transparent manner the qualitative, and many of the
quantitative, results of the network theory. Finally, we consider
the application of this theory to artificial neural networks and
show that a significant reduction in architectural complexity is
possible.
A SINGLE LAYER NETWORK AND THE MEAN FIELD
APPROXIMATION
We consider a single layer network of ideal neurons which
receive signals from outside of the layer and from cells within
the layer (Figure 1). The activity of the ith cell in the network is
c'1 -- m'1 d + """"'
~ T .. c'
~J J'

J

(1)

Each cell
d is a vector of afferent signals to the network.
receives input from n fibers outside of the cortical network
through the matrix of synapses mi' Intra-layer input to each cell
is then transmitted through the matrix of cortico-cortical
synapses L.
? American Institute of Physics 1988

684

Afferent
Signals

>

... ..

m2

m1

mn

~

r;.

"",...-

d

.L
:

1

,~

2

... ..

, ...c.. ,

~

~

Figure 1: The general single layer recurrent
network.
Light circles are the LGN -cortical
synapses.
Dark circles are the (nonmodifiable) cortico-cortical synapses.
We now expand the response of the i th cell into individual
terms describing the number of cortical synapses traversed by
the signal d before arriving through synapses Lij at cell i.
Expanding Cj in (1), the response of cell i becomes
ci

=mi d + l: ~j mj d + l: ~jL Ljk mk d + 2: ~j 2Ljk L Lkn mn d +... (2)
J

J

K

J

K' n

Note that each term contains a factor of the form

This factor describes the first order effect, on cell q, of the
cortical transformation of the signal d.
The mean field
approximation consists of estimating this factor to be a constant,
independant of cell location
(3)

685

This assumption does not imply that each cell in the network is
selective to the same pattern, (and thus that mi = mj). Rather,
the assumption is that the vector sum is a constant

This amounts to assuming that each cell in the network is
surrounded by a population of cells which represent, on average,
all possible pattern preferences.
Thus the vector sum of the
afferent synaptic states describing these pattern preferences is a
constant independent of location.
Finally, if we assume that the lateral connection strengths are
a function only of i-j then Lij becomes a circular matrix so that

r. Lij ::: ~J Lji = Lo = constan t.
1

Then the response of the cell i becomes
(4)

for I

~

I <1

where we define the spatial average of cortical cell activity C = in
d, and N is the average number of intracortical synapses.
Here, in a manner similar to that in the theory of magnetism,
we have replaced the effect of individual cortical cells by their
average effect (as though all other cortical cells can be replaced
by an 'effective' cell, figure 2). Note that we have retained all
orders of synaptic traversal of the signal d.
Thus, we now focus on the activity of the layer after
'relaxation' to equilibrium. In the mean field approximation we
can therefore write
(5)

where the mean field

a
with

=am

686

and we asume that
inhibitory).

Afferent
Signals
d

Lo < 0 (the network is,

on

average,

>

Figure 2: The single layer mean field network.
Detailed connectivity between all cells of the
network is replaced with a single (nonmodifiable) synapse from an 'effective' cell.
LEARNING IN THE CORTICAL NETWORK

We will first consider evolution of the network according to a
synaptic modification rule that has been studied in detail, for
single cells, elsewhere!? 3.
We consider the LGN - cortical
synapses to be the site of plasticity and assume for maximum
simplicity that there is no modification of cortico-cortical
synapses. Then
(6)

.

Lij = O.
In what follows c denotes the spatial average over cortical cells,
while Cj denotes the time averaged activity of the i th cortical cell.
The function cj> has been discussed extensively elsewhere.
Here
we note that cj> describes a function of the cell response that has
both hebbian and anti-hebbian regions.

687

This leads to a very complex set of non-linear stochastic
equations that have been analyzed partially elsewhere 2 . In
general, the afferent synaptic state has fixed points that are
stable and selective and unstable fixed points that are nonselective!, 2. These arguments may now be generalized for the
network. In the mean field approximation
(7)

The mean field, a has a time dependent component m. This
varies as the average over all of the network modifiable
synapses and, in most environmental situations, should change
slowly compared to the change of the modifiable synapses to a
single cell. Then in this approximation we can write

?

(mi(a)-a) = cj>[mi(a) - a] d.

(8)

We see that there is a mapping
mi' <-> mica) - a

(9)

such that for every mj(a) there exists a corresponding (mapped)
point mj' which satisfies

the original equation for the mean field zero theory. It can be
shown 2, 4 that for every fixed point of mj( a = 0), there exists a
corresponding fixed point mj( a) with the same selectivity and
stability properties.
The fixed points are available to the
neurons if there is sufficient inhibition in the network (ILo I is
sufficiently large).
APPLICATION OF THE MEAN FIELD NETWORK TO
LAYER IV OF VISUAL CORTEX
Neurons in the primary visual cortex of normal adult cats are
sharply tuned for the orientation of an elongated slit of light and
most are activated by stimulation of either eye. Both of these
properties--orientation selectivity and binocularity--depend on
the type of visual environment experienced during a critical

688

period of early postnatal development. For example, deprivation
of patterned input during this critical period leads to loss of
orientation selectivity while monocular deprivation (MD) results
in a dramatic shift in the ocular dominance of cortical neurons
such that most will be responsive exclusively to the open eye.
The ocular dominance shift after MD is the best known and most
intensively studied type of visual cortical plasticity.
The behavior of visual cortical cells in various rearing
conditions suggests that some cells respond more rapidly to
environmental changes than others.
In monocular deprivation,
for example, some cells remain responsive to the closed eye in
spite of the very large shift of most cells to the open eye- Singer
et. al. 5 found, using intracellular recording, that geniculo-cortical
synapses on inhibitory interneurons are more resistant to
monocular deprivation than are synapses on pyramidal cell
dendrites. Recent work suggests that the density of inhibitory
GABAergic synapses in kitten striate cortex is also unaffected by
MD during the cortical period 6, 7.
These results suggest that some LGN -cortical synapses modify
rapidly, while others modify relatively slowly, with slow
modification of some cortico-cortical synapses. Excitatory LGNcortical synapses into excitatory cells may be those that modify
primarily.
To embody these facts we introduce two types of
LGN -cortical synapses:
those (mj) that modify and those (Zk)
that remain relatively constant. In a simple limit we have

and

(10)

We assume for simplicity and consistent with the above
physiological interpretation that these two types of synapses are
confined to two different classes of cells and that both left and
right eye have similar synapses (both m i or both Zk) on a given
cell. Then, for binocular cells, in the mean field approximation
(where binocular terms are in italics)

689

where dl(r) are the explicit left (right) eye time averaged signals
arriving form the LGN.
Note that a1(r) contain terms from
modifiable and non-modifiable synapses:
al(r) =

a (ml(r) + zl(r?).

Under conditions of monocular deprivation, the animal is reared
with one eye closed. For the sake of analysis assume that the
right eye is closed and that only noise-like signals arrive at
cortex from the right eye. Then the environment of the cortical
cells is:
d = (di, n)

(12)

Further, assume that the left eye synapses have reached their
1

r

selective fixed point, selective to pattern d 1 ? Then (mi' m i )
(m:*, xi) with IXil ?lm!*1.
linear analysis of the
the closed eye

<I> -

=

Following the methods of BCM, a local
function is employed to show that for

Xi =

a (1 - }..a)-li.r.

(13)

where A. = NmIN is the ratio of the number modifiable cells to the
total number of cells in the network. That is, the asymptotic
state of the closed eye synapses is a scaled function of the meanfield due to non-modifiable (inhibitory) cortical cells. The scale
of this state is set not only by the proportion of non-modifiable
cells, but in addition, by the averaged intracortical synaptic
strength Lo.
Thus contrasted with the mean field zero theory the deprived
eye LGN-cortical synapses do not go to zero.
Rather they
approach the constant value dependent on the average inhibition
produced by the non-modifiable cells in such a way that the
asymptotic output of the cortical cell is zero (it cannot be driven
by the deprived eye). However lessening the effect of inhibitory
synapses (e.g. by application of an inhibitory blocking agent such
as bicuculine) reduces the magnitude of a so that one could once
more obtain a response from the deprived eye.

690

We find, consistent with previous theory and experiment,
that most learning can occur in the LGN-cortical synapse, for
inhibitory (cortico-cortical) synapses need not modify.
Some
non-modifiable LGN-cortical synapses are required.
THE MEAN FIELD APPROXIMATION AND
ARTIFICIAL NEURAL NETWORKS
The mean field approximation may be applied to networks in
which the cortico-cortical feedback is a general function of cell
activity. In particular, the feedback may measure the difference
between the network activity and memories of network activity.
In this way, a network may be used as a content addressable
memory.
We have been discussing the properties of a mean
field network after equilibrium has been reached. We now focus
on the detailed time dependence of the relaxation of the cell
activity to a state of equilibrium.
Hopfield8 introduced a simple formalism for the analysis of
the time dependence of network activity.
In this model,
network activity is mapped onto a physical system in which the
state of neuron activity is considered as a 'particle' on a potential
energy surface.
Identification of the pattern occurs when the
activity 'relaxes' to a nearby minima of the energy.
Thus
mlmma are employed as the sites of memories. For a Hopfield
network of N neurons, the intra-layer connectivity required is of
order N2. This connectivity is a significant constraint on the
practical implementation of such systems for large scale
problems. Further, the Hopfield model allows a storage capacity
which is limited to m < N memories 8, 9. This is a result of the
proliferation of unwanted local minima in the 'energy' surface.
Recently, Bachmann et al. l 0, have proposed a model for the
relaxation of network activity in which memories of activity
patterns are the sites of negative 'charges', and the activity
caused by a test pattern is a positive test 'charge'. Then in this
model, the energy function is the electrostatic energy of the
(unit) test charge with the collection of charges at the memory
sites

E = -IlL ~ Qj I J-l- Xj I - L,
J

(14)

691

where Jl (0) is a vector describing the initial network activity
caused by a test pattern, and Xj' the site of the jth memory. L is
a parameter related to the network size.
This model has the advantage that storage density is not
restricted by the the network size as it is in the Hopfield model,
and in addition, the architecture employs a connectivity of order
m x N.
Note that at each stage in the settling of Jl (t) to a memory
(of network activity) Xj' the only feedback from the network to
each cell is the scalar
~

J

Q. I Jl- X? I - L
J

J

(15)

This quantity is an integrated measure of the distance of the
current network state from stored memories.
Importantly, this
measure is the same for all cells; it is as if a single virtual cell
was computing the distance in activity space between the
current state and stored states. The result of the computation is
This is a
then broadcast to all of the cells in the network.
generalization of the idea that the detailed activity of each cell in
the network need not be fed back to each cell.
Rather some
global measure, performed by a single 'effective' cell is all that is
sufficient in the feedback.
DISCUSSION

We have been discussing a formalism for the analysis of
networks of ideal neurons based on a mean field approximation
of the detailed activity of the cells in the network. We find that
a simple assumption concerning the spatial distribution of the
pattern preferences of the cells allows a great simplification of
the analysis. In particular, the detailed activity of the cells of
the network may be replaced with a mean field that in effect is
computed by a single 'effective' cell.
Further, the application of this formalism to the cortical layer
IV of visual cortex allows the prediction that much of learning in
cortex may be localized to the LGN-cortical synaptic states, and
that cortico-cortical plasticity is relatively unimportant. We find,
in agreement with experiment, that monocular deprivation of
the cortical cells will drive closed-eye responses to zero, but
chemical blockage of the cortical inhibitory pathways would
reveal non-zero closed-eye synaptic states.

692

Finally, the mean field approximation allows the development
of single layer models of memory storage that are unrestricted
in storage density, but require a connectivity of order mxN. This
is significant for the fabrication of practical content addressable
memories.
ACKNOWLEOOEMENTS
I would like to thank Leon Cooper for many helpful discussions
and the contributions he made to this work.

*This work was supported by the Office of Naval Research and
the Army Research Office under contracts #NOOOI4-86-K-0041
and #DAAG-29-84-K-0202.

REFERENCES
[1] Bienenstock, E. L., Cooper, L. N & Munro, P. W. (1982) 1.
Neuroscience 2, 32-48.
[2] Scofield, C. L. (I984) Unpublished Dissertation.
[3] Cooper, L. N, Munro, P. W. & Scofield, C. L. (1985) in Synaptic
Modification, Neuron Selectivity and Nervous System
Organization, ed. C. Levy, J. A. Anderson & S. Lehmkuhle,
(Erlbaum Assoc., N. J.).
[4] Cooper, L. N & Scofield, C. L. (to be published) Proc. Natl. Acad.
Sci. USA ..
[5] Singer, W. (1977) Brain Res. 134, 508-000.
[6] Bear, M. F., Schmechel D. M., & Ebner, F. F. (1985) 1. Neurosci.
5, 1262-0000.
[7] Mower, G. D., White, W. F., & Rustad, R. (1986) Brain Res. 380,
253-000.
[8] Hopfield, J. J. (1982) Proc. Natl. A cad. Sci. USA 79, 2554-2558.
[9] Hopfield, J. J., Feinstein, D. 1., & Palmer, R. O. (1983) Nature
304, 158-159.
[10] Bachmann, C. M., Cooper, L. N, Dembo, A. & Zeitouni, O. (to be
published) Proc. Natl. Acad. Sci. USA.

"
100,1988,Storing Covariance by the Associative Long-Term Potentiation and Depression of Synaptic Strengths in the Hippocampus,,100-storing-covariance-by-the-associative-long-term-potentiation-and-depression-of-synaptic-strengths-in-the-hippocampus.pdf,Abstract Missing,"394

STORING COVARIANCE BY THE ASSOCIATIVE
LONG?TERM POTENTIATION AND DEPRESSION
OF SYNAPTIC STRENGTHS IN THE HIPPOCAMPUS
Patric K. Stanton? and Terrence J. Sejnowski t
Department of Biophysics
Johns Hopkins University
Baltimore, MD 21218
ABSTRACT

In modeling studies or memory based on neural networks, both the selective
enhancement and depression or synaptic strengths are required ror effident storage
or inrormation (Sejnowski, 1977a,b; Kohonen, 1984; Bienenstock et aI, 1982;
Sejnowski and Tesauro, 1989). We have tested this assumption in the hippocampus,
a cortical structure or the brain that is involved in long-term memory. A brier,
high-frequency activation or excitatory synapses in the hippocampus produces an
increase in synaptic strength known as long-term potentiation, or LTP (BUss and
Lomo, 1973), that can last ror many days. LTP is known to be Hebbian since it
requires the simultaneous release or neurotransmitter from presynaptic terminals
coupled with postsynaptic depolarization (Kelso et al, 1986; Malinow and Miller,
1986; Gustatrson et al, 1987). However, a mechanism ror the persistent reduction or
synaptic strength that could balance LTP has not yet been demonstrated. We studied the associative interactions between separate inputs onto the same dendritic
trees or hippocampal pyramidal cells or field CAl, and round that a low-frequency
input which, by itselr, does not persistently change synaptic strength, can either
increase (associative LTP) or decrease in strength (associative long-term depression
or LTD) depending upon whether it is positively or negatively correlated in time
with a second, high-frequency bursting input. LTP or synaptic strength is Hebbian,
and LTD is anti-Hebbian since it is elicited by pairing presynaptic firing with postsynaptic hyperpolarization sufficient to block postsynaptic activity. Thus, associative LTP and associative LTO are capable or storing inrormation contained in the
covariance between separate, converging hippocampal inputs?

?Present address: Dep~ents of NeW'Oscience and Neurology, Albert Einstein College
of Medicine, 1410 Pelham Parkway South, Bronx, NY 10461 USA.
tPresent address: Computational Neurobiology Laboratory, The Salk Institute, P.O. Box
85800, San Diego, CA 92138 USA.

Storing Covariance by Synaptic Strengths in the Hippocampus

INTRODUCTION
Associative LTP can be produced in some hippocampal neuroos when lowfrequency. (Weak) and high-frequency (Strong) inputs to the same cells are simultaneously activated (Levy and Steward, 1979; Levy and Steward, 1983; Barrionuevo and
Brown, 1983). When stimulated alone, a weak input does not have a long-lasting effect
on synaptic strength; however, when paired with stimulation of a separate strong input
sufficient to produce homo synaptic LTP of that pathway, the weak pathway is associatively potentiated. Neural network modeling studies have predicted that, in addition to
this Hebbian form of plasticity, synaptic strength should be weakened when weak and
strong inputs are anti-correlated (Sejnowski, 1977a,b; Kohonen, 1984; Bienenstock et al,
1982; Sejnowski and Tesauro, 1989). Evidence for heterosynaptic depression in the hippocampus has been found for inputs that are inactive (Levy and Steward, 1979; Lynch et
al, 1977) or weakly active (Levy and Steward, 1983) during the stimulation of a strong
input, but this depression did not depend on any pattern of weak input activity and was
not typically as long-lasting as LTP.
Therefore, we searched for conditions under which stimulation of a hippocampal
pathway, rather than its inactivity, could produce either long-term depression or potentiation of synaptic strengths, depending on the pattern of stimulation. The stimulus paradigm that we used, illustrated in Fig. I, is based on the finding that bursts of stimuli at 5
Hz are optimal in eliciting LTP in the hippocampus (Larson and Lynch, 1986). A highfrequency burst (S'IRONG) stimulus was applied to Schaffer collateral axons and a lowfrequency (WEAK) stimulus given to a separate subicular input coming from the opposite side of the recording site, but terminating on dendrites of the same population of CAl
pyramidal neurons. Due to the rhythmic nature of the strong input bursts, each weak
input shock could be either superimposed on the middle of each burst of the strong input
(IN PHASE), or placed symmetrically between bursts (OUT OF PHASE).

RESULTS
Extracellular evoked field potentials were recorded from the apical dendritic and
somatic layers of CAl pyramidal cells. The weak stimulus train was first applied alone
and did not itself induce long-lasting changes. The strong site was then stimulated alone,
which elicited homosynaptic LTP of the strong pathway but did not significantly alter
amplitude of responses to the weak input. When weak and strong inputs were activated
IN PHASE, there was an associative LTP of the weak input synapses, as shown in Fig.
2a. Both the synaptic excitatory post-synaptic potential (e.p.s.p.) (Ae.p.s.p. = +49.8 ?
7.8%, n=20) and population action potential (&Pike = +65.4 ? 16.0%, n=14) were
significantly enhanced for at least 60 min up to 180 min following stimulation.
In contrast, when weak and strong inputs were applied OUT OF PHASE, they elicited an associative long-term depression (LTO) of the weak input synapses, as shown in
Fig. 2b. There was a marked reduction in the population spike (-46.5 ? 11.4%, n=10)
with smaller decreases in the e.p.s.p. (-13.8 ? 3.5%, n=13). Note that the stimulus patterns applied to each input were identical in these two experiments, and only the relative

395

396

Stanton and Sejnowski

phase of the weak and strong stimuli was altered. With these stimulus patterns. synaptic
strength could be repeatedly enhanced and depressed in a single slice. as illustrated in Fig
2c. As a control experiment to determine whether information concerning covariance
between the inputs was actually a determinant of plasticity. we combined the in phase
and out of phase conditions, giving both the weak input shocks superimposed on the
bursts plus those between the bursts. for a net frequency of 10 Hz. This pattern. which
resulted in zero covariance between weak and strong inputs. produced no net change in
weak input synaptic strength measmed by extracellular evoked potentials. Thus. the assoa

b
A.SSOCIA.TIVE STIMULUS PA.RA.DIGMS
POSJTIVE.LY CORKELA TED ? ""IN PHASE""

~K~~ _I~__~I____~I____~I_
SI1IONG,NJO\IT

. u.Jj1l 11l. -1---1&1111.....
11 ---1&1
111.....
11 ---,I~IIII

NEGATIVELY CORRELATED? 'our OF PHASE""
W[AKIN'lTf

STIONG 'N''''

~I

11111

--,-;

11111

11111

Figure 1. Hippocampal slice preparation and stimulus paradigms. a: The in vitro hippocampal slice showing recording sites in CAl pyramidal cell somatic (stratum pyramidale) and dendritic (stratum radiatum) layers. and stimulus sites activating Schaffer collateral (STRONG) and commissural (WEAK) afferents. Hippocampal slices (400 Jlm
thick) were incubated in an interface slice chamber at 34-35 0 C. Extracellular (1-5 M!l
resistance, 2M NaCI filled) and intracellular (70-120 M 2M K-acetate filled) recording electrodes. and bipolar glass-insulated platinum wire stimulating electrodes (50 Jlm
tip diameter). were prepared by standard methods (Mody et al, 1988). b: Stimulus paradigms used. Strong input stimuli (STRONG INPUT) were four trains of 100 Hz bursts.
Each burst had 5 stimuli and the interburst interval was 200 msec. Each train lasted 2
seconds for a total of 50 stimuli. Weak input stimuli (WEAK INPUT) were four trains of
shocks at 5 Hz frequency. each train lasting for 2 seconds. When these inputs were IN
PHASE. the weak single shocks were superimposed on the middle of each burst of the
strong input. When the weak input was OUT OF PHASE. the single shocks were placed
symmetrically between the bursts.

n.

Storing Covariance by Synaptic Strengths in the Hippocampus

ciative LTP and LTD mechanisms appear to be balanced in a manner ideal for the
storage of temporal covariance relations.
The simultaneous depolarization of the postsynaptic membrane and activation of
glutamate receptors of the N-methyl-D-aspartate (NMDA) subtype appears to be necessary for LTP induction (Collingridge et ai, 1983; Harris et al, 1984; Wigstrom and Gustaffson, 1984). The SJ?read of current from strong to weak synapses in the dendritic tree,
d

ASSOCIATIVE

LON(;.TE~

I'OTENTIATION

LONG-TE~

DE,/tESSION

-

!!Ll!!!!.

b

ASSOCIATIVE

I

11111

?

11111.
I

c

e...

I

I

I

I

Figure 2. mustration of associative long-term potentiation (LTP) and associative longterm depression (LTD) using extracellular recordings. a: Associative LTP of evoked
excitatory postsynaptic potentials (e.p.s.p.'s) and population action potential responses in
the weak inpuL Test responses are shown before (Pre) and 30 min after (post) application of weak stimuli in phase with the coactive strong input. b: Associative LTD of
evoked e.p.s.p.'s and population spike responses in the weak input. Test responses are
shown before (Pre) and 30 min after (post) application of weak stimuli out of phase with
the coactive strong input. c: Time course of the changes in population spike amplitude
observed at each input for a typical experiment. Test responses from the strong input (S,
open circles), show that the high-frequency bursts (5 pulses/l00 Hz, 200 msec interburst
interval as in Fig. 1) elicited synapse-specific LTP independent of other input activity.
Test responses from the weak input (W. filled circles) show that stimulation of the weak
pathway out of phase with the strong one produced associative LTD (Assoc LTD) of this
input. Associative LTP (Assoc LTP) of the same pathway was then elicited following in
phase stimulation. Amplitude and duration of associative LTD or LTP could be increased
by stimulating input pathways with more trains of shocks.

397

398

Stanton and Sejnowski

coupled with release of glutamate from the weak inputs, could account for the ability of
the strong pathway to associatively potentiate a weak one (Kelso et al, 1986; Malinow
and Miller, 1986; Gustaffson et al, 1987). Consistent with this hypothesis, we find that
the NMDA receptor antagonist 2-amino-S-phosphonovaleric acid (APS, 10 J.1M) blocks
induction of associative LTP in CAl pyramidal neurons (data not shown, n=S). In contrast, the application of APS to the bathing solution at this same concentration had no
significant effect on associative LTD (data not shown, n=6). Thus, the induction of LTD
seems to involve cellular mechanisms different from associative LTP.
The conditions necessary for LTD induction were explored in another series of
experiments using intracellular recordings from CAl pyramidal neurons made using
standard techniques (Mody et al, 1988). Induction of associative LTP (Fig 3; WEAK
S+W IN PHASE) produced an increase in amplitude of the single cell evoked e.p.s.p. and
a lowered action potential threshold in the weak pathway, as reported previously (Barrionuevo and Brown, 1983). Conversely, the induction of associative LTD (Fig. 3;
WEAK S+W OUT OF PHASE) was accompanied by a long-lasting reduction of e.p.s.p.
amplitude and reduced ability to elicit action potential firing. As in control extracellular
experiments, the weak input alone produced no long-lasting alterations in intracellular
e.p.s.p.'s or firing properties, while the strong input alone yielded specific increases of
the strong pathway e.p.s.p. without altering e.p.s.p. 's elicited by weak input stimulation.

PRE

30 min POST
S+W OUT OF PHASE

30 min POST
S+W IN PHASE

Figure 3. Demonstration of associative LTP and LTD using intracellular recordings from
a CAl pyramidal neuron. Intracellular e.p.s.p.'s prior to repetitive stimulation (pre), 30
min after out of phase stimulation (S+W OUT OF PHASE), and 30 min after subsequent in phase stimuli (S+W IN PHASE). The strong input (Schaffer collateral side,
lower traces) exhibited LTP of the evoked e.p.s.p. independent of weak input activity.
Out of phase stimulation of the weak (Subicular side, upper traces) pathway produced a
marked, persistent reduction in e.p.s.p. amplitude. In the same cell, subsequent in phase
stimuli resulted in associative LTP of the weak input that reversed the LTD and enhanced
amplitude of the e.p.s.p. past the original baseline. (RMP = -62 mY, RN = 30 MO)

Storing Covariance by Synaptic Strengths in the Hippocampus

A weak stimulus that is out of phase with a strong one anives when the postsynaptic neuron is hyperpolarized as a consequence of inhibitory postsynaptic potentials and
afterhyperpolarization from mechanisms intrinsic to pyramidal neurons. This suggests
that postsynaptic hyperpolarization coupled with presynaptic activation may trigger L'ID.
To test this hypothesis, we injected current with intracellular microelectrodes to hyperpolarize or depolarize the cell while stimulating a synaptic input. Pairing the injection of
depolarizing current with the weak input led to LTP of those synapses (Fig. 4a; STIM;

a

PRE

? ?IDPOST
S'I1M ? DEPOL

~l""V
lS.,.c

r
,"" i

COI'ITROL

-Jj

b

I

--"" \

""----

(W.c:ULVllj

PRE

lOlIIin POST
STlM ? HYPERPOL

Figure 4. Pairing of postsynaptic hyperpolarization with stimulation of synapses on CAl
hippocampal pyramidal neurons produces L'ID specific to the activated pathway, while
pairing of postsynaptic depolarization with synaptic stimulation produces synapsespecific LTP. a: Intracellular evoked e.p.s.p.'s are shown at stimulated (STIM) and
unstimulated (CONTROL) pathway synapses before (Pre) and 30 min after (post) pairing a 20 mY depolarization (constant current +2.0 nA) with 5 Hz synaptic stimulation.
The stimulated pathway exhibited associative LTP of the e.p.s.p., while the control,
unstimulated input showed no change in synaptic strength. (RMP = -65 mY; RN = 35
Mfl) b: Intracellular e.p.s.p. 's are shown evoked at stimulated and control pathway
synapses before (Pre) and 30 min after (post) pairing a 20 mV hyperpolarization (constant current -1.0 nA) with 5 Hz synaptic stimulation. The input (STIM) activated during
the hyperpolarization showed associative LTD of synaptic evoked e.p.s.p.'s, while
synaptic strength of the silent input (CONTROL) was unaltered. (RMP =-62 mV; RN =
38M!l)

399

400

Stanton and Sejnowski

+64.0 -9.7%, n=4), while a control input inactive during the stimulation did not change
(CONTROL), as reported previously (Kelso et al, 1986; Malinow and Miller, 1986; Gustaffson et al, 1987). Conversely, prolonged hyperpolarizing current injection paired with
the same low-frequency stimuli led to induction of LTD in the stimulated pathway (Fig.
4b; STIM; -40.3 ? 6.3%, n=6). but not in the unstimulated pathway (CONTROL). The
application of either depolarizing current, hyperpolarizing current, or the weak 5 Hz
synaptic stimulation alone did not induce long-term alterations in synaptic strengths.
Thus. hyperpolarization and simultaneous presynaptic activity supply sufficient conditions for the induction of LTD in CAl pyramidal neurons.

CONCLUSIONS
These experiments identify a novel fono of anti-Hebbian synaptic plasticity in the
hippocampus and confirm predictions made from modeling studies of information storage
in neural networks. Unlike previous reports of synaptic depression in the hippocampus,
the plasticity is associative, long-lasting, and is produced when presynaptic activity
occurs while the postsynaptic membrane is hyperpolarized. In combination with Hebbian
mechanisms also present at hippocampal synapses. associative LTP and associative LTD
may allow neurons in the hippocampus to compute and store covariance between inputs
(Sejnowski, 1977a,b; Stanton and Sejnowski. 1989). These finding make temporal as
well as spatial context an important feature of memory mechanisms in the hippocampus.
Elsewhere in the brain, the receptive field properties of cells in cat visual cortex
can be altered by visual experience paired with iontophoretic excitation or depression of
cellular activity (Fregnac et al, 1988; Greuel et al, 1988). In particular, the chronic hyperpolarization of neurons in visual cortex coupled with presynaptic transmitter release leads
to a long-teno depression of the active. but not inactive, inputs from the lateral geniculate
nucleus (Reiter and Stryker, 1988). Thus. both Hebbian and anti-Hebbian mechanisms
found in the hippocampus seem to also be present in other brain areas, and covariance of
firing patterns between converging inputs a likely key to understanding higher cognitive
function.
This research was supported by grants from the National Science Foundation and
the Office of Naval research to TJS. We thank Drs. Charles Stevens and Richard Morris
for discussions about related experiments.

Rererences
Bienenstock, E., Cooper. LN. and Munro. P. Theory for the development of neuron
selectivity: orientation specificity and binocular interaction in visual cortex. J. Neurosci. 2. 32-48 (1982).
Barrionuevo, G. and Brown, T.H. Associative long-teno potentiation in hippocampal
slices. Proc. Nat. Acad. Sci. (USA) 80, 7347-7351 (1983).
Bliss. T.V.P. and Lomo, T. Long-lasting potentiation of synaptic ttansmission in the dentate area of the anaesthetized rabbit following stimulation of the perforant path. J.
Physiol. (Lond.) 232. 331-356 (1973).

Storing Covariance by Synaptic Strengths in the Hippocampus

Collingridge, GL., Kehl, SJ. and McLennan, H. Excitatory amino acids in synaptic
transmission in the Schaffer collateral-commissural pathway of the rat hippocampus. J.
Physiol. (Lond.) 334, 33-46 (1983).
Fregnac, Y., Shulz, D., Thorpe, S. and Bienenstock, E. A cellular analogue of visual cortical plasticity. Nature (Lond.) 333, 367-370 (1988).
Greuel. J.M.. Luhmann. H.J. and Singer. W. Pharmacological induction of usedependent receptive field modifications in visual cortex. Science 242,74-77 (1988).
Gustafsson, B., Wigstrom, H., Abraham, W.C. and Huang. Y.Y. Long-term potentiation
in the hippocampus using depolarizing current pulses as the conditioning stimulus to
single volley synaptic potentials. J. Neurosci. 7, 774-780 (1987).
Harris. E.W., Ganong, A.H. and Cotman, C.W. Long-term potentiation in the hippocampus involves activation of N-metbyl-D-aspartate receptors. Brain Res. 323, 132137 (1984).
Kelso, S.R.. Ganong, A.H. and Brown, T.H. Hebbian synapses in hippocampus. Proc.
Natl. Acad. Sci. USA 83, 5326-5330 (1986).
Kohonen. T. Self-Organization and Associative Memory. (Springer-Verlag. Heidelberg,
1984).
Larson. J. and Lynch. G. Synaptic potentiation in hippocampus by patterned stimulation
involves two events. Science 232, 985-988 (1986).
Levy. W.B. and Steward, O. Synapses as associative memory elements in the hippocampal formation. Brain Res. 175,233-245 (1979).
Levy. W.B. and Steward, O. Temporal contiguity requirements for long-term associative
potentiation/depression in the hippocampus. Neuroscience 8, 791-797 (1983).
Lynch. G.S., Dunwiddie. T. and Gribkoff. V. Heterosynaptic depression: a postsynaptic
correlate oflong-term potentiation. Nature (Lond.) 266. 737-739 (1977).
Malinow. R. and Miller, J.P. Postsynaptic hyperpolarization during conditioning reversibly blocks induction of long-term potentiation Nature (Lond.)32.0. 529-530 (1986).
Mody. I.. Stanton. PK. and Heinemann. U. Activation of N-methyl-D-aspartate
(NMDA) receptors parallels changes in cellular and synaptic properties of dentate
gyrus granule cells after kindling. J. Neurophysiol. 59. 1033-1054 (1988).
Reiter, H.O. and Stryker, M.P. Neural plasticity without postsynaptic action potentials:
Less-active inputs become dominant when kitten visual cortical cells are pharmacologically inhibited. Proc. Natl. Acad. Sci. USA 85, 3623-3627 (1988).
Sejnowski, T J. and Tesauro, G. Building network learning algorithms from Hebbian
synapses, in: Brain Organization and Memory JL. McGaugh, N.M. Weinberger, and
G. Lynch, Eds. (Oxford Univ. Press, New York, in press).
Sejnowski, TJ. Storing covariance with nonlinearly interacting neurons. J. Math. Biology 4, 303-321 (1977).
Sejnowski, T. J. Statistical constraints on synaptic plasticity. J. Theor. Biology 69, 385389 (1977).
Stanton, P.K. and Sejnowski, TJ. Associative long-term depression in the hippocampus:
Evidence for anti-Hebbian synaptic plasticity. Nature (Lond.), in review.
Wigstrom, H. and Gustafsson, B. A possible correlate of the postsynaptic condition for
long-lasting potentiation in the guinea pig hippocampus in vitro. Neurosci. Lett. 44,
327?332 (1984).

401

"
1000,1994,Bayesian Query Construction for Neural Network Models,,1000-bayesian-query-construction-for-neural-network-models.pdf,Abstract Missing,"Bayesian Query Construction for Neural
Network Models
Gerhard Paass
Jorg Kindermann
German National Research Center for Computer Science (GMD)
D-53757 Sankt Augustin, Germany
paass@gmd.de
kindermann@gmd.de

Abstract
If data collection is costly, there is much to be gained by actively selecting particularly informative data points in a sequential way. In
a Bayesian decision-theoretic framework we develop a query selection criterion which explicitly takes into account the intended use
of the model predictions. By Markov Chain Monte Carlo methods
the necessary quantities can be approximated to a desired precision. As the number of data points grows, the model complexity
is modified by a Bayesian model selection strategy. The properties of two versions of the criterion ate demonstrated in numerical
experiments.

1

INTRODUCTION

In this paper we consider the situation where data collection is costly, as when
for example, real measurements or technical experiments have to be performed. In
this situation the approach of query learning ('active data selection', 'sequential
experimental design', etc.) has a potential benefit. Depending on the previously
seen examples, a new input value ('query') is selected in a systematic way and
the corresponding output is obtained. The motivation for query learning is that
random examples often contain redundant information, and the concentration on
non-redundant examples must necessarily improve generalization performance.
We use a Bayesian decision-theoretic framework to derive a criterion for query construction. The criterion reflects the intended use of the predictions by an appropriate

444

Gerhard Paass. Jorg Kindermann

loss function. We limit our analysis to the selection of the next data point, given a
set of data already sampled. The proposed procedure derives the expected loss for
candidate inputs and selects a query with minimal expected loss.
There are several published surveys of query construction methods [Ford et al. 89,
Plutowski White 93, Sollich 94]. Most current approaches, e.g. [Cohn 94], rely
on the information matrix of parameters. Then however, all parameters receive
equal attention regardless of their influence on the intended use of the model
[Pronzato Walter 92]. In addition, the estimates are valid only asymptotically. Bayesian approaches have been advocated by [Berger 80], and applied to neural networks
[MacKay 92]. In [Sollich Saad 95] their relation to maximum information gain is
discussed. In this paper we show that by using Markov Chain Monte Carlo methods it is possible to determine all quantities necessary for the selection of a query.
This approach is valid in small sample situations, and the procedure's precision can
be increased with additional computational effort. With the square loss function,
the criterion is reduced to a variant of the familiar integrated mean square error
[Plutowski White 93].

In the next section we develop the query selection criterion from a decision-theoretic
point of view. In the third section we show how the criterion can be calculated using
Markov Chain Monte Carlo methods and we discuss a strategy for model selection.
In the last section, the results of two experiments with MLPs are described.
2

A DECISION-THEORETIC FRAMEWORK

Assume we have an input vector x and a scalar output y distributed as y """" p(y I x, w)
where w is a vector of parameters. The conditional expected value is a deterministic
function !(x, w) := E(y I x, w) where y = !(x, w)+? and ? is a zero mean error term.
Suppose we have iteratively collected observations D(n) := ((Xl, iii), .. . , (Xn, Yn)).
We get the Bayesian posterior p(w I D(n)) = p(D(n) Iw) p(w)/ J p(D(n) Iw) p(w) dw
and the predictive distribution p(y I x, D(n)) = p(y I x, w)p(w I D(n)) dw if p(w) is
the prior distribution.

J

We consider the situation where, based on some data x, we have to perform an
action a whose result depends on the unknown output y. Some decisions may have
more severe effects than others. The loss function L(y, a) E [0,00) measures the
loss if y is the true value and we have taken the action a E A. In this paper we
consider real-valued actions, e.g. setting the temperature a in a chemical process.
We have to select an a E A only knowing the input x. According to the Bayes
Principle [Berger 80, p.14] we should follow a decision rule d : x --t a such that
the average risk J R(w, d) p(w I D(n)) dw is minimal, where the risk is defined as
R(w, d) := J L(y, d(x)) p(y I x, w) p(x) dydx. Here p(x) is the distribution of future
inputs, which is assumed to be known.
For the square loss function L(y, a) = (y - a)2, the conditional expectation
d(x) := E(y I x, D(n)) is the optimal decision rule. In a control problem the loss
may be larger at specific critical points. This can be addressed with a weighted square loss function L(y, a) := h(y)(y - a)2, where h(y) 2: a [Berger 80,
p.1U]. The expected loss for an action is J(y - a)2h(y) p(y I x, D(n)) dy. Replacing the predictive density p(y I x, D(n)) with the weighted predictive density

Bayesian Query Construction for Neural Network Models

445

p(y I x, Den) := h(y) p(y I x, Den)/G(x), where G(x) := I h(y) p(y I x, Den) dy,
we get the optimal decision rule d(x) := I yp(y I x, Den) dy and the average loss
G(x) I(y - E(y I x, D(n))2 p(y I x, Den) dy for a given input x. With these modifications, all later derIvations for the square loss function may be applied to the
weighted square loss.
The aim of query sampling is the selection of a new observation x in such a way
that the average risk will be maximally reduced. Together with its still unknown
y-value, x defines a new observation (x, y) and new data Den) U (x, y). To determine
this risk for some given x we have to perform the following conceptual steps for a
candidate query x:
1. Future Data: Construct the possible sets of 'future' observations Den) U

(x, y), where y """"' p(y I x, Den).
2. Future posterior: Determine a 'future' posterior distribution of parameters
p(w I Den) U (x, y? that depends on y in the same way as though it had
actually been observed.
3. Future Loss: Assuming d~,x(x) is the optimal decision rule for given values
of x, y, and x, compute the resulting loss as

1';,x(x):=

J

L(y,d;,x(x?p(ylx,w)p(wIDen)U(x,y?dydw

(1)

4. Averaging: Integrate this quantity over the future trial inputs x distributed
as p(x) and the different possible future outputs y, yielding

1';:= Ir;,x(x)p(x)p(ylx,Den)dxdy.
This procedure is repeated until an x with minimal average risk is found. Since local
optima are typical, a global optimization method is required. Subsequently we then
try to determine whether the current model is still adequate or whether we have to
increase its complexity (e.g. by adding more hidden units).

3

COMPUTATIONAL PROCEDURE

Let us assume that the real data Den) was generated according to a regression model
y = !(x, w)+{ with i.i.d. Gaussian noise {""""' N(O, (T2(w?. For example !(x, w) may
be a multilayer perceptron or a radial basis function network. Since the error terms
are independent, the posterior density is p( w I Den) ex: p( w) rr~=l P(Yi I Xi, w) even
in the case of query sampling [Ford et al. 89].
As the analytic derivation of the posterior is infeasible except in trivial cases, we
have to use approximations. One approach is to employ a normal approximation
[MacKay 92], but this is unreliable if the number of observations is small compared to the number of parameters. We use Markov Chain Monte Carlo procedures
[PaaB 91, Neal 93] to generate a sample WeB) := {WI, .. .WB} of parameters distributed according to p( w I Den). If the number of sampling steps approaches infinity,
the distribution of the simulated Wb approximates the posterior arbitrarily well.
To take into account the range of future y-values, we create a set of them by simulation. For each Wb E WeB) a number of y """"' p(y I x, Wb) is generated. Let

446

y(x.R)

Gerhard Paass. JiJrg Kindermann

{YI, ... , YR} be the resulting set. Instead of performing a new Markov
Monte Carlo run to generate a new sample according to p(w I DCn) U (x, y)), we
:=

use the old set WCB) of parameters and reweight them (importance sampling).
In this way we may approximate integrals of some function g( w) with respect to
p(w I DCn) U (x, y)) [Kalos Whitlock 86, p.92]:

- -))d
j 9 (w ) P(W IDCn) U( X,
Y
W

__

--

L~-lg(Wb)P(ylx,Wb)
B

Lb=l p(Y I x, Wb)

(2)

The approximation error approaches zero as the size of WCB) increases.

3.1

APPROXIMATION OF FUTURE LOSS

Consider the future loss f;,x(x) given new observation (x, y) and trial input Xt. In
the case of the square loss function, (1) can be transformed to

f~,.t(Xt)

=

j[!(Xt,w)-E(yIXt,Dcn)U(X,y)Wp(wIDcn)U(x,y))dw (3)

+ j ?T2(w) p(w I DCn) U (x, y)) dw
where ?T2(w) := Var(y I x, w) is independent of x. Assume a set XT = {Xl, ... , XT}
is given, which is representative of trial inputs for the distribution p(x). Define
S(x, y) := L~=i p(Y I x, Wb) for y E YCx,R) . Then from equations (2) and (3) we get
E(ylxt,DCn)U(x,y)):= 1/S(x,Y)L~=1!(Xt,Wb)P(Ylx,Wb) and
1

B

S(x -) L?T 2(Wb)P(Ylx,Wb)
,y b=l
1

+ S(x

(4)

B

-) I)!(Xt, Wb) - E(y I Xt, DCn) U (x, y))]2 p(Y I x, Wb)

,y

b=l

The final value of f; is obtained by averaging over the different y E YCx,R) and
different trial inputs Xt E XT. To reduce the variance, the trial inputs Xt should
be selected by importance sampling (2) to concentrate them on regions with high
current loss (see (5) below). To facilitate the search for an x with minimal f; we
reduce the extent of random fluctuations of the y values. Let (Vi, ... , VR) be a
vector of random numbers Vr -- N(O,1), and let jr be randomly selected from
{1, ... , B}. Then for each x the possible observations Yr E YCx,R) are defined as
Yr := !(x, wir) + V r?T2(wir). In this way the difference between neighboring inputs
is not affected by noise, and search procedures can exploit gradients.

3.2

CURRENT LOSS

As a proxy for the future loss, we may use the current loss at

x,

rcurr(x) = p(x) j L(y, d*(x)) p(y I x, DCn)) dy

(5)

Bayesian Query Construction for Neural Network Models

447

where p(x) weights the inputs according to their relevance. For the square loss
function the average loss at x is the conditional variance Var(y I x, DCn?. We get

=

Tcurr(X)

p(x) jU(x,w)-E(YIX,DCn?)2p(wIDcn?dw

(6)

+ p(x) j 0""2(w) p(w I D(n? dw
If E(y I x,DCn?
fr~~=lf(x,wb) and the sample WCB):= {Wl, ... ,WB} is
representative of p(w I DCn? we can approximate the current loss with

Tcurr(X)

~

p( x) ~

13 L..tU(x, Wb) -

2

E(y I x, DCn?) +
A

p( x) ~

13 L..t 0""

b=l

2

(Wb)

(7)

b=l

If the input distribution p( x) is uniform, the second term is independent of x.
3.3

COMPLEXITY REGULARIZATION

Neural network models can represent arbitrary mappings between finite-dimensional
spaces if the number of hidden units is sufficiently large [Hornik Stinchcombe 89].
As the number of observations grows, more and more hidden units are necessary to catch the details of the mapping. Therefore we use a sequential procedure to increase the capacity of our networks during query learning. White and
Wooldridge call this approach the ""method of sieves"" and provide some asymptotic results on its consistency [White Wooldridge 91]. Gelfand and Dey compare Bayesian approaches for model selection and prove that, in the case of nested models Ml and M2, model choice by the ratio of popular Bayes factors
p(DCn) I Mi) := J p(DCn) I W, Mi ) p(w I Mi) dw will always choose the full model
regardless of the data as n --t 00 [Gelfand Dey 94]. They show that the pseudoBayes factor, a Bayesian variant of crossvalidation, is not affected by this paradox

A(Ml' M2) :=

n

n

;=1

j=1

II p(y; I x;, DCn,j), Mt}j II p(Y; Ix;, DCn,j), M2)

(8)

Here DCn ,;) := D(n) \ (x;, y;). As the difference between p(w I DCn? and p( wi D(n,j?
is usually small, we use the full posterior as the importance function (2) and get

p(Y;

I x;, DCn,j),Mi) =

j p(Y; IXj,w,Mi)p(wIDCn,j),Mi)dw

'"" B/(t,l/P(Y;li;,W""M,))
4

(9)

NUMERICAL DEMONSTRATION

In a first experiment we tested the approach for a small a 1-2-1 MLP target function with Gaussian noise N(0,0.05 2 ). We assumed the square loss function and a
uniform input distribution p(x) over [-5,5]. Using the ""true"" architecture for the
approximating model we started with a single randomly generated observation. We

448

Gerhard Paass, JiJrg Kindermann

~

=~!?~

--- ~tuo:io_

~

.. .' .

1'01

..

on

~

I - '~ ' =~ I

:;

""

. ..

a:
0

::::.:::::.::::\....
d

:;

....

\~.

'\ ------ -- - - - - - - -----

\., 1\l

. . ......_. _-_._...........__................... _. ._......._..

~

\!

~

,

\

:.,.
\, '

""

\!

'""
0

..

-2

10

15
20
No.d_

25

30

Figure 1: Future loss exploration: predicted posterior mean, future loss and current
loss for 12 observations (left), and root mean square error of prediction (right) .
estimated the future loss by (4) for 100 different inputs and selected the input with
smallest future loss as the next query. B = 50 parameter vectors were generated requiring 200,000 Metropolis steps. Simultaneously we approximated the current loss
criterion by (7). The left side of figure 1 shows the typical relation of both measures.
In most situations the future loss is low in the same regions where the current loss
(posterior standard deviation of mean prediction) is high. The queries are concentrated in areas of high variation and the estimated posterior mean approximates
the target function quite well.
In the right part of figure 1 the RMSE of prediction averaged over 12 independent
experiments is shown. After a few observations the RMSE drops sharply. In our
example there is no marked difference between the prediction errors resulting from
the future loss and the current loss criterion (also averaged over 12 experiments).
Considering the substantial computing effort this favors the current loss criterion.
The dots indicate the RMSE for randomly generated data (averaged over 8 experiments) using the same Bayesian prediction procedure. Because only few data points
were located in the critical region of high variation the RMSE is much larger.
In the second experiment, a 2-3-1 MLP defined the target function I(x, wo) , to which
Gaussian noise of standard deviation 0.05 was added. I( x, wo) is shown in the left
part of figure 2. We used five MLPs with 2-6 hidden units as candidate models
Ml, .. . , M5 and generated B = 45 samples WeB) of the posterior pew I D(n)' M.),
where D(n) is the current data. We started with 30,000 Metropolis steps for small
values of n and increased this to 90,000 Metropolis steps for larger values of n.
For a network with 6 hidden units and n = 50 observations, 10,000 Metropolis
steps took about 30 seconds on a Sparc10 workstation. Next, we used equation (9)
to compare the different models, and then used the optimal model to calculate the
current loss (7) on a regular grid of 41 x 41 = 1681 query points x. Here we assumed
the square loss function and a uniform input distribution p(x) over [-5,5] x [-5,5].
We selected the query point with maximal current loss and determined the final
query point with a hillclimbing algorithm. In this way we were rather sure to get
close to the true global optimum.
The main result of the experiment is summarized in the right part of figure 2. It

Bayesian Query Construct.ion for Neural Network Models

449

?
o

"".

.m

eXDlorati~n
random a

:2""': \
<:>

\

~\?{l? .
.,
.""

o .. o .. o ............. __ (). ...

\

. . .......... 0 ... .. ........ --

..

~.

20

40

0

60

80

100

No. of Observations

Figure 2: Current loss exploration: MLP target function and root mean square error.
shows - averaged over 3 experiments - the root mean square error between the true
mean value and the posterior mean E(y I x) on the grid of 1681 inputs in relation to
the sample size. Three phases of the exploration can be distinguished (see figure 3).
In the beginning a search is performed with many queries on the border of the
input area. After about 20 observations the algorithm knows enough detail about
the true function to concentrate on the relevant parts of the input space. This leads
to a marked reduction ofthe mean square error. After 40 observations the systematic
part of the true function has been captured nearly perfectly. In the last phase of
the experiment the algorithm merely reduces the uncertainty caused by the random
noise. In contrast , the data generated randomly does not have sufficient information
on the details of f(x , w), and therefore the error only gradually decreases. Because
of space constraints we cannot report experiments with radial basis functions which
led to similar results.
Acknowledgements
This work is part of the joint project 'REFLEX' of the German Fed. Department
of Science and Technology (BMFT), grant number 01 IN 111Aj4. We would like to
thank Alexander Linden, Mark Ring, and Frank Weber for many fruitful discussions.

References
[Berger 80] Berger, J. (1980): Statistical Decision Theory, Foundations, Concepts, and
Methods. Springer Verlag, New York.
[Cohn 94] Cohn, D. (1994): Neural Network Exploration Using Optimal Experimental
Design. In J. Cowan et al. (eds.): NIPS 5. Morgan Kaufmann, San Mateo.
[Ford et al. 89] Ford, I. , Titterington, D.M., Kitsos, C.P. (1989): Recent Advances in Nonlinear Design. Technometrics, 31, p.49-60.
[Gelfand Dey 94] Gelfand, A.E., Dey, D.K. (1994): Bayesian Model Choice: Asymptotics
and Exact Calculations. J. Royal Statistical Society B, 56, pp.501-514.

450

Gerhard Paass, Jorg Kindermann

Figure 3: Squareroot of current loss (upper row) and absolute deviation from true
function (lower row) for 10,25, and 40 observations (which are indicated by dots) .
[Hornik Stinchcombe 89] Hornik, K., Stinchcombe, M. (1989): Multilayer Feedforward
Networks are Universal Approximators. Neural Networks 2, p.359-366.
[Kalos Whitlock 86] Kalos, M.H., Whitlock, P.A. (1986): Monte Carlo Methods, Wiley,
New York.
[MacKay 92] MacKay, D. (1992): Information-Based Objective Functions for Active Data
Selection. Neural Computation 4, p .590-604.
[Neal 93] Neal, R.M. (1993): Probabilistic Inference using Markov Chain Monte Carlo
Methods. Tech. Report CRG-TR-93-1, Dep. of Computer Science, Univ. of Toronto.
[PaaB 91] PaaB, G. (1991): Second Order Probabilities for Uncertain and Conflicting Evidence. In: P.P. Bonissone et al. (eds.) Uncertainty in Artificial Intelligence 6. Elsevier,
Amsterdam, pp. 447-456.
[Plutowski White 93] Plutowski, M., White, H. (1993): Selecting Concise Training Sets
from Clean Data. IEEE Tr. on Neural Networks, 4, p.305-318.
[Pronzato Walter 92] Pronzato, L., Walter, E. (1992): Nonsequential Bayesian Experimental Design for Response Optimization. In V. Fedorov, W.G. Miiller, I.N. Vuchkov
(eds.): Model Oriented Data-Analysis. Physica Verlag, Heidelberg, p. 89-102.
[Sollich 94] Sollich, P. (1994): Query Construction, Entropy and Generalization in Neural
Network Models. To appear in Physical Review E.
[Sollich Saad 95] Sollich, P., Saad, D. (1995): Learning from Queries for Maximum Information Gain in Unlearnable Problems. This volume.
[White Wooldridge 91] White, H., Wooldridge, J. (1991): Some Results for Sieve Estimation with Dependent Observations. In W. Barnett et al. (eds.) : Nonparametric and
Semiparametric Methods in Econometrics and Statistics, New York, Cambridge Univ.
Press.

"
1001,1994,"Neural Network Ensembles, Cross Validation, and Active Learning",,1001-neural-network-ensembles-cross-validation-and-active-learning.pdf,Abstract Missing,"Neural Network Ensembles, Cross
Validation, and Active Learning

Anders Krogh""
Nordita
Blegdamsvej 17
2100 Copenhagen, Denmark

Jesper Vedelsby
Electronics Institute, Building 349
Technical University of Denmark
2800 Lyngby, Denmark

Abstract
Learning of continuous valued functions using neural network ensembles (committees) can give improved accuracy, reliable estimation of the generalization error, and active learning. The ambiguity
is defined as the variation of the output of ensemble members averaged over unlabeled data, so it quantifies the disagreement among
the networks. It is discussed how to use the ambiguity in combination with cross-validation to give a reliable estimate of the ensemble
generalization error, and how this type of ensemble cross-validation
can sometimes improve performance. It is shown how to estimate
the optimal weights of the ensemble members using unlabeled data.
By a generalization of query by committee, it is finally shown how
the ambiguity can be used to select new training data to be labeled
in an active learning scheme.

1

INTRODUCTION

It is well known that a combination of many different predictors can improve predictions. In the neural networks community ""ensembles"" of neural networks has been
investigated by several authors, see for instance [1, 2, 3]. Most often the networks
in the ensemble are trained individually and then their predictions are combined.
This combination is usually done by majority (in classification) or by simple averaging (in regression), but one can also use a weighted combination of the networks .
.. Author to whom correspondence should be addressed. Email: kroghlnordita. elk

232

Anders Krogh, Jesper Vedelsby

At the workshop after the last NIPS conference (December, 1993) an entire session
was devoted to ensembles of neural networks ( ""Putting it all together"", chaired by
Michael Perrone) . Many interesting papers were given, and it showed that this area
is getting a lot of attention .
A combination of the output of several networks (or other predictors) is only useful
if they disagree on some inputs. Clearly, there is no more information to be gained
from a million identical networks than there is from just one of them (see also
[2]). By quantifying the disagreement in the ensemble it turns out to be possible
to state this insight rigorously for an ensemble used for approximation of realvalued functions (regression). The simple and beautiful expression that relates the
disagreement (called the ensemble ambiguity) and the generalization error is the
basis for this paper, so we will derive it with no further delay.

2

THE BIAS-VARIANCE TRADEOFF

Assume the task is to learn a function J from RN to R for which you have a sample
of p examples, (xiJ , yiJ), where yiJ = J(xiJ) and J.t = 1, . . . ,p. These examples
are assumed to be drawn randomly from the distribution p(x) . Anything in the
following is easy to generalize to several output variables.
The ensemble consists of N networks and the output of network a on input x is
called va (x). A weighted ensemble average is denoted by a bar , like

V(x) =

L Wa Va(x).

(1)

a

This is the final output of the ensemble. We think of the weight Wa as our belief in
network a and therefore constrain the weights to be positive and sum to one. The
constraint on the sum is crucial for some of the following results.
The ambiguity on input x of a single member of the ensemble is defined as aa (x)
(V a(x) - V(x))2 . The ensemble ambiguity on input x is

a(x)

= Lwaaa(x) = LWa(va(x) a

V(x))2 .

=

(2)

a

It is simply the variance of the weighted ensemble around the weighed mean, and
it measures the disagreement among the networks on input x. The quadratic error
of network a and of the ensemble are

(J(x) - V a(x))2
(J(x) - V(X))2

(3)
(4)

respectively. Adding and subtracting J( x) in (2) yields

a(x)

=L

Wafa(X) - e(x)

(5)

a

(after a little algebra using that the weights sum to one) . Calling the weighted
average of the individual errors ?( x) = La Wa fa (x) this becomes

e(x)

= ?(x) -

a(x).

(6)

Neural Network Ensembles, Cross Validation, and Active Learning

233

All these formulas can be averaged over the input distribution . Averages over the
input distribution will be denoted by capital letter, so

J dxp(xVl! (x)
J dxp(x)aa(x)
J dxp(x)e(x).

E

(7)
(8)
(9)

The first two of these are the generalization error and the ambiguity respectively
for network n , and E is the generalization error for the ensemble. From (6) we then
find for the ensemble generalization error
(10)
The first term on the right is the weighted average of the generalization errors of
the individual networks (E = La waEa), and the second is the weighted average
of the ambiguities (A = La WaAa), which we refer to as the ensemble ambiguity.
The beauty of this equation is that it separates the generalization error into a term
that depends on the generalization errors of the individual networks and another
term that contain all correlations between the networks . Furthermore, the correlation term A can be estimated entirely from unlabeled data, i. e., no knowledge is
required of the real function to be approximated. The term ""unlabeled example"" is
borrowed from classification problems, and in this context it means an input x for
which the value of the target function f( x) is unknown.
Equation (10) expresses the tradeoff between bias and variance in the ensemble ,
but in a different way than the the common bias-variance relation [4] in which the
averages are over possible training sets instead of ensemble averages. If the ensemble
is strongly biased the ambiguity will be small , because the networks implement very
similar functions and thus agree on inputs even outside the training set. Therefore
the generalization error will be essentially equal to the weighted average of the
generalization errors of the individual networks. If, on the other hand , there is a
large variance , the ambiguity is high and in this case the generalization error will
be smaller than the average generalization error . See also [5].
From this equation one can immediately see that the generalization error of the
ensemble is always smaller than the (weighted) average of the ensemble errors,
E < E. In particular for uniform weights:

E

~ ~ 'fEcx

(11)

which has been noted by several authors , see e.g. [3] .

3

THE CROSS-VALIDATION ENSEMBLE

From (10) it is obvious that increasing the ambiguity (while not increasing individual
generalization errors) will improve the overall generalization. We want the networks
to disagree! How can we increase the ambiguity of the ensemble? One way is to
use different types of approximators like a mixture of neural networks of different
topologies or a mixture of completely different types of approximators. Another

234

Anders Krogh, Jesper Vedelsby

.

:~

1. -

t

-

,',

.. ,

E o...... -' '.- .. ' ........ ....,.

.'

..... , ...

v '. --:

,

.~.--c??

__ .. -.tI""

.

. -- - -\\

'1

-

.~

~.

, . _ ? ."" ?

.. - .....

_._ ..... .'-._._.1

,

-

>

-

-1.k!
~

-4

.t.

f.

1\.1

:\,'. - ?-.l

:--,____
..

~~
.

~.

,

,'

-2

.~

If

o

2

\.
~
:
?

' 0'

~:

4

x

Figure 1: An ensemble of five networks were trained to approximate the square
wave target function f(x). The final ensemble output (solid smooth curve) and
the outputs of the individual networks (dotted curves) are shown. Also the square
root of the ambiguity is shown (dash-dot line) _ For training 200 random examples
were used, but each network had a cross-validation set of size 40, so they were each
trained on 160 examples.

obvious way is to train the networks on different training sets. Furthermore, to be
able to estimate the first term in (10) it would be desirable to have some kind of
cross-validation. This suggests the following strategy.
Chose a number K :::; p. For each network in the ensemble hold out K examples for
testing, where the N test sets should have minimal overlap, i. e., the N training sets
should be as different as possible. If, for instance, K :::; piN it is possible to choose
the K test sets with no overlap. This enables us to estimate the generalization error
E(X of the individual members of the ensemble, and at the same time make sure
that the ambiguity increases . When holding out examples the generalization errors
for the individual members of the ensemble, E(X, will increase, but the conjecture
is that for a good choice of the size of the ensemble (N) and the test set size
(K), the ambiguity will increase more and thus one will get a decrease in overall
generalization error.
This conjecture has been tested experimentally on a simple square wave function
of one variable shown in Figure 1. Five identical feed-forward networks with one
hidden layer of 20 units were trained independently by back-propagation using 200
random examples. For each network a cross-validation set of K examples was held
out for testing as described above. The ""true"" generalization and the ambiguity were
estimated from a set of 1000 random inputs. The weights were uniform, w(X
1/5
(non-uniform weights are addressed later).

=

In Figure 2 average results over 12 independent runs are shown for some values of

Neural Network Ensembles, Cross Validation, and Active Learning

Figure 2: The solid line shows the generalization error for uniform weights as
a function of K, where K is the size
of the cross-validation sets. The dotted
line is the error estimated from equation (10) . The dashed line is for the
optimal weights estimated by the use of
the generalization errors for the individual networks estimated from the crossvalidation sets as described in the text.
The bottom solid line is the generalization error one would obtain if the individual generalization errors were known
exactly (the best possible weights).

0.08

235

,-----r----,--~---r-----,

o

t=
w
0.06

c

o
~

.!::!

co...

~ 0.04

Q)

(!)

0 .02 '---_---1_ _---'-_ _--'-_ _-----'
o
20
40
60
80
Size of CV set

K (top solid line) . First, one should note that the generalization error is the same
for a cross-validation set of size 40 as for size 0, although not lower, so it supports
the conjecture in a weaker form. However, we have done many experiments, and
depending on the experimental setup the curve can take on almost any form, sometimes the error is larger at zero than at 40 or vice versa. In the experiments shown,
only ensembles with at least four converging networks out of five were used . If all
the ensembles were kept, the error would have been significantly higher at ]{ = a
than for K > a because in about half of the runs none of the networks in the ensemble converged - something that seldom happened when a cross-validation set
was used. Thus it is still unclear under which circumstances one can expect a drop
in generalization error when using cross-validation in this fashion.

The dotted line in Figure 2 is the error estimated from equation (10) using the
cross-validation sets for each of the networks to estimate Ea, and one notices a
good agreement.

4

OPTIMAL WEIGHTS

The weights Wa can be estimated as described in e.g. [3]. We suggest instead
to use unlabeled data and estimate them in such a way that they minimize the
generalization error given in (10) .
There is no analytical solution for the weights , but something can be said about
the minimum point of the generalization error. Calculating the derivative of E as
given in (10) subject to the constraints on the weights and setting it equal to zero
shows that
Ea - Aa
E or Wa = O.
(12)

=

(The calculation is not shown because of space limitations, but it is easy to do.)
That is, Ea - Aa has to be the same for all the networks. Notice that Aa depends
on the weights through the ensemble average of the outputs. It shows that the
optimal weights have to be chosen such that each network contributes exactly waE

236

Anders Krogh, Jesper Vedelsby

to the generalization error. Note, however, that a member of the ensemble can have
such a poor generalization or be so correlated with the rest of the ensemble that it
is optimal to set its weight to zero.
The weights can be ""learned"" from unlabeled examples, e.g. by gradient descent
minimization of the estimate of the generalization error (10). A more efficient
approach to finding the optimal weights is to turn it into a quadratic optimization
problem. That problem is non-trivial only because of the constraints on the weights
(L:a Wa = 1 and Wa 2:: 0). Define the correlation matrix,
C af3

=

f

dxp(x)V a (x)V f3 (x) .

(13)

Then, using that the weights sum to one, equation (10) can be rewritten as
E

=

L
a

wa Ea

+ L w a C af3 w f3 - L
af3

waCaa .

(14)

a

Having estimates of E a and C af3 the optimal weights can be found by linear programming or other optimization techniques. Just like the ambiguity, the correlation
matrix can be estimated from unlabeled data to any accuracy needed (provided that
the input distribution p is known).
In Figure 2 the results from an experiment with weight optimization are shown.
The dashed curve shows the generalization error when the weights are optimized as
described above using the estimates of Ea from the cross-validation (on K exampies). The lowest solid curve is for the idealized case, when it is assumed that the
errors Ea are known exactly, so it shows the lowest possible error. The performance
improvement is quite convincing when the cross-validation estimates are used.
It is important to notice that any estimate of the generalization error of the individual networks can be used in equation (14). If one is certain that the individual
networks do not overfit, one might even use the training errors as estimates for
Ea (see [3]). It is also possible to use some kind of regularization in (14), if the
cross-validation sets are small.

5

ACTIVE LEARNING

In some neural network applications it is very time consuming and/or expensive
to acquire training data, e.g., if a complicated measurement is required to find the
value of the target function for a certain input. Therefore it is desirable to only use
examples with maximal information about the function. Methods where the learner
points out good examples are often called active learning.
We propose a query-based active learning scheme that applies to ensembles of networks with continuous-valued output. It is essentially a generalization of query by
committee [6, 7] that was developed for classification problems. Our basic assumption is that those patterns in the input space yielding the largest error are those
points we would benefit the most from including in the training set.
Since the generalization error is always non-negative, we see from (6) that the
weighted average of the individual network errors is always larger than or equal to
the ensemble ambiguity,
f(X) 2:: a(x),
(15)

Neural Network Ensembles. Cross Validation. and Active Learning

237

2.5 r""':"":'T---r--""T""""--.-----r---,

.

.

.

:

0.5

o

10

20

30

Training set size

40

50

o

10

20

30

40

50

Training set size

Figure 3: In both plots the full line shows the average generalization for active
learning, and the dashed line for passive learning as a function of the number of
training examples. The dots in the left plot show the results of the individual
experiments contributing to the mean for the active learning. The dots in right plot
show the same for passive learning.

which tells us that the ambiguity is a lower bound for the weighted average of the
squared error. An input pattern that yields a large ambiguity will always have a
large average error. On the other hand, a low ambiguity does not necessarily imply
a low error. If the individual networks are trained to a low training error on the
same set of examples then both the error and the ambiguity are low on the training
points. This ensures that a pattern yielding a large ambiguity cannot be in the close
neighborhood of a training example. The ambiguity will to some extent follow the
fluctuations in the error. Since the ambiguity is calculated from unlabeled examples
the input-space can be scanned for these areas to any detail. These ideas are well
illustrated in Figure 1, where the correlation between error and ambiguity is quite
strong, although not perfect.
The results of an experiment with the active learning scheme is shown in Figure 3.
An ensemble of 5 networks was trained to approximate the square-wave function
shown in Figure 1, but in this experiments the function was restricted to the interval
from - 2 to 2. The curves show the final generalization error of the ensemble in a
passive (dashed line) and an active learning test (solid line). For each training set
size 2x40 independent tests were made, all starting with the same initial training
set of a single example. Examples were generated and added one at a time. In the
passive test examples were generated at random, and in the active one each example
was selected as the input that gave the largest ambiguity out of 800 random ones.
Figure 3 also shows the distribution of the individual results of the active and
passive learning tests. Not only do we obtain significantly better generalization by
active learning, there is also less scatter in the results. It seems to be easier for the
ensemble to learn from the actively generated set.

238

6

Anders Krogh. Jesper Vedelsby

CONCLUSION

The central idea in this paper was to show that there is a lot to be gained from
using unlabeled data when training in ensembles. Although we dealt with neural
networks, all the theory holds for any other type of method used as the individual
members of the ensemble.
It was shown that apart from getting the individual members of the ensemble to
generalize well, it is important for generalization that the individuals disagrees as
much as possible, and we discussed one method to make even identical networks
disagree. This was done by training the individuals on different training sets by
holding out some examples for each individual during training. This had the added
advantage that these examples could be used for testing, and thereby one could
obtain good estimates of the generalization error.
It was discussed how to find the optimal weights for the individuals of the ensemble.
For our simple test problem the weights found improved the performance of the
ensemble significantly.

Finally a method for active learning was described, which was based on the method
of query by committee developed for classification problems. The idea is that if the
ensemble disagrees strongly on an input, it would be good to find the label for that
input and include it in the training set for the ensemble. It was shown how active
learning improves the learning curve a lot for a simple test problem.
Acknowledgements

We would like to thank Peter Salamon for numerous discussions and for his implementation of linear programming for optimization of the weights. We also thank
Lars Kai Hansen for many discussions and great insights, and David Wolpert for
valuable comments.

References
[1] L.K. Hansen and P Salamon. Neural network ensembles. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 12(10):993- 1001, Oct. 1990.
[2] D.H Wolpert. Stacked generalization. Neural Networks, 5(2):241-59, 1992.
[3] Michael P. Perrone and Leon N Cooper. When networks disagree: Ensemble method
for neural networks. In R. J. Mammone, editor, Neural Networks for Speech and Image
processing. Chapman-Hall, 1993.
[4] S. Geman , E . Bienenstock, and R Doursat. Neural networks and the bias/variance
dilemma. Neural Computation, 4(1):1-58, Jan. 1992.
[5] Ronny Meir. Bias, variance and the combination of estimators; the case of linear least
squares. Preprint (In Neuroprose), Technion, Heifa, Israel, 1994.
[6] H.S. Seung, M. Opper, and H. Sompolinsky. Query by committee. In Proceedings of
the Fifth Workshop on Computational Learning Theory, pages 287-294, San Mateo,
CA, 1992. Morgan Kaufmann.
[7] Y. Freund, H.S. Seung, E. Shamir, and N. Tishby. Information, prediction, and query
by committee. In Advances in Neural Information Processing Systems, volume 5, San
Mateo, California, 1993. Morgan Kaufmann.

"
1002,1994,Using a neural net to instantiate a deformable model,,1002-using-a-neural-net-to-instantiate-a-deformable-model.pdf,Abstract Missing,"U sing a neural net to instantiate a
deformable model
Christopher K. I. Williams; Michael D. Revowand Geoffrey E. Hinton
Department of Computer Science, University of Toronto
Toronto, Ontario, Canada M5S lA4

Abstract
Deformable models are an attractive approach to recognizing nonrigid objects which have considerable within class variability. However, there are severe search problems associated with fitting the
models to data. We show that by using neural networks to provide
better starting points, the search time can be significantly reduced.
The method is demonstrated on a character recognition task.
In previous work we have developed an approach to handwritten character recognition based on the use of deformable models (Hinton, Williams and Revow, 1992a;
Revow, Williams and Hinton, 1993). We have obtained good performance with this
method, but a major problem is that the search procedure for fitting each model to
an image is very computationally intensive, because there is no efficient algorithm
(like dynamic programming) for this task. In this paper we demonstrate that it is
possible to ""compile down"" some of the knowledge gained while fitting models to
data to obtain better starting points that significantly reduce the search time.

1

DEFORMABLE MODELS FOR DIGIT RECOGNITION

The basic idea in using deformable models for digit recognition is that each digit has
a model, and a test image is classified by finding the model which is most likely to
have generated it. The quality of the match between model and test image depends
on the deformation of the model, the amount of ink that is attributed to noise and
the distance of the remaining ink from the deformed model.
?Current address: Department of Computer Science and Applied Mathematics, Aston
University, Birmingham B4 7ET, UK.

966

Christopher K. T. Williams, Michael D. Revow, Geoffrey E. Hinton

More formally, the two important terms in assessing the fit are the prior probability distribution for the instantiation parameters of a model (which penalizes very
distorted models), and the imaging model that characterizes the probability distribution over possible images given the instantiated model l . Let I be an image, M
be a model and z be its instantiation parameters. Then the evidence for model M
is given by
P(IIM)

=

J

P(zIM)P(IIM, z)dz

(1)

The first term in the integrand is the prior on the instantiation parameters and the
second is the imaging model i.e., the likelihood of the data given the instantiated
model. P(MII) is directly proportional to P(IIM), as we assume a uniform prior
on each digit.
Equation 1 is formally correct, but if z has more than a few dimensions the evaluation of this integral is very computationally intensive. However, it is often possible
to make an approximation based on the assumption that the integrand is strongly
peaked around a (global) maximum value z*. In this case, the evidence can be approximated by the highest peak of the integrand times a volume factor ~(zII, M),
which measures the sharpness of the peak 2 .
P(IIM) ~ P(z*IM)P(Ilz*, M)~(zII, M)

(2)

By Taylor expanding around z* to second order it can be shown that the volume
factor depends on the determinant of the Hessian of 10gP(z, 11M) . Taking logs
of equation 2, defining EdeJ as the negative log of P(z*IM), and EJit as the corresponding term for the imaging model, then the aim of the search is to find the
minimum of E tot = EdeJ + EJit . Of course the total energy will have many local
minima; for the character recognition task we aim to find the global minimum by
using a continuation method (see section 1.2).
1.1

SPLINES, AFFINE TRANSFORMS AND IMAGING MODELS

This section presents a brief overview of our work on using deformable models for
digit recognition. For a fuller treatment, see Revow, Williams and Hinton (1993) .
Each digit is modelled by a cubic B-spline whose shape is determined by the positions of the control points in the object-based frame. The models have eight control
points, except for the one model which has three, and the seven model which has
five. To generate an ideal example of a digit the control points are positioned at
their ""home"" locations. Deformed characters are produced by perturbing the control points away from their home locations. The home locations and covariance
matrix for each model were adapted in order to improve the performance.
The deformation energy only penalizes shape deformations. Affine transformations,
i.e., translation, rotation, dilation, elongation, and shear, do not change the underlying shape of an object so we want the deformation energy to be invariant under
them . We achieve this by giving each model its own ""object-based frame"" and
computing the deformation energy relative to this frame.
lThis framework has been used by many authors, e.g. Grenander et al (1991) .
2The Gaussian approximation has been popularized in the neural net community by
MacKay (1992) .

Using a Neural Net to Instantiate a Deformable Model

967

The data we used consists of binary-pixel images of segmented handwritten digits.
The general flavour of a imaging model for this problem is that there should be a
high probability of inked pixels close to the spline, and lower probabilities further
away. This can be achieved by spacing out a number of Gaussian ""ink generators""
uniformly along the contour; we have found that it is also useful to have a uniform
background noise process over the area of the image that is able to account for
pixels that occur far away from the generators. The ink generators and background
process define a mixture model. Using the assumption that each data point is
generated independently given the instantiated model, P(Ilz*, M) factors into the
product of the probability density of each black pixel under the mixture model.

1.2

RECOGNIZING ISOLATED DIGITS

For each model, the aim of the search is to find the instantiation parameters that
minimize E tot . The search starts with zero deformations and an initial guess for
the affine parameters which scales the model so as to lie over the data with zero
skew and rotation. A small number of generators with the same large variance are
placed along the spline, forming a broad, smooth ridge of high ink-probability along
the spline. We use a search procedure similar to the (iterative) Expectation Maximization (EM) method of fitting an unconstrained mixture of Gaussians, except
that (i) the Gaussians are constrained to lie on the spline (ii) there is a deformation energy term and (iii) the affine transformation must be recalculated on each
iteration. During the search the number of generators is gradually increased while
their variance decreases according to predetermined ""annealing"" schedule3 .
After fitting all the models to a particular image, we wish to evaluate which of the
models best ""explains"" the data. The natural measure is the sum of Ejit, Edej
and the volume factor. However, we have found that performance is improved by
including four additional terms which are easily obtained from the final fits of the
model to the image. These are (i) a measure which penalizes matches in which
there are beads far from any inked pixels (the ""beads in white space"" problem),
and (ii) the rotation, shear and elongation of the affine transform. It is hard to
decide in a principled way on the correct weightings for all of these terms in the
evaluation function. We estimated the weightings from the data by training a
simple postprocessing neural network. These inputs are connected directly to the
ten output units. The output units compete using the ""softmax"" function which
guarantees that they form a probability distribution, summing to one.

2

PREDICTING THE INSTANTIATION PARAMETERS

The search procedure described above is very time consuming. However, given many
examples of images and the corresponding instantiation parameters obtained by the
slow method, it is possible to train a neural network to predict the instantiation
parameters of novel images. These predictions provide better starting points, so the
search time can be reduced.
3The schedule starts with 8 beads increasing to 60 beads in six steps, with the variance
decreasing from 0.04 to 0.0006 (measured in the object frame). The scale is set in the
object-based frame so that each model is 1 unit high.

968

2.1

Christopher K. I. Williams, Michael D. Revow, Geoffrey E. Hinton

PREVIOUS WORK

Previous work on hypothesizing instantiation parameters can be placed into two
broad classes, correspondence based search and parameter space search. In correspondence based search, the idea is to extract features from the image and identify
corresponding features in the model. Using sufficient correspondences the instantiation parameters of the model can be determined. The problem is that simple, easily
detectable image features have many possible matches, and more complex features
require more computation and are more difficult to detect. Grimson (1990) shows
how to search the space of possible correspondences using an interpretation tree.
An alternative approach, which is used in Hough transform techniques, is to directly work in parameter space. The Hough transform was originally designed for
the detection of straight lines in images, and has been extended to cover a number
of geometric shapes, notably conic sections. Ballard (1981) further extended the
approach to arbitrary shapes with the Generalized Hough Transform . The parameter space for each model is divided into cells (""binned""), and then for each image
feature a vote is added to each parameter space bin that could have produced that
feature. After collecting votes from all image features we then search for peaks in
the parameter space accumulator array, and attempt to verify pose. The Hough
transform can be viewed as a crude way of approximating the logarithm of the
posterior distribution P(zII, M) (e.g. Hunt et al , 1988).
However, these two techniques have only been used on problems involving rigid
models, and are not readily applicable to the digit recognition problem. For the
Hough space method, binning and vote collection is impractical in the high dimensional parameter space, and for the correspondence based approach there is a
lack of easily identified and highly discriminative features. The strengths of these
two techniques, namely their ability to deal with arbitrary scalings, rotations and
translations of the data, and their tolerance of extraneous features, are not really
required for a task where the input data is fairly well segmented and normalized.
Our approach is to use a neural network to predict the instantiation parameters for
each model, given an input image. Zemel and Hinton (1991) used a similar method
with simple 2-d objects, and more recently, Beymer et al (1993) have constructed
a network which maps from a face image to a 2-d parameter space spanning head
rotations and a smile/no-smile dimension. However, their method does not directly
map from images to instantiation parameters; they use a computer vision correspondence algorithm to determine the displacement field of pixels in a novel image
relative to a reference image, and then use this field as the input to the network.
This step limits the use of the approach to images that are sufficiently similar so
that the correspondence algorithm functions well.

2.2

INSTANTIATING DIGIT MODELS USING NEURAL
NETWORKS
The network which is used to predict the model instantiation parameters is shown
in figure 1. The (unthinned) binary images are normalized to give 16 x 16 8-bit
greyscale images which are fed into the neural network. The network uses a standard
three-layer architecture; each hidden unit computes a weighted sum of its inputs,
and then feeds this value through a sigmoidal nonlinearity u(x) = 1/(1 + e- X ). The

Using a Neural Net to Instantiate a Deformable Model

cps for 0 model

cps for I model

969

cps for 9 model

o

Figure 1: The prediction network architecture. ""cps"" stands for control points.
output values are a weighted linear combination of the hidden unit activities plus
output biases. The targets are the locations of the control points in the normalized
image, found from fitting models as described in section 1.2.
The network was trained with backpropagation to minimize the squared error, using
900 training images and 200 validation images of each digit drawn from the br
set of the CEDAR CDROM 1 database of Cities, States, ZIP Codes, Digits, and
Alphabetic Characters4 . Two test sets were used; one was obtained from data in the
br dataset, and the other was the (official) bs test set. After some experimentation
we chose a network with twenty hidden units, which means that the net has over
8,000 weights . With such a large number of weights it is important to regularize the
solution obtained by the network by using a complexity penalty; we used a weight
and optimized A on a validation set. Targets were only set for the
penalty AL: j
correct digit at the output layer; nothing was backpropagated from the other output
units. The net took 440 epochs to train using the default conjugate gradient search
method in the Xerion neural network simulator 5 . It would be possible to construct
ten separate networks to carry out the same task as the net described above, but
this would intensify the danger of overfitting, which is reduced by giving the network
a common pool of hidden units which it can use as it decides appropriate.

wJ

For comparison with the prediction net described above, a trivial network which
just consisted of output biases was trained; this network simply learns the average
value of the control point locations. On a validation set the squared error of the
prediction net was over three times smaller than the trivial net. Although this is
encouraging, the acid test is to compare the performance of elastic models settled
from the predicted positions using a shortened annealing schedule; if the predictions
are good, then only a short amount of settling will be required.
4Made available by the Unites States Postal Service Office of Advanced Technology.
5Xerion was designed and implemented by Drew van Camp, Tony Plate and Geoffrey
Hinton at the University of Toronto.

970

Christopher K. I. Williams, Michael D. Revow, Geoffrey E. Hinton

Figure 2: A comparision of the initial instantiations due to the prediction net (top row)
and the trivial net (bottom row) on an image of a 2. Notice that for the two model the
prediction net is much closer to the data. The other digit models mayor may not be greatly
affected by the input data; for example, the predictions from both nets seem essentially
the same for the zero, but for the seven the prediction net puts the model nearer to the
data.
The feedforward net predicts the position of the control points in the normalized
image. By inverting the normalization process, the positions of the control points
in the un-normalized image are determined. The model deformation and affine
transformation corresponding to these image control point locations can then be
determined by running a part of one iteration of the search procedure. Experiments
were then conducted with a number of shortened annealing schedules; for each one,
data obtained from settling on a part of the training data was used to train the
postprocessing net. The performance was then evaluated on the br test set.
The full annealing schedule has six stages. The shortened annealing schedules are:
1. No settling at all
2. Two iterations at the final variance of 0.0006
3. One iteration at 0.0025 and two at 0.0006
4. The full annealing schedule (for comparison)
The results on the br test set are shown in table 1. The general trends are that the
performance obtained using the prediction net is consistently better than the trivial
net, and that longer annealing schedules lead to better performance. A comparison
of schedules 3 and 4 in table 1 indicates that the performance of the prediction
net/schedule 3 combination is similar to (or slightly better than) that obtained
with the full annealing schedule, and is more than a factor of two faster. The
results with the full schedule are almost identical to the results obtained with the
default ""box"" initialization described in section 1.2. Figure 2 compares the outputs
of the prediction and trivial nets on a particular example. Judging from the weight

Using a Neural Net to Instantiate a Deformable Model

Schedule number

Trivial net

Prediction net

1
2
3
4

427
329
160
40

200
58
32
36

971

Average time required
to settle one model (s)
0.12
0.25
0.49
1.11

Table 1: Errors on the internal test set of 2000 examples for different annealing schedules.
The timing trials were carried out on a R-4400 machine.

vectors and activity patterns of the hidden units, it does not seem that some of the
units are specialized for a particular digit class.
A run on the bs test set using schedule 3 gave an error rate of 4.76 % (129 errors),
which is very similar to the 125 errors obtained using the full annealing schedule
and the box initialization. A comparison of the errors made on the two runs shows
that only 67 out of the 129 errors were common to the two sets. This suggests that
it would be very sensible to reject cases where the two methods do not agree.

3

DISCUSSION

The prediction net used above can be viewed as an interpolation scheme in the
control point position space of each digit z(I) = Zo + 2:i ai(I)zi, where z(I) is
the predicted position in the control point space, Zo is the contribution due to the
biases, ai is the activity of hidden unit i and Zi is its location in the control point
position space (learned from the data) . If there are more hidden units than output
dimensions, then for any particular image there are an infinite number of ways to
make this equation hold exactly. However, the network will tend to find solutions
so that the ai(I)'s will vary smoothly as the image is perturbed.
The nets described above output just one set of instantiation parameters for a
given model. However, it may be preferable to be able to represent a number of
guesses about model instantiation parameters; one way of doing this is to train a
network that has multiple sets of output parameters, as in the ""mixture of experts""
architecture of Jacobs et aI (1991). The outputs can be interpreted as a mixture
distribution in the control point position space, conditioned on the input image.
Another approach to providing more information about the posterior distribution
is described in (Hinton, Williams and Revow, 1992b), where P(zlI) is approximated
using a fixed set of basis functions whose weighting depends on the input image I.
The strategies descriped above directly predict the instantiation parameters in parameter space. It is also possible to use neural networks to hypothesize correspondences, i.e. to predict an inked pixel's position on the spline given a local window
of context in the image. With sufficient matches it is then possible to compute
the instantiation parameters of the model. We have conducted some preliminary
experiments with this method (described in Williams, 1994), which indicate that
good performance can be achieved for the correspondence prediction task.

972

Christopher K. I. Williams, Michael D. Revow, Geoffrey E. Hinton

We have shown that the we can obtain significant speedup using the prediction net.
The schemes outlined above which allow multimodal predictions in instantiation
parameter space may improve performance and deserve further investigation. We
are also interested in improving the performance of the prediction net, for example
by outputting a confidence measure which could be used to adjust the length of
the elastic models' search appropriately. We believe that using machine learning
techniques like neural networks to help reduce the amount of search required to fit
complex models to data may be useful for many other problems.
Acknowledgements
This research was funded by Apple and by the Ontario Information Technology Research
Centre. We thank Allan Jepson, Richard Durbin, Rich Zemel, Peter Dayan, Rob Tibshirani
and Yann Le Cun for helpful discussions. Geoffrey Hinton is the Noranda Fellow of the
Canadian Institute for Advanced Research.

References
Ballard, D. H. (1981). Generalizing the Hough transfrom to detect arbitrary shapes.
Pattern Recognition, 13(2):111-122.
Beymer, D., Shashua, A., and Poggio, T . (1993). Example Based Image Analysis and
Synthesis. AI Memo 1431, AI Laboratory, MIT.
Grenander, U., Chow, Y., and Keenan, D. M. (1991). Hands: A pattern theoretic study of
biological shapes. Springer-Verlag.
Grimson, W. E. 1. (1990) . Object recognition by computer. MIT Press, Cambridge, MA.
Hinton, G. E., Williams, C. K. 1., and Revow, M. D. (1992a). Adaptive elastic models
for hand-printed character recognition. In Moody, J. E., Hanson, S. J., and Lippmann, R. P., editors, Advances in Neural Information Processing Systems 4. Morgan
Kauffmann.
Hinton, G. E., Williams, C. K. 1., and Revow, M. D. (1992b). Combinining two methods
of recognizing hand-printed digits. In Aleksander, 1. and Taylor, J., editors, Artificial
Neural Networks 2. Elsevier Science Publishers.
Hunt, D. J., Nolte, L. W., and Ruedger, W . H. (1988) . Performance of the Hough Transform and its Relationship to Statistical Signal Detection Theory. Computer Vision,
Graphics and Image Processing, 43:221- 238.
Jacobs, R. A., Jordan, M. 1., Nowlan, S. J., and Hinton, G. E. (1991). Adaptive mixtures
of local experts. Neural Computation, 3(1).
MacKay, D. J. C. (1992). Bayesian Interpolation. Neural Computation, 4(3):415-447.
Revow, M. D., Williams, C. K. 1., and Hinton, G. E. (1993) . Using mixtures of deformable
models to capture variations in hand printed digits. In Srihari, S., editor, Proceedings
of the Third International Workshop on Frontiers in Handwriting Recognition, pages
142-152, Buffalo, New York, USA.
Williams, C. K. 1. (1994) . Combining deformable models and neural networks for handprinted digit recognition. PhD thesis, Dept. of Computer Science, University of
Toronto.
Zemel, R . S. and Hinton, G. E. (1991) . Discovering viewpoint-invariant relationships that
characterize objects. In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, Advances In Neural Information Processing Systems 3, pages 299-305. Morgan
Kaufmann Publishers.

"
1003,1994,Plasticity-Mediated Competitive Learning,,1003-plasticity-mediated-competitive-learning.pdf,Abstract Missing,"Plasticity-Mediated Competitive Learning

Terrence J. Sejnowski
terry@salk.edu

Nicol N. Schraudolph
nici@salk.edu

Computational Neurobiology Laboratory
The Salk Institute for Biological Studies
San Diego, CA 92186-5800

and
Computer Science & Engineering Department
University of California, San Diego
La Jolla, CA 92093-0114

Abstract
Differentiation between the nodes of a competitive learning network is conventionally achieved through competition on the basis of neural activity. Simple inhibitory mechanisms are limited
to sparse representations, while decorrelation and factorization
schemes that support distributed representations are computationally unattractive. By letting neural plasticity mediate the competitive interaction instead, we obtain diffuse, nonadaptive alternatives for fully distributed representations. We use this technique
to Simplify and improve our binary information gain optimization algorithm for feature extraction (Schraudolph and Sejnowski,
1993); the same approach could be used to improve other learning
algorithms.

1 INTRODUCTION
Unsupervised neural networks frequently employ sets of nodes or subnetworks
with identical architecture and objective function. Some form of competitive interaction is then needed for these nodes to differentiate and efficiently complement
each other in their task.

476

Nicol Schraudolph, Terrence 1. Sejnowski

1.00 -

-

j ................................. '.'

f(y)

?4r(y)'....

0.50 -

........../ /....??1

0.00 -

'.:!' ...."" ? ? , , ? ? . , ,. .. ' ..1???? ?????? ?? ?

=...:::....::::...:j:....:........-.. -~

=...

-4.00

y
-2.00

0.00

2.00

4.00

Figure 1: Activity f and plasticity f' of a logistic node as a function of its net input
y. Vertical lines indicate those values of y whose pre-images in input space are
depicted in Figure 2.

Inhibition is the simplest competitive mechanism: the most active nodes suppress
the ability of their peers to learn, either directly or by depressing their activity.
Since inhibition can be implemented by diffuse, nonadaptive mechanisms, it is an
attractive solution from both neurobiological and computational points of view.
However, inhibition can only form either localized (unary) or sparse distributed
representations, in which each output has only one state with significant information content.
For fully distributed representations, schemes to decorrelate (Barlow and Foldiak,
1989; Leen, 1991) and even factorize (Schmidhuber, 1992; Bell and Sejnowski, 1995)
node activities do exist. Unfortunately these require specific, weighted lateral
connections whose adaptation is computationally expensive and may interfere
with feedforward learning. While they certainly have their place as competitive
learning algorithms, the capability of biological neurons to implement them seems
questionable.
In this paper, we suggest an alternative approach: we extend the advantages of

simple inhibition to distributed representations by decoupling the competition
from the activation vector. In particular, we use neural plasticity - the derivative
of a logistic activation function - as a medium for competition.
Plasticity is low for both high and low activation values but high for intermediate
ones (Figure 1); distributed patterns of activity may therefore have localized plasticity. If competition is controlled by plasticity then, simple competitive mechanisms
will constrain us to localized plasticity but allow representations with distributed
activity.
The next section reintroduces the binary information gain optimization (BINGO)
algorithm for a single node; we then discuss how plasticity-mediated competition
improves upon the decorrelation mechanism used in our original extension to
multiple nodes. Finally, we establish a close relationship between the plasticity
and the entropy of a logistiC node that provides an intuitive interpretation of
plasticity-mediated competitive learning in this context.

Plasticity-Med;ated Competitive Learning

477

2 BINARY INFORMATION GAIN OPTIMIZATION
In (Schraudolph and Sejnowski, 1993), we proposed an unsupervised learning rule

that uses logistic nodes to seek out binary features in its input. The output
z

= f(y),

where f(y)

1

= 1 + e- Y

and y

= tV ? x

(1)

of each node is interpreted stochastically as the probability that a given feature is
present. We then search for informative directions in weight space by maximizing
the information gained about an unknown binary feature through observation of
z. This binary infonnation gain is given by

D.H(z)

= H(Z) -

H(z) ,

(2)

where H(z) is the entropy of a binary random variable with probability z, and z
is a prediction of z based on prior knowledge. Gradient ascent in this objective
results in the learning rule

D.w

<X

J'(y) . (y - fI) . x,

(3)

where fI is a prediction of y. In the simplest case, fI is an empirical average (y) of past
activity, computed either over batches of input data or by means of an exponential
trace; this amounts to a nonlinear version of the covariance rule (Sejnowski, 1977).
Using just the average as prediction introduces a strong preference for splitting the
data into two equal-sized clusters. While such a bias is appropriate in the initial
phase of learning, it fails to take the nonlinear nature of f into account. In order
to discount data in the saturated regions of the logistic function appropriately, we
weigh the average by the node's plasticity J'(y):

(y . f'(y))
(f'(y)) + C ,

fI = --'-'---'--'-'--'-'--

(4)

where c is a very small positive constant introduced to ensure numerical stability
for large values of y. Now the bias for splitting the data evenly is gradually relaxed
as the network's weights grow and data begins to fall into saturated regions of f.

3

PLASTICITY-MEDIATED COMPETITION

For multiple nodes the original BINGO algorithm used a decorrelating predictor
as the competitive mechanism:

g = y + (Qg -

2I)(y - (y)) ,

(5)

where Qg is the autocorrelation matrix of y, and I the identity matrix. Note that
Qg is computationally expensive to maintain; in connectionist implementations it

478

Nicol Schraudolph, Terrence J. Sejnowski

j

!
i

.: f
. ....~'. ..i..

,.

.: . ?,""f. e: 1',.

..... ...

"" ... ~.',, "" . ..:.....

, , :~X ~.""

..

?'IJ""~~ .~~~.
.. . ~~

.. .

.j

I . .

::

': "" !

Figure 2: The ""three cigars"" problem. Each plot shows the pre-image of zero net
. input, superimposed on a scatter plot of the data set, in input space. The two
flanking lines delineate the ""plastic region"" where the logistic is not saturated,
providing an indication of weight vector size. Left, two-node BINGO network
using decorrelation (Equations 3 & 5) fails to separate the three data clusters. Right,
same network using plasticity-mediated competition (Equations 4 & 6) succeeds.

is often approximated by lateral anti-Hebbian connections whose adaptation must
occur on a faster time scale than that of the feedforward weights (Equation 3) for
reasons of stability (Leen, 1991). In practice this means that learning is slowed
significantly.
In addition, decorrelation can be inappropriate when nonlinear objectives are optimized - in our case, two prominent binary features may well be correlated.
Consider the ""three cigars"" problem illustrated in Figure 2: the decorrelating predictor (left) forces the two nodes into a near-orthogonal arrangement, interfering
with their ability to detect the parallel gaps separating the data clusters.
For our purposes, decorrelation is thus too strong a constraint on the discriminants:
all we require is that the discovered features be distinct. We achieve this by reverting
to the simple predictor of Equation 4 while adding a global, plasticity-mediated
excitation l factor to the weight update:
~Wi ex: f'(Yi) . (Yi - 1li) . X ?

L

f'(Yj)

(6)

j

As Figure 2 (right) illustrates, this arrangement solves the ""three cigars"" problem. In the high-dimensional environment of hand-written digit recognition, this
algorithm discovers a set of distributed binary features that preserve most of the
information needed to classify the digits, even though the network was never given
any class labels (Figure 3).
1 The interaction is excitatory rather than inhibitory since a node's plasticity is inversely
correlated with the magnitude of its net input.

Plasticity-Mediated Competitive Learning

.... ........
.
..
.....
.......
....
. ..
.
....
?
???????
........ .. ................
........
-.....
..
.......
.
..........
??????
....
?????????
..............
.
..
??????????

................... . ..
...................
,

...?

479

""

..,

?

............ ............. ......
.......
?????
???????? ....
"" ,

? I

,

~

'

.

""

...

,

???

..?.............. ,
?...
.-

a ....

""

I.

I

......

?

.....
.....
....
.?..????
??

? I ............

.. ..

""

""

t

'""

~

_

......
.....
......
..... ....
......
..?????.....
..
..
???
...?........
??
........
.
????

..
, ?????
a .......
'

? ......

?
?????
???
'

?

'

'

'""

,

...... I

??? ,
...........

l

. . . . . . to. . . . .

.. ... a ..

Figure 3: Weights found by a four-node network running the improved BINGO
algorithm (Equations 4 & 6) on a set of 1200 handwritten digits due to (Guyon et aI.,
1989). Although the network is unsupervised, its four-bit output conveys most of
the information necessary to classify the digits.

4 PLASTICITY AND BINARY ENTROPY
It is possible to establish a relationship between the plasticity /' of a logistiC node
and its entropy that provides an intuitive account of plasticity-mediated competition as applied to BINGO. Consider the binary entropy

H(z)

= - z logz -

(1 - z) log(l - z)

(7)

A well-known quadratic approximation is

= 8e- 1 z (1 -

H(z)

z) ~ H(z)

(8)

Now observe that the plasticity of a logistic node

!'(Y)=:Y l+le _ y =, .. =z(l-z)

(9)

is in fact proportional to H(z) - that is, a logistic node's plasticity is in effect
a convenient quadratic approximation to its binary output entropy. The overall
entropy in a layer of such nodes equals the sum of individual entropies less their
redundancy:
(10)
H(z) =
H(zj) - R(Z)

L
j

The plasticity-mediated excitation factor in Equation 6

(11)
j

j

is thus proportional to an approximate upper bound on the entropy of the layer,
which in turn indicates how much more information remains to be gained by
learning from a particular input. In the context of BINGO, plasticity-mediated

480

Nicol SchraudoLph. Terrence J. Sejnowski

competition thus scales weight changes according to a measure of the network's
ignorance: the less it is able to identify a given input in terms of its set of binary
features, the more it tries to learn doing so.

5 CONCLUSION
By using the derivative of a logistic activation function as a medium for competitive
interaction, we were able to obtain differentiated, fully distributed representations
without resorting to computationally expensive decorrelation schemes. We have
demonstrated this plasticity-mediated competition approach on the BINGO feature
extraction algorithm, which is significantly improved by it. A close relationship
between the plasticity of a logistic node and its binary output entropy provides an
intuitive interpretation of this unusual form of competition.
Our general approach of using a nonmonotonic function of activity - rather than
activity itself - to control competitive interactions may prove valuable in other
learning schemes, in particular those that seek distributed rather than local representations.
Acknowledgements

We thank Rich Zemel and Paul Viola for stimulating discussions, and the McDonnell-Pew Center for Cognitive Neuroscience in San Diego for financial support.
References

Barlow, H. B. and Foldiak, P. (1989). Adaptation and decorrelation in the cortex. In
Durbin, R. M., Miall, c., and Mitchison, G. J., editors, The Computing Neuron,
chapter 4, pages 54-72. Addison-Wesley, Wokingham.
Bell, A. J. and Sejnowski, T. J. (1995). A non-linear information maximisation
algorithm that performs blind separation. In Advances in Neural Information
Processing Systems, volume 7, Denver 1994.
Guyon,!., Poujaud, 1., Personnaz, L., Dreyfus, G., Denker, J., and Le Cun, Y. (1989).
Comparing different neural network architectures for classifying handwritten
digits. In Proceedings of the International Joint Conference on Neural Networks,
volume II, pages 127-132. IEEE.
Leen, T. K. (1991). Dynamics of learning in linear feature-discovery networks.

Network, 2:85-105.
Schmidhuber, J. (1992). Learning factorial codes by predictability minimization.
Neural Computation, 4(6):863-879.
Schraudolph, N. N. and Sejnowski, T. J. (1993). Unsupervised discrimination of
clustered data via optimization of binary information gain. In Hanson, S. J.,
Cowan, J. D., and Giles, C. L., editors, Advances in Neural Information Processing Systems, volume 5, pages 499-506, Denver 1992. Morgan Kaufmann, San
Mateo.
Sejnowski, T. J. (1977). Storing covariance with nonlinearly interacting neurons.
Journal of Mathematical Biology, 4:303-321.

"
1004,1994,ICEG Morphology Classification using an Analogue VLSI Neural Network,,1004-iceg-morphology-classification-using-an-analogue-vlsi-neural-network.pdf,Abstract Missing,"ICEG Morphology Classification using an
Analogue VLSI Neural Network

Richard Coggins, Marwan Jabri, Barry Flower and Stephen Pickard
Systems Engineering and Design Automation Laboratory
Department of Electrical Engineering J03,
University of Sydney, 2006, Australia.
Email: richardc@sedal.su.oz.au

Abstract
An analogue VLSI neural network has been designed and tested
to perform cardiac morphology classification tasks. Analogue techniques were chosen to meet the strict power and area requirements
of an Implantable Cardioverter Defibrillator (ICD) system. The robustness of the neural network architecture reduces the impact of
noise, drift and offsets inherent in analogue approaches. The network is a 10:6:3 multi-layer percept ron with on chip digital weight
storage, a bucket brigade input to feed the Intracardiac Electrogram (ICEG) to the network and has a winner take all circuit
at the output. The network was trained in loop and included a
commercial ICD in the signal processing path. The system has successfully distinguished arrhythmia for different patients with better
than 90% true positive and true negative detections for dangerous
rhythms which cannot be detected by present ICDs. The chip was
implemented in 1.2um CMOS and consumes less than 200nW maximum average power in an area of 2.2 x 2.2mm2.

1

INTRODUCTION

To the present time, most ICDs have used timing information from ventricular
leads only to classify rhythms which has meant some dangerous rhythms can not
be distinguished from safe ones, limiting the use of the device. Even two lead

732

Richard Coggins, Marwan Jabri, Barry Flower, Stephen Pickard

4.00

HO
3.00

2.00

I.SO

_ _ _:::::::!

Q
1.00

O.SO

Figure 1: The Morphology of ST and VT retrograde 1:1.

atrial/ventricular systems fail to distinguish some rhythms when timing information alone is used [Leong and Jabri, 1992]. A case in point is the separation of Sinus Tachycardia (ST) from Ventricular Tachycardia with 1:1 retrograde conduction.
ST is a safe arrhythmia which may occur during vigorous exercise and is characterised by a heart rate of approximately 120 beats/minute. VT retrograde 1:1 also
occurs at the same low rate but can be a potentially fatal condition. False negative
detections can cause serious heart muscle injury while false positive detections deplete the batteries, cause patient suffering and may lead to costly transplantation
of the device. Figure 1 shows however, the way in which the morphology changes
on the ventricular lead for these rhythms. Note, that the morphology change is
predominantly in the ""QRS complex"" where the letters QRS are the conventional
labels for the different points in the conduction cycle during which the heart is
actually pumping blood.
For a number of years, researchers have studied template matching schemes in order
to try and detect such morphology changes. However, techniques such as correlation
waveform analysis [Lin et. al., 1988], though quite successful are too computationally intensive to meet power requirements. In this paper, we demonstrate that
an analogue VLSI neural network can detect such morphology changes while still
meeting the strict power and area requirements of an implantable system. The
advantages of an analogue approach are born out when one considers that an energy efficient analogue to digital converter such as [Kusumoto et. al., 1993] uses
1.5nJ per conversion implying 375nW power consumption for analogue to digital
conversion of the ICEG alone. Hence, the integration of a bucket brigade device and
analogue neural network provides a very efficient way of interfacing to the analogue
domain. Further, since the network is trained in loop with the ICD in real time,
the effects of device offsets, noise, QRS detection jitter and signal distortion in the
analogue circuits are largely alleviated.
The next section discusses the chip circuit designs. Section 3 describes the method

ICEG Morphology Classification Using an Analogue VLSI Neural Network

733

AowAcId. . .

1axl Syna.... AIRy

""-

Column
AoIcIr.-

I

o.ta Reglsl...

IClkcMmux

I

Bu1I...

I WTAI

10 DOD DO

Figure 2: Floor Plan and Photomicrograph of the chip
used to train the network for the morphology classification task. Section 4 describes
the classifier performance on seven patients with arrhythmia which can not be
distinguished using the heart rate only. Section 5 summarises the results, remaining
problems and future directions for the work .

2

ARCHITECTURE

The neural network chip consists of a 10:6:3 multilayer perceptron, an input bucket
brigade device (BBD) and a winner take all (WTA) circuit at the output. A floor
plan and photomicrograph of the chip appears in figure 2. The BBD samples the
incoming ICEG at a rate of 250Hz. For three class problems, the winner take all
circuit converts the winning class to a digital signal. For the two class problem
considered in this paper , a simple thresholding function suffices. The following
subsections briefly describe the functional elements of the chip . The circuit diagrams
for the chip building blocks appear in figure 3.

2.1

BUCKET BRIGADE DEVICE

One stage of the bucket brigade circuit is shown in figure 3. The BBD uses a
two phase clock to shift charge from cell to cell and is based on a design by
Leong [Leong, 1992] . The BBD operates by transferring charge deficits from S
to D in each of the cells. PHIl and PHI2 are two phase non-overlapping clocks.
The cell is buffered from the synapse array to maintain high charge transfer efficiency. A sample and hold facility is provided to store the input on the gates of the
synapses. The BBD clocks are generated off chip and are controlled by the QRS
complex detector in the lCD.

2.2

SYNAPSE

This synapse has been used on a number of neural network chips previously.
e.g . [Coggins et. al., 1994] . The synapse has five bits plus sign weight storage which

734

Richard Coggins, Marwan Jabri, Barry Flower, Stephen Pickard

NEURON

.-----------------------------------------------------------,,,
,,
~ !
BUJIOIII'

00

BUCKET BRIGADE ClLL

""

Figure 3: Neuron, Bucket Brigade and Synapse Circuit Diagrams.
sets the bias to a differential pair which performs the multiplication. The bias references for the weights are derived from a weighted current source in the corner of
the chip. A four quadrant multiplication is achieved by the four switches at the top
of the differential pair.

2.3

NEURON

Due to the low power requirements, the bias currents of the synapse arrays are of
the order of hundreds of nano amps, hence the neurons must provide an effective
resistance of many mega ohms to feed the next synapse layer while also providing
gain control. Without special high resistance polysilicon, simple resistive neurons
use prohibitive area, However, for larger networks with fan-in much greater than
ten, an additional problem of common mode cancellation is encountered, That is,
as the fan-in increases, a larger common mode range is required or a cancellation
scheme using common mode feedback is needed.
The neuron of figure 3 implements such a cancellation scheme, The mirrors MO/M2
and Ml/M3 divide the input current and facilitate the sum at the drain of M7.
M7/M8 mirrors the sum so that it may be split into two equal currents by the
mirrors formed by M4, M5 and M6 which are then subtracted from the input
currents. Thus, the differential voltage vp - Vm is a function of the transistor
transconductances, the common mode input current and the feedback factor , The
gain of the neuron can be controlled by varying the width to length ratio of the
mirror transistors MO and Ml. The implementation in this case allows seven gain
combinations, using a three bit RAM cell to store the gain,

ICEG Morphology Classification Using an Analogue VLSI Neural Network

735

Implantable
C.cio?erlor

DefibrillalOr

RunnngMUME

Ne .....1
Nelwa'1<
Chip

Figure 4: Block Diagram of the Training and Testing System.
The importance of a common mode cancellation scheme for large networks can
be seen when compared to the straight forward approach of resistive or switched
capacitor neurons. This may be illustrated by considering the energy usage of
the two approaches. Firstly, we need to define the required gain of the neuron
as a function of its fan-in . If we assume that useful inputs to the network are
mostly sparse, i.e. with a small fraction of non-zero values, then the gain is largely
independent of the fan-in, yet the common mode signal increases linearly with fanin. For the case of a neuron which does not cancel the common mode, the power
supply voltage must be increased to accommodate the common mode signal, thus
leading to a quadratic increase in energy use with fan-in. A common mode cancelling
neuron on the other hand , suffers only a linear increase in energy use with fan-in
since extra voltage range is not required and the increased energy use arises only
due to the linear increase in common mode current.

3

TRAINING SYSTEM

The system used to train and test the neural network is shown in figure 4. Control
of training and testing takes place on the PC. The PC uses a PC-LAB card to
provide analogue and digital I/O . The PC plays the ICEG signal to the input of
the commercial ICD in real time. Note, that the PC is only required for initially
training the network and in this case as a source of the heart signal. The commercial
ICD performs the function of QRS complex detection using analogue circuits. The
QRS complex detection signal is then used to freeze the BBD clocks of the chip, so
that a classification can take place.
When training, a number of examples of the arrhythmia to be classified are selected
from a single patient data base recorded during an electrophysiological study and
previously classified by a cardiologist. Since most of the morphological information
is in the QRS complex, only these segments of the data are repeatedly presented to

736

Richard Coggins. Marwan Jabri. Barry Flower. Stephen Pickard

Patient
1
2
3
4
5
6
7

% Training Attempts Converged
Run ~
Run 1

H=3
80
80
0
60
100
100
80

H= 6
10
100
0
10
80
40
100

H=3
60
0
0
40
0
60
40

H=6
60
10
10
40
60
60
100

Average
Iterations
62
86
101
77
44
46
17

Table 1: Training Performance of the system on seven patients.
the network. The weights are adjusted according to the training algorithm running
on the PC using the analogue outputs of the network to reduce the output error .
The PC writes weights to the chip via the digital I/Os of the PC-LAB card and the
serial weight bus of network. The software package implementing the training and
testing, called MUME [Jabri et. al ., 1992], provides a suite of training algorithms
and control options. Online training was used due to its success in training small
networks and because the presentation of the QRS complexes to the network was
the slowest part of the training procedure. The algorithm used for weight updates
in this paper was summed weight node perturbation [Flower and Jabri, 1993].
The system was trained on seven different patients separately all of whom had
VT with 1: 1 retrograde conduction. Note, that patient independent training has
been tried but with mixed results [Tinker, 1992] . Table 1 summarises the training
statistics for the seven patients. For each patient and each architecture, five training
runs were performed starting from a different random initial weight set. Each
of the patients was trained with eight of each class of arrhythmia. The network
architecture used was 10:H:1, where H is the number of hidden layer neurons and
the unused neurons being disabled by setting their input weights to zero. Two sets
of data were collected denoted Run 1 and Run 2. Run 1 corresponded to output
target values of ?0.6V within margin 0.45V and Run 2 to output target values of
?0.2V within margin 0.05V. A training attempt was considered to have converged
when the training set was correctly classified within two hundred training iterations.
Once the morphologies to be distinguished have been learned for a given patient,
the remainder of the patient data base is played back in a continuous stream and
the outputs of the classifier at each QRS complex are logged and may be compared
to the classifications of a cardiologist. The resulting generalisation performance is
discussed in the next section.

4

MORPHOLOGY CLASSIFIER GENERALISATION
PERFORMANCE

Table 2 summarises the generalisation performance of the system on the seven
patients for the training attempts which converged. Most of the patients show a
correct classification rate better than 90% for at least one architecture on one of the

ICEG Morphology Classification Using an Analogue VLSI Neural Network

Patient
1
2
3
4
5
6
7

No. of
Complexes
ST
VT
440
61
57
94
67
146
166
65
61
96
61
99
28
80

1
2
3
4
5
6
7

440
94
67
166
61
61
28

61
57
146
65
96
99
80

737

% Correct Classifications Run 1
H = 6
H - i3
VT
ST
ST
VT
89?10 89?3
58?0
99?0
99?1
99?1
100?0 99?1
66?44 76?37
99?1
50?3
82?1 75?13
89?9
94?6
84?8
97?1
90?5
99?1
97?3
98?5
99?1
99?1
% Correct Classifications Run 2
86?14 99?1
88?2
99?1
94?6
94?3
84?2
99?1
76?18 59?2
87?7 100?0
88?2
49?5
84?1
82?5
92?6 90?10
99?1
99?1
94?3
99?0
94?3
92?3

Table 2: Generalisation Performance of the system on seven patients.
runs, whereas, a timing based classifier can not separate these arrhythmia at all.
For each convergent weight set the network classified the test set five times. Thus,
the ""% Correct"" columns denote the mean and standard deviation of the classifier
performance with respect to both training and testing variations. By duty cycling
the bias to the network and buffers, the chip dissipates less than 200n W power for
a nominal heart rate of 120 beats/minute during generalisation.

5

DISCUSSION

Referring to table 1 we see that the patient 3 data was relatively difficult to train.
However, for the one occasion when training converged generalisation performance
was quite acceptable. Inspection of this patients data showed that typically, the
morphologies of the two rhythms were very similar. The choice of output targets,
margins and architecture appear to be patient dependent and possibly interacting
factors. Although larger margins make training easier for some patients they appear
to also introduce more variability in generalisation performance. This may be due
to the non-linearity of the neuron circuit. Further experiments are required to
optimise the architecture for a given patient and to clarify the effect of varying
targets, margins and neuron gain. Penalty terms could also be added to the error
function to minimise the possibility of missed detections of the dangerous rhythm.
The relatively slow rate of the heart results in the best power consumption being
obtained by duty cycling the bias currents to the synapses and the buffers. Hence,
the bias settling time of the weighted current source is the limiting factor for reducing power consumption further for this design. By modifying the connection of the
current source to the synapses using a bypassing technique to reduce transients in

Riclulrd Coggins, Marwan Jabri, Barry Flower, Stephen Pickard

738

the weighted currents, still lower power consumption could be achieved.

6

CONCLUSION

The successful classification of a difficult cardiac arrhythmia problem has been
demonstrated using. an analogue VLSI neural network approach. Furthermore, the
chip developed has shown very low power consumption of less than 200n W, meeting the requirements of an implantable system. The chip has performed well, with
over 90% classification performance for most patients studied and has proved to be
robust when the real world influence of analogue QRS detection jitter is introduced
by a commercial implantable cardioverter defibrillator placed in the signal path to
the classifier.
Acknowledgements

The authors acknowledge the funding for the work in this paper provided under
Australian Generic Technology Grant Agreement No. 16029 and thank Dr. Phillip
Leong of the University of Sydney and Dr. Peter Nickolls of Telectronics Pacing
Systems Ltd., Australia for their helpful suggestions and advice.
References

[Castro et. al., 1993] H.A. Castro, S.M. Tam, M.A. Holler, ""Implementation and
Performance of an analogue Nonvolatile Neural Network,"" Analogue Integrated
Circuits and Signal Processing, vol. 4(2), pp. 97-113, September 1993.
[Lin et. al., 1988] D. Lin, L.A. Dicarlo, and J .M. Jenkins, ""Identification of Ventricular Tachycardia using Intracavitary Electrograms: analysis of time and frequency domain patterns,"" Pacing (3 Clinical Electrophysiology, pp. 1592-1606,
November 1988.
[Leong, 1992] P.H.W. Leong, Arrhythmia Classification Using Low Power VLSI,
PhD Thesis, University of Sydney, Appendix B, 1992.
[ Kusumoto et. al., 1993] K. Kusumoto et. al., ""A lObit 20Mhz 30mW Pipelined
Interpolating ADC,"" ISSCC, Digest of Technical Papers, pp. 62-63, 1993.
[Leong and Jabri, 1992] P.H.W. Leong and M. Jabri, ""MATIC - An Intracardiac Tachycardia Classification System"", Pacing (3 Clinical Electrophysiology,
September 1992.
[Coggins et. al., 1994] R.J. Coggins and M.A. Jabri, ""WATTLE: A Trainable Gain
Analogue VLSI Neural Network"", NIPS6, Morgan Kauffmann Publishers, 1994.
[Jabri et. al., 1992] M.A. Jabri, E.A. Tinker and L. Leerink, ""MUME- A MultiNet-Multi-Architecture Neural Simulation Environment"", Neural Network Simulation Environments, Kluwer Academic Publications, January, 1994.
[Flower and Jabri, 1993] B. Flower and M. Jabri, ""Summed Weight Neuron Perturbation: an O(N) improvement over Weight Perturbation,"" NIPS5, Morgan
Kauffmann Publishers, pp. 212-219, 1993.
[Tinker, 1992] E.A. Tinker, ""The SPASM Algorithm for Ventricular Lead Timing and Morphology Classification,"" SEDAL ICEG-RPT-016-92, Department of
Electrical Engineering, University of Sydney, 1992.

"
1005,1994,Real-Time Control of a Tokamak Plasma Using Neural Networks,,1005-real-time-control-of-a-tokamak-plasma-using-neural-networks.pdf,Abstract Missing,"Real-Time Control of a Tokamak Plasma
Using Neural Networks

Chris M Bishop
Neural Computing Research Group
Department of Computer Science
Aston University
Birmingham, B4 7ET, U.K.
c.m .bishop@aston .ac .uk

Paul S Haynes, Mike E U Smith, Tom N Todd,
David L Trotman and Colin G Windsor
AEA Technology, Culham Laboratory,
Oxfordshire OX14 3DB
(Euratom/UKAEA Fusion Association)

Abstract
This paper presents results from the first use of neural networks
for the real-time feedback control of high temperature plasmas in
a tokamak fusion experiment. The tokamak is currently the principal experimental device for research into the magnetic confinement approach to controlled fusion. In the tokamak, hydrogen
plasmas, at temperatures of up to 100 Million K, are confined
by strong magnetic fields. Accurate control of the position and
shape of the plasma boundary requires real-time feedback control
of the magnetic field structure on a time-scale of a few tens of microseconds. Software simulations have demonstrated that a neural
network approach can give significantly better performance than
the linear technique currently used on most tokamak experiments.
The practical application of the neural network approach requires
high-speed hardware, for which a fully parallel implementation of
the multilayer perceptron, using a hybrid of digital and analogue
technology, has been developed.

1008

1

C. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor

INTRODUCTION

Fusion of the nuclei of hydrogen provides the energy source which powers the sun.

It also offers the possibility of a practically limitless terrestrial source of energy.
However, the harnessing of this power has proved to be a highly challenging problem. One of the most promising approaches is based on magnetic confinement of a
high temperature (10 7 - 108 Kelvin) plasma in a device called a tokamak (from the
Russian for 'toroidal magnetic chamber') as illustrated schematically in Figure 1.
At these temperatures the highly ionized plasma is an excellent electrical conductor, and can be confined and shaped by strong magnetic fields. Early tokamaks
had plasmas with circular cross-sections, for which feedback control of the plasma
position and shape is relatively straightforward. However, recent tokamaks, such as
the COMPASS experiment at Culham Laboratory, as well as most next-generation
tokamaks, are designed to produce plasmas whose cross-sections are strongly noncircular. Figure 2 illustrates some of the plasma shapes which COMPASS is designed to explore. These novel cross-sections provide substantially improved energy
confinement properties and thereby significantly enhance the performance of the
tokamak.

z

R

Figure 1: Schematic cross-section of a tokamak experiment showing the toroidal vacuum vessel (outer D-shaped curve) and plasma
(shown shaded). Also shown are the radial (R) and vertical (Z) coordinates. To a good approximation, the tokamak can be regarded
as axisymmetric about the Z-axis, and so the plasma boundary can
be described by its cross-sectional shape at one particular toroidal
location.
Unlike circular cross-section plasmas, highly non-circular shapes are more difficult to
produce and to control accurately, since currents through several control coils must
be adjusted simultaneously. Furthermore, during a typical plasma pulse, the shape
must evolve, usually from some initial near-circular shape. Due to uncertainties
in the current and pressure distributions within the plasma, the desired accuracy
for plasma control can only be achieved by making real-time measurements of the
position and shape of the boundary, and using error feedback to adjust the currents
in the control coils.
The physics of the plasma equilibrium is determined by force balance between the

1009

Real-Time Control of Tokamak Plasma Using Neural Networks

circle

ellipse

O-shape

bean

Figure 2: Cross-sections of the COMPASS vacuum vessel showing
some examples of potential plasma shapes. The solid curve is the
boundary of the vacuum vessel, and the plasma is shown by the
shaded regions.

thermal pressure of the plasma and the pressure of the magnetic field, and is relatively well understood. Particular plasma configurations are described in terms
of solutions of a non-linear partial differential equation called the Grad-Shafranov
(GS) equation. Due to the non-linear nature of this equation, a general analytic
solution is not possible. However, the GS equation can be solved by iterative numerical methods, with boundary conditions determined by currents flowing in the
external control coils which surround the vacuum vessel. On the tokamak itself it
is changes in these currents which are used to alter the position and cross-sectional
shape of the plasma. Numerical solution of the GS equation represents the standard technique for post-shot analysis of the plasma, and is also the method used
to generate the training dataset for the neural network, as described in the next
section. However , this approach is computationally very intensive and is therefore
unsuitable for feedback control purposes.
For real-time control it is necessary to have a fast (typically:::; 50J.lsec.) determination of the plasma boundary shape. This information can be extracted from a
variety of diagnostic systems , the most important being local magnetic measurements taken at a number of points around the perimeter of the vacuum vessel.
Most tokamaks have several tens or hundreds of small pick up coils located at carefully optimized points around the torus for this purpose. We shall represent these
magnetic signals collectively as a vector m .
For a large class of equilibria, the plasma boundary can be reasonably well represented in terms of a simple parameterization, governed by an angle-like variable B,
given by

R(B)
Z(B)

Ro + a cos(B + 8 sinB)
Zo + a/\,sinB

where we have defined the following parameters

(1)

1010

Ro
Zo
a
K

6

C. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor

radial distance of the plasma center from the major axis of the torus,
vertical distance of the plasma center from the torus midplane,
minor radius measured in the plane Z = Zo,
elongation,
triangularity.

We denote these parameters collectively by Yk. The basic problem which has to be
addressed, therefore, is to find a representation for the (non-linear) mapping from
the magnetic signals m to the values of the geometrical parameters Yk, which can
be implemented in suitable hardware for real-time control.
The conventional approach presently in use on many tokamaks involves approximating the mapping between the measured magnetic signals and the geometrical
parameters by a single linear transformation. However, the intrinsic non-linearity
of the mappings suggests that a representation in terms of feedforward neural networks should give significantly improved results (Lister and Schnurrenberger, 1991;
Bishop et a/., 1992; Lagin et at., 1993). Figure 3 shows a block diagram of the
control loop for the neural network approach to tokamak equilibrium control.
Neural

Network

Figure 3: Block diagram of the control loop used for real-time
feedback control of plasma position and shape.

2

SOFTWARE SIMULATION RESULTS

The dataset for training and testing the network was generated by numerical solution of the GS equation using a free-boundary equilibrium code. The data base
currently consists of over 2,000 equilibria spanning the wide range of plasma positions and shapes available in COMPASS. Each equilibrium configuration takes
several minutes to generate on a fast workstation. The boundary of each configuration is then fitted using the form in equation 1, so that the equilibria are labelled
with the appropriate values of the shape parameters. Of the 120 magnetic signals
available on COMPASS which could be used to provide inputs to the network, a

1011

Real-Time Control o/Tokamak PLasma Using Neural Networks

subset of 16 has been chosen using sequential forward selection based on a linear
representation for the mapping (discussed below) .
It is important to note that the transformation from magnetic signals to flux surface
parameters involves an exact linear invariance. This follows from the fact that, if all
of the currents are scaled by a constant factor, then the magnetic fields will be scaled
by this factor, and the geometry of the plasma boundary will be unchanged . It is
important to take advantage of this prior knowledge and to build it into the network
structure, rather than force the network to learn it by example. We therefore
normalize the vector m of input signals to the network by dividing by a quantity
proportional to the total plasma current. Note that this normalization has to be
incorporated into the hardware implementation of the network, as will be discussed
in Section 3.
1.2

4
01

2

2

01

c

c
.5.

0-

~
:E

.5.
CIS

:E

1iI

~
::J

?

1iI
CD

-2

go.8

.5.
0-

?

CIS

:E

1iI 0 .4
CD

c
::J

c

::J

-2

-4
Database

?
Database

1.2

4
~

~CD

Z

~
:::I

CD

z

.2
Database

2

~O.8
~

?

CD

z

~O.4

-2

:::I

CD

Z

?

-4
Database

Database

.2
Database

Figure 4: Plots of the values from the test set versus the values
predicted by the linear mapping for the 3 equilibrium parameters,
together with the corresponding plots for a neural network with 4
hidden units.

The results presented in this paper are based on a multilayer perceptron architecture
having a single layer of hidden units with 'tanh' activation functions , and linear
output units. Networks are trained by minimization of a sum-of-squares error using
a standard conjugate gradients optimization algorithm, and the number of hidden

J012

C. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor

units is optimized by measuring performance with respect to an independent test
set. Results from the neural network mapping are compared with those from the
optimal linear mapping, that is the single linear transformation which minimizes
the same sum-of-squares error as is used in the neural network training algorithm,
as this represents the method currently used on a number of present day tokamaks .
Initial results were obtained on networks having 3 output units, corresponding to
the values of vertical position ZQ, major radius RQ, and elongation K; these being
parameters which are of interest for real-time feedback control. The smallest normalized test set error of 11.7 is obtained from the network having 16 hidden units.
By comparison, the optimal linear mapping gave a normalized test set error of 18.3.
This represents a reduction in error of about 30% in going from the linear mapping
to the neural network. Such an improvement, in the context of this application , is
very significant.
For the experiments on real-time feedback control described in Section 4 the currently available hardware only permitted networks having 4 hidden units, and so we
consider the results from this network in more detail. Figure 4 shows plots of the
network predictions for various parameters versus the corresponding values from
the test set portion of the database. Analogous plots for the optimal linear map
predictions versus the database values are also shown. Comparison of the corresponding figures shows the improved predictive capability of the neural network,
even for this sub-optimal network topology.

3

HARDWARE IMPLEMENTATION

The hardware implementation of the neural network must have a bandwidth of 2:
20 kHz in order to cope with the fast timescales of the plasma evolution. It must
also have an output precision of at least (the the analogue equivalent of) 8 bits in
order to ensure that the final accuracy which is attainable will not be limited by the
hardware system. We have chosen to develop a fully parallel custom implementation
of the multilayer perceptron, based on analogue signal paths with digitally stored
synaptic weights (Bishop et al., 1993). A VME-based modular construction has
been chosen as this allows flexibility in changing the network architecture, ease of
loading network weights, and simplicity of data acquisition. Three separate types
of card have been developed as follows:
? Combined 16-input buffer and signal normalizer.
This provides an analogue hardware implementation of the input normalization described earlier.
? 16 x 4 matrix multiplier
The synaptic weights are produced using 12 bit frequency-compensated
multiplying DACs (digital to analogue converters) which can be configured
to allow 4-quadrant multiplication of analogue signals by a digitally stored
number.
? 4-channel sigmoid module
There are many ways to produce a sigmoidal non-linearity, and we have
opted for a solution using two transistors configured as along-tailed-pair,

Real-Time Control of Tokamak Plasma Using Neural Networks

1013

to generate a 'tanh ' sigmoidal transfer characteristic. The principal drawback of such an approach is the strong temperature sensitivity due to the
appearance of temperature in the denominator of the exponential transistor
transfer characteristic. An elegant solution to this problem has been found
by exploiting a chip containing 5 transistors in close thermal contact. Two
of the transistors form the long-tailed pair, one of the transistors is used
as a heat source, and the remaining two transistors are used to measure
temperature. External circuitry provides active thermal feedback control,
and stability to changes in ambient temperature over the range O?C to 50?C
is found to be well within the acceptable range.
The complete network is constructed by mounting the appropriate combination
of cards in a VME rack and configuring the network topology using front panel
interconnections. The system includes extensive diagnostics, allowing voltages at
all key points within the network to be monitored as a function of time via a series
of multiplexed output channels.

4

RESULTS FROM REAL-TIME FEEDBACK CONTROL

Figure 5 shows the first results obtained from real-time control of the plasma in
the COMPASS tokamak using neural networks. The evolution of the plasma elongation, under the control of the neural network, is plotted as a function of time
during a plasma pulse. Here the desired elongation has been preprogrammed to
follow a series of steps as a function of time. The remaining 2 network outputs
(radial position Ro and vertical position Zo) were digitized for post-shot diagnosis ,
but were not used for real-time control. The solid curve shows the value of elongation given by the corresponding network output, and the dashed curve shows the
post-shot reconstruction of the elongation obtained from a simple 'filament' code,
which gives relatively rapid post-shot plasma shape reconstruction but with limited
accuracy. The circles denote the elongation values given by the much more accurate
reconstructions obtained from the full equilibrium code. The graph clearly shows
the network generating the required elongation signal in close agreement with the
reconstructed values. The typical residual error is of order 0.07 on elongation values
up to around 1.5. Part of this error is attributable to residual offset in the integrators used to extract magnetic field information from the pick-up coils, and this is
currently being corrected through modifications to the integrator design. An additional contribution to the error arises from the restricted number of hidden units
available with the initial hardware configuration. While these results represent the
first obtained using closed loop control, it is clear from earlier software modelling of
larger network architectures (such as 32- 16-4) that residual errors of order a few %
should be attainable. The implementation of such larger networks is being persued,
following the successes with the smaller system.
Acknowledgements
We would like to thank Peter Cox, Jo Lister and Colin Roach for many useful
discussions and technical contributions. This work was partially supported by the
UK Department of Trade and Industry.

C. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor

1014

1.8
shot 9576

c:

o

~
14
C)
?
c:

o

as
1.0
0.0

0.1

0.2

time (sec.)
Figure 5: Plot of the plasma elongation K. as a function of time
during shot no. 9576 on the COMPASS tokamak, during which the
elongation was being controlled in real-time by the neural network.

References

Bishop C M, Cox P, Haynes P S, Roach C M, Smith M E U, Todd T N and Trotman
D L, 1992. A neural network approach to tokamak equilibrium control. In Neural
Network Applications, Ed. J G Taylor, Springer Verlag, 114-128.
Bishop C M, Haynes P S, Roach C M, Smith ME U, Todd T N, and Trotman D L.
1993. Hardware implementation of a neural network for plasma position control in
COMPASS-D. In Proceedings of the 17th. Symposium on Fusion Technology, Rome,
Italy. 2 997-1001.
Lagin L, Bell R, Davis S, Eck T, Jardin S, Kessel C, Mcenerney J, Okabayashi
M, Popyack J and Sauthoff N. 1993. Application of neural networks for real-time
calculations of plasma equilibrium parameters for PBX-M, In Proceedings of the
17th. Symposium on Fusion Technology, Rome, Italy. 21057-106l.
Lister J Band Schnurrenberger H. 1991. Fast non-linear extraction of plasma
parameters using a neural network mapping. Nuclear Fusion. 31, 1291-1300.

"
1006,1994,Pulsestream Synapses with Non-Volatile Analogue Amorphous-Silicon Memories,,1006-pulsestream-synapses-with-non-volatile-analogue-amorphous-silicon-memories.pdf,Abstract Missing,"Real-Time Control of a Tokamak Plasma
Using Neural Networks

Chris M Bishop
Neural Computing Research Group
Department of Computer Science
Aston University
Birmingham, B4 7ET, U.K.
c.m .bishop@aston .ac .uk

Paul S Haynes, Mike E U Smith, Tom N Todd,
David L Trotman and Colin G Windsor
AEA Technology, Culham Laboratory,
Oxfordshire OX14 3DB
(Euratom/UKAEA Fusion Association)

Abstract
This paper presents results from the first use of neural networks
for the real-time feedback control of high temperature plasmas in
a tokamak fusion experiment. The tokamak is currently the principal experimental device for research into the magnetic confinement approach to controlled fusion. In the tokamak, hydrogen
plasmas, at temperatures of up to 100 Million K, are confined
by strong magnetic fields. Accurate control of the position and
shape of the plasma boundary requires real-time feedback control
of the magnetic field structure on a time-scale of a few tens of microseconds. Software simulations have demonstrated that a neural
network approach can give significantly better performance than
the linear technique currently used on most tokamak experiments.
The practical application of the neural network approach requires
high-speed hardware, for which a fully parallel implementation of
the multilayer perceptron, using a hybrid of digital and analogue
technology, has been developed.

1008

1

C. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor

INTRODUCTION

Fusion of the nuclei of hydrogen provides the energy source which powers the sun.

It also offers the possibility of a practically limitless terrestrial source of energy.
However, the harnessing of this power has proved to be a highly challenging problem. One of the most promising approaches is based on magnetic confinement of a
high temperature (10 7 - 108 Kelvin) plasma in a device called a tokamak (from the
Russian for 'toroidal magnetic chamber') as illustrated schematically in Figure 1.
At these temperatures the highly ionized plasma is an excellent electrical conductor, and can be confined and shaped by strong magnetic fields. Early tokamaks
had plasmas with circular cross-sections, for which feedback control of the plasma
position and shape is relatively straightforward. However, recent tokamaks, such as
the COMPASS experiment at Culham Laboratory, as well as most next-generation
tokamaks, are designed to produce plasmas whose cross-sections are strongly noncircular. Figure 2 illustrates some of the plasma shapes which COMPASS is designed to explore. These novel cross-sections provide substantially improved energy
confinement properties and thereby significantly enhance the performance of the
tokamak.

z

R

Figure 1: Schematic cross-section of a tokamak experiment showing the toroidal vacuum vessel (outer D-shaped curve) and plasma
(shown shaded). Also shown are the radial (R) and vertical (Z) coordinates. To a good approximation, the tokamak can be regarded
as axisymmetric about the Z-axis, and so the plasma boundary can
be described by its cross-sectional shape at one particular toroidal
location.
Unlike circular cross-section plasmas, highly non-circular shapes are more difficult to
produce and to control accurately, since currents through several control coils must
be adjusted simultaneously. Furthermore, during a typical plasma pulse, the shape
must evolve, usually from some initial near-circular shape. Due to uncertainties
in the current and pressure distributions within the plasma, the desired accuracy
for plasma control can only be achieved by making real-time measurements of the
position and shape of the boundary, and using error feedback to adjust the currents
in the control coils.
The physics of the plasma equilibrium is determined by force balance between the

1009

Real-Time Control of Tokamak Plasma Using Neural Networks

circle

ellipse

O-shape

bean

Figure 2: Cross-sections of the COMPASS vacuum vessel showing
some examples of potential plasma shapes. The solid curve is the
boundary of the vacuum vessel, and the plasma is shown by the
shaded regions.

thermal pressure of the plasma and the pressure of the magnetic field, and is relatively well understood. Particular plasma configurations are described in terms
of solutions of a non-linear partial differential equation called the Grad-Shafranov
(GS) equation. Due to the non-linear nature of this equation, a general analytic
solution is not possible. However, the GS equation can be solved by iterative numerical methods, with boundary conditions determined by currents flowing in the
external control coils which surround the vacuum vessel. On the tokamak itself it
is changes in these currents which are used to alter the position and cross-sectional
shape of the plasma. Numerical solution of the GS equation represents the standard technique for post-shot analysis of the plasma, and is also the method used
to generate the training dataset for the neural network, as described in the next
section. However , this approach is computationally very intensive and is therefore
unsuitable for feedback control purposes.
For real-time control it is necessary to have a fast (typically:::; 50J.lsec.) determination of the plasma boundary shape. This information can be extracted from a
variety of diagnostic systems , the most important being local magnetic measurements taken at a number of points around the perimeter of the vacuum vessel.
Most tokamaks have several tens or hundreds of small pick up coils located at carefully optimized points around the torus for this purpose. We shall represent these
magnetic signals collectively as a vector m .
For a large class of equilibria, the plasma boundary can be reasonably well represented in terms of a simple parameterization, governed by an angle-like variable B,
given by

R(B)
Z(B)

Ro + a cos(B + 8 sinB)
Zo + a/\,sinB

where we have defined the following parameters

(1)

1010

Ro
Zo
a
K

6

C. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor

radial distance of the plasma center from the major axis of the torus,
vertical distance of the plasma center from the torus midplane,
minor radius measured in the plane Z = Zo,
elongation,
triangularity.

We denote these parameters collectively by Yk. The basic problem which has to be
addressed, therefore, is to find a representation for the (non-linear) mapping from
the magnetic signals m to the values of the geometrical parameters Yk, which can
be implemented in suitable hardware for real-time control.
The conventional approach presently in use on many tokamaks involves approximating the mapping between the measured magnetic signals and the geometrical
parameters by a single linear transformation. However, the intrinsic non-linearity
of the mappings suggests that a representation in terms of feedforward neural networks should give significantly improved results (Lister and Schnurrenberger, 1991;
Bishop et a/., 1992; Lagin et at., 1993). Figure 3 shows a block diagram of the
control loop for the neural network approach to tokamak equilibrium control.
Neural

Network

Figure 3: Block diagram of the control loop used for real-time
feedback control of plasma position and shape.

2

SOFTWARE SIMULATION RESULTS

The dataset for training and testing the network was generated by numerical solution of the GS equation using a free-boundary equilibrium code. The data base
currently consists of over 2,000 equilibria spanning the wide range of plasma positions and shapes available in COMPASS. Each equilibrium configuration takes
several minutes to generate on a fast workstation. The boundary of each configuration is then fitted using the form in equation 1, so that the equilibria are labelled
with the appropriate values of the shape parameters. Of the 120 magnetic signals
available on COMPASS which could be used to provide inputs to the network, a

1011

Real-Time Control o/Tokamak PLasma Using Neural Networks

subset of 16 has been chosen using sequential forward selection based on a linear
representation for the mapping (discussed below) .
It is important to note that the transformation from magnetic signals to flux surface
parameters involves an exact linear invariance. This follows from the fact that, if all
of the currents are scaled by a constant factor, then the magnetic fields will be scaled
by this factor, and the geometry of the plasma boundary will be unchanged . It is
important to take advantage of this prior knowledge and to build it into the network
structure, rather than force the network to learn it by example. We therefore
normalize the vector m of input signals to the network by dividing by a quantity
proportional to the total plasma current. Note that this normalization has to be
incorporated into the hardware implementation of the network, as will be discussed
in Section 3.
1.2

4
01

2

2

01

c

c
.5.

0-

~
:E

.5.
CIS

:E

1iI

~
::J

?

1iI
CD

-2

go.8

.5.
0-

?

CIS

:E

1iI 0 .4
CD

c
::J

c

::J

-2

-4
Database

?
Database

1.2

4
~

~CD

Z

~
:::I

CD

z

.2
Database

2

~O.8
~

?

CD

z

~O.4

-2

:::I

CD

Z

?

-4
Database

Database

.2
Database

Figure 4: Plots of the values from the test set versus the values
predicted by the linear mapping for the 3 equilibrium parameters,
together with the corresponding plots for a neural network with 4
hidden units.

The results presented in this paper are based on a multilayer perceptron architecture
having a single layer of hidden units with 'tanh' activation functions , and linear
output units. Networks are trained by minimization of a sum-of-squares error using
a standard conjugate gradients optimization algorithm, and the number of hidden

J012

C. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor

units is optimized by measuring performance with respect to an independent test
set. Results from the neural network mapping are compared with those from the
optimal linear mapping, that is the single linear transformation which minimizes
the same sum-of-squares error as is used in the neural network training algorithm,
as this represents the method currently used on a number of present day tokamaks .
Initial results were obtained on networks having 3 output units, corresponding to
the values of vertical position ZQ, major radius RQ, and elongation K; these being
parameters which are of interest for real-time feedback control. The smallest normalized test set error of 11.7 is obtained from the network having 16 hidden units.
By comparison, the optimal linear mapping gave a normalized test set error of 18.3.
This represents a reduction in error of about 30% in going from the linear mapping
to the neural network. Such an improvement, in the context of this application , is
very significant.
For the experiments on real-time feedback control described in Section 4 the currently available hardware only permitted networks having 4 hidden units, and so we
consider the results from this network in more detail. Figure 4 shows plots of the
network predictions for various parameters versus the corresponding values from
the test set portion of the database. Analogous plots for the optimal linear map
predictions versus the database values are also shown. Comparison of the corresponding figures shows the improved predictive capability of the neural network,
even for this sub-optimal network topology.

3

HARDWARE IMPLEMENTATION

The hardware implementation of the neural network must have a bandwidth of 2:
20 kHz in order to cope with the fast timescales of the plasma evolution. It must
also have an output precision of at least (the the analogue equivalent of) 8 bits in
order to ensure that the final accuracy which is attainable will not be limited by the
hardware system. We have chosen to develop a fully parallel custom implementation
of the multilayer perceptron, based on analogue signal paths with digitally stored
synaptic weights (Bishop et al., 1993). A VME-based modular construction has
been chosen as this allows flexibility in changing the network architecture, ease of
loading network weights, and simplicity of data acquisition. Three separate types
of card have been developed as follows:
? Combined 16-input buffer and signal normalizer.
This provides an analogue hardware implementation of the input normalization described earlier.
? 16 x 4 matrix multiplier
The synaptic weights are produced using 12 bit frequency-compensated
multiplying DACs (digital to analogue converters) which can be configured
to allow 4-quadrant multiplication of analogue signals by a digitally stored
number.
? 4-channel sigmoid module
There are many ways to produce a sigmoidal non-linearity, and we have
opted for a solution using two transistors configured as along-tailed-pair,

Real-Time Control of Tokamak Plasma Using Neural Networks

1013

to generate a 'tanh ' sigmoidal transfer characteristic. The principal drawback of such an approach is the strong temperature sensitivity due to the
appearance of temperature in the denominator of the exponential transistor
transfer characteristic. An elegant solution to this problem has been found
by exploiting a chip containing 5 transistors in close thermal contact. Two
of the transistors form the long-tailed pair, one of the transistors is used
as a heat source, and the remaining two transistors are used to measure
temperature. External circuitry provides active thermal feedback control,
and stability to changes in ambient temperature over the range O?C to 50?C
is found to be well within the acceptable range.
The complete network is constructed by mounting the appropriate combination
of cards in a VME rack and configuring the network topology using front panel
interconnections. The system includes extensive diagnostics, allowing voltages at
all key points within the network to be monitored as a function of time via a series
of multiplexed output channels.

4

RESULTS FROM REAL-TIME FEEDBACK CONTROL

Figure 5 shows the first results obtained from real-time control of the plasma in
the COMPASS tokamak using neural networks. The evolution of the plasma elongation, under the control of the neural network, is plotted as a function of time
during a plasma pulse. Here the desired elongation has been preprogrammed to
follow a series of steps as a function of time. The remaining 2 network outputs
(radial position Ro and vertical position Zo) were digitized for post-shot diagnosis ,
but were not used for real-time control. The solid curve shows the value of elongation given by the corresponding network output, and the dashed curve shows the
post-shot reconstruction of the elongation obtained from a simple 'filament' code,
which gives relatively rapid post-shot plasma shape reconstruction but with limited
accuracy. The circles denote the elongation values given by the much more accurate
reconstructions obtained from the full equilibrium code. The graph clearly shows
the network generating the required elongation signal in close agreement with the
reconstructed values. The typical residual error is of order 0.07 on elongation values
up to around 1.5. Part of this error is attributable to residual offset in the integrators used to extract magnetic field information from the pick-up coils, and this is
currently being corrected through modifications to the integrator design. An additional contribution to the error arises from the restricted number of hidden units
available with the initial hardware configuration. While these results represent the
first obtained using closed loop control, it is clear from earlier software modelling of
larger network architectures (such as 32- 16-4) that residual errors of order a few %
should be attainable. The implementation of such larger networks is being persued,
following the successes with the smaller system.
Acknowledgements
We would like to thank Peter Cox, Jo Lister and Colin Roach for many useful
discussions and technical contributions. This work was partially supported by the
UK Department of Trade and Industry.

C. Bishop, P. Haynes, M. Smith, T. Todd, D. Trotman, C. Windsor

1014

1.8
shot 9576

c:

o

~
14
C)
?
c:

o

as
1.0
0.0

0.1

0.2

time (sec.)
Figure 5: Plot of the plasma elongation K. as a function of time
during shot no. 9576 on the COMPASS tokamak, during which the
elongation was being controlled in real-time by the neural network.

References

Bishop C M, Cox P, Haynes P S, Roach C M, Smith M E U, Todd T N and Trotman
D L, 1992. A neural network approach to tokamak equilibrium control. In Neural
Network Applications, Ed. J G Taylor, Springer Verlag, 114-128.
Bishop C M, Haynes P S, Roach C M, Smith ME U, Todd T N, and Trotman D L.
1993. Hardware implementation of a neural network for plasma position control in
COMPASS-D. In Proceedings of the 17th. Symposium on Fusion Technology, Rome,
Italy. 2 997-1001.
Lagin L, Bell R, Davis S, Eck T, Jardin S, Kessel C, Mcenerney J, Okabayashi
M, Popyack J and Sauthoff N. 1993. Application of neural networks for real-time
calculations of plasma equilibrium parameters for PBX-M, In Proceedings of the
17th. Symposium on Fusion Technology, Rome, Italy. 21057-106l.
Lister J Band Schnurrenberger H. 1991. Fast non-linear extraction of plasma
parameters using a neural network mapping. Nuclear Fusion. 31, 1291-1300.

Pulsestream Synapses with Non-Volatile
Analogue Amorphous-Silicon Memories.

A.J. Holmes, A.F. Murray, S. Churcher and J. Hajto
Department of Electrical Engineering
University of Edinburgh
Edinburgh, EH9 3JL
M. J. Rose
Dept. of Applied Physics and Electronics,
Dundee University
Dundee DD14HN

Abstract
A novel two-terminal device, consisting of a thin lOooA layer of p+
a-Si:H sandwiched between Vanadium and Chromium electrodes,
exhibits a non-volatile, analogue memory action. This device stores
synaptic weights in an ANN chip, replacing the capacitor previously
used for dynamic weight storage. Two different synapse designs are
discussed and results are presented.

1

INTRODUCTION

Analogue hardware implementations of neural networks have hitherto been hampered by the lack of a straightforward (local) analogue memory capability. The
ideal storage mechanism would be compact, non-volatile, easily reprogrammable,
and would not interfere with the normal silicon chip fabrication process.
Techniques which have been used to date include resistors (these are not generally
reprogrammable, and suffer from being large and difficult to fabricate with any accuracy), dynamic capacitive storage [4] (this is compact, reprogrammable and simple,
but implies an increase in system complexity, arising from off-chip refresh circuitry),

764

A. J. Holmes, A. F. Murray, S. Churcher, J. Hajto, M. J. Rose

EEPROM (""floating gate"") memory [5] (which is compact, reprogrammable, and
non-volatile, but is slow, and cannot be reprogrammed in situ), and local digital
storage (which is non-volatile, easily programmable and simple, but consumes area
horribly).
Amorphous silicon has been used for synaptic weight storage [1, 2], but only as
either a high-resistance fixed weight medium or a binary memory.
In this paper, we demonstrate that novel amorphous silicon memory devices can be
incorporated into standard CMOS synapse circuits, to provide an analogue weight
storage mechanism which is compact, non-volatile, easily reprogrammable, and simple to implement.

2

a-Si:H MEMORY DEVICES

The a-Si:H analogue memory device [3] comprises a lOooA thick layer of amorphous
silicon (p+ a-Si:H) sandwiched between Vanadium and Chromium electrodes.
The a-Si device takes the form of a two-terminal, programmable resistor. It is an
""add-on"" to a conventional CMOS process, and does not demand that the normal
CMOS fabrication cycle be disrupted. The a-Si device sits on top of the completed
chip circuitry, making contact with the CMOS arithmetic elements via holes cut in
the protective passivation layer, as shown in Figure 1.

CMOS Passivation
Figure 1: The construction of a-Si:H Devices on a CMOS chip
After fabrication a number of electronic procedures must be performed in order to
program the device to a given resistance state.
Programming, and Pre-Programming Procedures

Before the a-Si device is usable, the following steps must be carried out:
? Forming: This is a once-only process, applied to the a-Si device in its
""virgin"" state, where it has a resistance of several MO. A series of 300ns
pulses, increasing in amplitude from 5v to 14v, is applied to the device
electrodes. This creates a vertical conducting channel or filament whose
approximate resistance is 1KO. This filament can then be programmed to
a value in the range lKO to 1 MO . The details of the physical mechanisms
are not yet fully established, but it is clear that conduction occurs through
a narrow (sub-micron) conducting channel.

Pulsestream Synapses with Non-Volatile Analogue Amorphous-Silicon Memories

765

? Write: To decrease the device's resistance, negative ""Write"", pulses are
applied.
? Erase: To increase the device's resistance, positive"" Erase"" , pulses are applied.
? Usage: Pulses below O.5v do not change the device resistance. The resistance can therefore be utilised as a weight storage medium using a voltage
of less than O.5v without causing reprogramming.
Programming pulses, which range between 2v and 5v, are typically 120ns in duration. Programming is therefore much faster than for other EEPROM (floating
gate) devices used in the same context, which use a series of 100jls pulses to set the
threshold voltage [5].
The following sections describe synapse circuits using the a-Si:H devices. These
synapses use the reprogrammable a-Si:H resistor in the place of a storage capacitor
or EEPROM cell. These new synapses were implemented on a chip referred to as
ASiTEST2, consisting of five main test blocks, each comprising of four synapses
connected to a single neuron.

3

The EPSILON based synapse

The first synapse to be designed used the a-Si:H resistor as a direct replacement for
the storage capacitor used in the EPSILON [4] synapse.

+Sv

Neuron

1
I

V

..

t.

~:l:

><!:

~

Mirror Set

E

30

a-Si => Vw

Circuitry

Original
Storage
Capacitor

O.5v

<0-----------.,...
__

...

EPSILON Synapse

Figure 2: The EPSILON Synapse with a-Si:H weight storage

In the original EPSILON chip the weight voltage was stored as a voltage on a
capacitor. In this new synapse design, shown in Figure 2, the a-Si:H resistance is
set such that the voltage drop produced by Iset is equivalent to the original weight
voltage, Vw, that was stored dynamically on the capacitor.
A new, simpler, synapse, which can be operated from a single +5v supply, was also
be included on the ASiTEST2 chip.

A. J. Holmes, A. F. Murray, S. Churcher, J. Hajto, M. J. Rose

766

4

The MkII synapse

The circuit is shown in Figure 3. The a-Si:H memory is used to store a current,
Iasi. This current is subtracted from a zero current, Isy...:z"" to give a weight current
, +/-Iw, which adds or subtracts charge from the activity capacitor, Cact, thus
implementing excitation or inhibition respectively.
For the circuit to function correctly we must limit the voltage on the activity capacitor to the range [1.5v,3.5v], to ensure that the transistors mirroring Isy_z and
Iasi remain in saturation. As Figure 3 shows, there are few reference signals and
the circuit operates from a single +5v power supply rail, in sharp contrast to many
earlier analogue neural circuits, including our own.

1v""""

+5vPWm

II .

~

881

Vsel

.r--\.

-.L

....L

*

Comparator
PWout
..rL

Cact

Vramp

~

Ov
E
;.
Mirror Set

""'E~-----------:~~

E

Synapse

Power Supplies
V5_0=5.Ov

References
Vrstv?2.5v
Isy_z=5uA

;.

Neuron

Tail Currents

Ineu=4uA

Figure 3: The MkII synapse

On first inspection the main drawback of this design would appear to be a reliance
on the accuracy with which the zero current Isy...:z, is mirrored across an entire chip.
The variation in this current means that two cells with the same synapse resistance
could produce widely differing values of Iw. However, during programming we
do not use the resistance of the a-Si:H device as a target value. We monitor the
voltage on Cact for a given PWin signal, increasing or decreasing the resistance
of the a-Si:H device until the desired voltage level is achieved.
Example: To set a weight to be the maximum positive value, we adjust the a-Si
resistance until a PWin signal of 5us, the maximum input signal, gives a voltage of
3.5v on the integration capacitor.
We are able to set the synapse weight using the whole integration range of [1.5v,3.5v]
by only closing Vsel for the desired synapse during programming. In normal operating mode all four Vsel switches will be closed so that the integration charge is
summed over all four local capacitors.

Pulsestream Synapses with Non-Volatile Analogue Amorphous-Silicon Memories

4.1

767

Example - Stability Test

As an example of the use of integration voltage as means of monitoring the resistance
of a particular synapse we have included a stability test. This was carried out on
one of the test chips which contained the MkII synapse.
The four synapses on the test chip were programmed to give different levels of
activation. The chip was then powered up for 30mins each day during a 7-day
period, and the activation levels for each synapse were measured three times.
3.5

Stability Test - PWin = 3us

,----~-_._--;--__..___r...:....,.-_._,.__-,.--.,..,.-__..__:___.,

testl

test2

test4

test3

testS

test7

test6

3

?
,.
I

t:'""
-~

1:1
?

't

,

?

~

?

- ~ -:- - ~ -:- -

25
?

2

?

~.

?

-:- - - . of - ~-:- -

..

.

I

I

?

?

{,o-:- - .
- ~-s4
.

,
.
.
'.
.?
.?
.?
.?
- - ~ - - ~ - - --:- - - ~ - - -~- - - w. -- --r s2
- - - .;. '"" - -: -011>- - :.. ~ - ~ -oGii - -:- - - - i-- ~ - ..; -sl
?
.
.
.
.
.
?

?

~ - - ~ - - -~- - - ~ - ?

I

?

I
I

,
I

?
?

?

,

?
?
I

I
?
?

10

20

30

-.
L---L- -- -~

~ -s3

,

?

,
,

?
?

?
?

?
?

?

,

I

,

?

?
I
:

I
?
:

?
I
?

?
?
?

?
?
?

40

50

60

,

70

80

?

90

Measurement Index

Figure 4: ASiTEST2- Stability Test
As figure 4 shows, the memories remain in the same resistance state (i.e retain their
programmed weight value) over the whole 7-day period. Separate experiments on
isolated devices indicate much longer hold times - of the order of months at least.

5

ASiTEST3

Recently we have received our latest, overtly neural, a-Si:H based test chip. This
contains an 8x8 array of the MkII synapses.
The circuit board for this device has been constructed and partially tested while
the ASiTEST3 chips are awaiting the deposition of the a-Si:H layers. We have been
able to use an ASiTEST2 chip containing two of the MkII synapse test blocks i.e.
8 synapses and 2 neurons to exercise much of the board's functionality.
The test board contains a simple state machine which has four different states:
? State 0: Load Input Pulsewidths into SRAM from PC.
? State 1: Apply Input Pulsewidth signals to chipl.
? State 2: Use Vramp to generate threshold function for chipl. The resulting
Pulsewidth outputs are used as the inputs to chip2, as well as being stored

A. J. Holmes, A. F. Murray, S. Churcher, J. Hajto, M. J. Rose

768

in SRAM .
? State 3: Use Vramp to generate threshold function for chip2. Read resulting
Pulsewidth Outputs into SRAM .
? State 0: Read Output Pulsewidths from SRAM into PC.
The results obtained during a typical test cycle are shown in Figure 5.
IE-- Statel --;,,;,,*1E E - - State2

-----;>~i~

State3 ---;!>~I

~r-IF~~~--r-------~---------l
4v
3v

PWin_O

2v
Iv

Ov~ . . . . . . . . .

3.5v r;;;;;""""-~,"""",,,or;:::::;;:;::;;;;:;;;::;;t---~----t--------,

~:~

....... ~;a~;""""""'""

2.Ov

~'Sig~;,id""""""""""
... ~""Li~""""""""
......

. . . . . . . . . . . . . . . . . . . . . . . ,.

....... .

.... .....

..........

.. ..

l.~
e?_e

_____ ...... ___

?

_____

?

____________ . . . . . . . .

_____

????

____

..........

.

5v
4v
3v

2v

:;; ~,...,................-..--t;?.~...:...~__--! ..........~~.n-.-~~~~~~......J
IS.

10.

Figure 5: ASiTEST3 Board Scope Waveforms
As this figure shows different ramp signals, corresponding to different threshold
functions, can be applied to chipl and chip2 neurons.
10.0

Single Buffer PulscWidth Sweeps

.----.,..----r------.----r----.------,

9.0
8.0

!

7.0 -

~

5'O~~~~~~~-~~-+++~~~~N~~~

~

i6.o~

~----

J::
2D

Neal-Syal
N~~JIII

1.0

N~~ya2

o.oL---~--~- --~--?

o

0.5

1.0

1.5

2.0

2.5

3.0

Pulscwldlh Input [WI)

Figure 6: ASiTEST3 Board - MkII Synapse Characteristic
While the signals shown in Figure 5 appear noisy the multiplier characteristic that
the chip produces is still admirably linear, as shown in Figure 6. In this experiment
all eight synapses on a test chip were programmed into different resistance states
and PWin was swept from 0 to 3us.

Pulsestream Synapses with Non- Volatile Analogue Amorphous-Silicon Memories

6

769

Conclusions

We have demonstrated the use of novel a-Si:H analogue memory devices as a means
of storing synaptic weights in a Pulsewidth ANN. We have also demonstrated the
operation of an interface board which allows two 8x8 ANN chips, operating as a
two layer network, to be controlled by a simple PC interface card.
This technology is most suitable for small networks in, for example, remote control and other embedded-system applications where cost and power considerations
favour a single all-inclusive ANN chip with non-volatile, but programmable weights.
Another possible application of this technology is in large networks constructed
using Thin Film Technology(TFT). If TFT's were used in place of the CMOS transistors then the area constraint imposed by crystalline silicon would be removed,
allowing truly massively parallel networks to be integrated.
In summary - the a-Si:H analogue memory devices described in this paper provide a
route to an analogue, non-volatile and fast synaptic weight storage medium. At the
present time neither the programming nor storage mechanisms are fully understood
making it difficult to compare this new device with more established technologies
such as the ubiquitous Floating-Gate EEPROM technique. Current research is
focused on firstly, improving the yield on the a-Si:H device which is unacceptably
low at present, a demerit that we attribute to imperfections in the a-Si fabrication
process and secondly, improving understanding of the device physics and hence the
programming and storage mechanisms.
Acknowledgements
This research has been jointly funded by BT, and EPSRC (formerly SERC), the
Engineering and Physical Sciences Research Council.

References
[1] W. Hubbard et al.(1986) Electronic Neural Networks AlP Conference Proceedings - Snowbird 1986 :227-234
[2] H.P. Graf (1986) VLSI Implementation of a NN memory with several hundreds
of neurons AlP Conference Proceedings - Snowbird 1986 :182-187.
[3] M.J. Rose et al (1989) Amorphous Silicon Analogue Memory Devices Journal
of Non-Crystalline Solids 1(115):168-170
[4] A.Hamilton et al. (1992) Integrated Pulse-Stream Neural Networks - Results,
Issues and Pointers IEEE Transactions on N.N.s 3(3):385-393
[5] M.Holler, S.Tam, H.Castro and R.Benson (1989) An Electrically Trainable ANN
with 10240 Floating Gate Synapses. Int Conf on N.N.s Proc :191-196
[6] A.F.Murray and A.V.W.Smith.(1987) Asynchronous Arithmetic for VLSI Neural Systems. Electronics Letters 23(12):642-643
[7] A.J. Holmes et al. (1993) Use of a-Si:H Memory Devices for Non-volatile Weight
Storage in ANNs. Proc lCAS 15 :817-820

"
1007,1994,Learning to Play the Game of Chess,,1007-learning-to-play-the-game-of-chess.pdf,Abstract Missing,"Learning To Play the Game of Chess

Sebastian Thrun
University of Bonn
Department of Computer Science III
Romerstr. 164, 0-53117 Bonn, Germany
E-mail: thrun@carbon.informatik.uni-bonn.de

Abstract
This paper presents NeuroChess, a program which learns to play chess from the final
outcome of games. NeuroChess learns chess board evaluation functions, represented
by artificial neural networks. It integrates inductive neural network learning, temporal
differencing, and a variant of explanation-based learning. Performance results illustrate
some of the strengths and weaknesses of this approach.

1 Introduction
Throughout the last decades, the game of chess has been a major testbed for research on
artificial intelligence and computer science. Most oftoday's chess programs rely on intensive
search to generate moves. To evaluate boards, fast evaluation functions are employed which
are usually carefully designed by hand, sometimes augmented by automatic parameter tuning
methods [1]. Building a chess machine that learns to play solely from the final outcome of
games (win/loss/draw) is a challenging open problem in AI.
In this paper, we are interested in learning to play chess from the final outcome of games.
One of the earliest approaches, which learned solely by playing itself, is Samuel's famous
checker player program [10]. His approach employed temporal difference learning (in short:
TO) [14], which is a technique for recursively learning an evaluation function . Recently,
Tesauro reported the successful application of TO to the game of Backgammon, using
artificial neural network representations [16]. While his TO-Gammon approach plays grandmaster-level backgammon, recent attempts to reproduce these results in the context of Go
[12] and chess have been less successful. For example, Schafer [11] reports a system just
like Tesauro's TO-Gammon, applied to learning to play certain chess endgames. Gherrity [6]
presented a similar system which he applied to entire chess games. Both approaches learn
purely inductively from the final outcome of games. Tadepalli [15] applied a lazy version
of explanation-based learning [5, 7] to endgames in chess. His approach learns from the
final outcome, too, but unlike the inductive neural network approaches listed above it learns
analytically, by analyzing and generalizing experiences in terms of chess-specific knowledge.

1070

Sebastian Thrun

The level of play reported for all these approaches is still below the level of GNU-Chess, a
publicly available chess tool which has frequently been used as a benchmark. This illustrates
the hardness of the problem of learning to play chess from the final outcome of games.
This paper presents NeuroChess, a program that learns to play chess from the final outcome
of games. The central learning mechanisms is the explanation-based neural network (EBNN)
algorithm [9, 8]. Like Tesauro's TD-Gammon approach, NeuroChess constructs a neural
network evaluation function for chess boards using TO. In addition, a neural network version
of explanation-based learning is employed, which analyzes games in terms of a previously
learned neural network chess model. This paper describes the NeuroChess approach, discusses several training issues in the domain of chess, and presents results which elucidate
some of its strengths and weaknesses.

2

Temporal Difference Learning in the Domain of Chess

Temporal difference learning (TO) [14] comprises a family of approaches to prediction in
cases where the event to be predicted may be delayed by an unknown number of time steps.
In the context of game playing, TD methods have frequently been applied to learn functions
which predict the final outcome of games. Such functions are used as board evaluation
functions.
The goal of TO(O), a basic variant of TO which is currently employed in the NeuroChess
approach, is to find an evaluation function, V, which ranks chess boards according to their
goodness: If the board S is more likely to be a winning board than the board Sf, then
V(s) > V(Sf). To learn such a function, TO transforms entire chess games, denoted by
a sequence of chess boards So, SI, s2, . . . , StunaJ' into training patterns for V. The TO(O)
learning rule works in the following way. Assume without loss of generality we are learning
white's evaluation function. Then the target values for the final board is given by
{

I,
0,
-1,

if Stu.?tI is a win for white
if StUnaJ is a draw
if StonaJ is a loss for white

and the targets for the intermediate chess boards So, SI , S2, . .. , Stu.?tI-2 are given by
Vt.1fget( St)
I? V (St+2)

=

(1)

(2)

This update rule constructs V recursively. At the end of the game, V evaluates the final
outcome of the game (Eq. (l In between, when the assignment of V -values is less obvious,
V is trained based on the evaluation two half-moves later (Eq. (2?. The constant I (with
o ~ I ~ 1) is a so-called discount factor. It decays V exponentially in time and hence
favors early over late success. Notice that in NeuroChess V is represented by an artificial
neural network, which is trained to fit the target values vtarget obtained via Eqs. (l) and (2)
(cj [6, 11, 12, 16]).

?.

3

Explanation-Based Neural Network Learning

In a domain as complex as chess, pure inductive learning techniques. such as neural network Back-Propagation, suffer from enormous training times. To illustrate why, consider
the situation of a knight fork. in which the opponent's knight attacks our queen and king
simultaneously. Suppose in order to save our king we have to move it, and hence sacrifice
our queen. To learn the badness of a knight fork, NeuroChess has to discover that certain
board features (like the position of the queen relative to the knight) are important, whereas

Learning to Play the Game of Chess

1071

Figure 1: Fitting values and slopes in EBNN: Let V be the target function for which three
examples (s\, V(S\)), (S2' V(S2)), and (S3, V(S3)) are known. Based on these points the
S2)OS2, and a~;:3) are
learner might generate the hypothesis V'. If the slopes a~;:I),
also known, the learner can do much better: V"".

ar

others (like the number of weak pawns) are not. Purely inductive learning algorithms such
as Back-propagation figure out the relevance of individual features by observing statistical
correlations in the training data. Hence, quite a few versions of a knight fork have to be
experienced in order to generalize accurately. In a domain as complex as chess, such an
approach might require unreasonably large amounts of training data.
Explanation-based methods (EBL) [5, 7, 15] generalize more accurately from less training
data. They rely instead on the availability of domain knowledge, which they use for explaining
and generalizing training examples. For example, in the explanation of a knight fork, EBL
methods employ knowledge about the game of chess to figure out that the position of the
queen is relevant, whereas the number of weak pawns is not. Most current approaches to
EBL require that the domain knowledge be represented by a set of symbolic rules. Since
NeuroChess relies on neural network representations, it employs a neural network version
of EBL, called explanation-based neural network learning (EBNN) [9]. In the context of
chess, EBNN works in the following way: The domain-specific knowledge is represented
by a separate neural network, called the chess model M. M maps arbitrary chess boards St
to the corresponding expected board St+2 two half-moves later. It is trained prior to learning
V, using a large database of grand-master chess games. Once trained, M captures important
knowledge about temporal dependencies of chess board features in high-quality chess play.
EBNN exploits M to bias the board evaluation function V. It does this by extracting slope
constraints for the evaluation function V at all non-final boards, i.e., all boards for which V
is updated by Eq. (2). Let
with

t E

{a, 1,2, ... , tlioa\ - 2}

denote the target slope of V at St, which, because
Eq. (2), can be rewritten as

oV target ( St)

=

'Y.

oV( St+2) OSt+2
._OSt+2
OSt

vtarget ( St)

(3)

is set to 'Y V (St+2) according
(4)

using the chain rule of differentiation. The rightmost term in Eq. (4) measures how infinitesimal small changes of the chess board St influence the chess board St+2. It can be
approximated by the chess model M:

ovtarget(St)
OSt

~

'Y.

OV(St+2) oM(st)
.
OSt+2
OSt

(5)

The right expression is only an approximation to the left side, because M is a trained neural

Sebastian Thrun

1072

~

bmrd at time

f

(W""T""~)

~

board attime 1+ I
(black to move)

~

board at time 1+2

(w""'?ro~)

predictive model network M

165 hidden unit,

V(1+2)

Figure 2: Learning an evaluation function in NeuroChess. Boards are mapped into a
high-dimensionalJeature vector, which forms the input for both the evaluation network V
and the chess model M. The evaluation network is trained by Back-propagation and the
TD(O) procedure. Both networks are employed for analyzing training example in order to
derive target slopes for V.
network and thus its first derivative might be erroneous. Notice that both expressions on
the right hand side of Eq. (5) are derivatives of neural network functions, which are easy to
compute since neural networks are differentiable.
The result of Eq . (5) is an estimate of the slope of the target function V at 8t . This slope
adds important shape information to the target values constructed via Eq. (2). As depicted in
Fig. 1, functions can be fit more accurately if in addition to target values the slopes of these
values are known. Hence, instead of just fitting the target values vtarget ( 8t), NeuroChess also
fits these target slopes. This is done using the Tangent-Prop algorithm [13].
The complete NeuroChess learning architecture is depicted in Fig. 2. The target slopes
provide a first-order approximation to the relevance of each chess board feature in the
goodness of a board position. They can be interpreted as biasing the network V based on
chess-specific domain knowledge, embodied in M . For the relation ofEBNN and EBL and
the accommodation of inaccurate slopes in EBNN see [8].

4

Training Issues

In this section we will briefly discuss some training issues that are essential for learning good
evaluation functions in the domain of chess. This list of points has mainly been produced
through practical experience with the NeuroChess and related TD approaches. It illustrates
the importance of a careful design of the input representation, the sampling rule and the

Learning to Play the Game of Chess

1073

parameter setting in a domain as complex as chess.
Sampling. The vast majority of chess boards are, loosely speaking, not interesting. If, for
example, the opponent leads by more than a queen and a rook, one is most likely to loose.
Without an appropriate sampling method there is the danger that the learner spends most
of its time learning from uninteresting examples. Therefore, NeuroChess interleaves selfplay and expert play for guiding the sampling process. More specifically, after presenting
a random number of expert moves generated from a large database of grand-master games,
NeuroChess completes the game by playing itself. This sampling mechanism has been found
to be of major importance to learn a good evaluation function in a reasonable amount of time.
Quiescence. In the domain of chess certain boards are harder to evaluate than others. For
example, in the middle of an ongoing material exchange, evaluation functions often fail to
produce a good assessment. Thus, most chess programs search selectively. A common
criterion for determining the depth of search is called quiescence. This criterion basically
detects material threats and deepens the search correspondingly. NeuroChess' search engine
does the same. Consequently, the evaluation function V is only trained using quiescent
boards.
Smoothness. Obviously, using the raw, canonical board description as input representation is
a poor choice. This is because small changes on the board can cause a huge difference in value,
contrasting the smooth nature of neural network representations. Therefore, NeuroChess
maps chess board descriptions into a set of board features . These features were carefully
designed by hand.
Discounting. The variable 'Y in Eq. (2) allows to discount values in time. Discounting has
frequently been used to bound otherwise infinite sums of pay-off. One might be inclined to
think that in the game of chess no discounting is needed, as values are bounded by definition.
Indeed, without discounting the evaluation function predicts the probability for winning-in
the ideal case. In practice, however, random disturbations of the evaluation function can
seriously hurt learning, for reasons given in [4, 17]. Empirically we found that learning
failed completely when no discount factor was used. Currently, NeuroChess uses 'Y = 0.98.
Learning rate. TO approaches minimize a Bellman equation [2]. In the NeuroChess
domain, a close-to-optimal approximation of the Bellman equation is the constant function
V(s) == O. This function violates the Bellman equation only at the end of games (Eq. (1?,
which is rare if complete games are considered. To prevent this, we amplified the learning
rate for final values by a factor of20, which was experimentally found to produce sufficiently
non-constant evaluation functions.
Software architecture. Training is performed completely asynchronously on up to 20
workstations simultaneously. One of the workstations acts as a weight server, keeping track
of the most recent weights and biases of the evaluation network. The other workstations
can dynamically establish links to the weight server and contribute to the process of weight
refinement. The main process also monitors the state of all other workstations and restarts
processes when necessary. Training examples are stored in local ring buffers (1000 items
per workstation).

5

Results

In this section we will present results obtained with the NeuroChess architecture. Prior to
learning an evaluation function, the model M (175 input, 165 hidden, and 175 output units)
is trained using a database of 120,000 expert games. NeuroChess then learns an evaluation

1074

I. e2e3 b8c6
2. dlf3 c6e5
3. f3d5 d7d6
4. flb5 c7c6
5. b5a4 g8f6
6. d5d4 c8f5
7. f2f4 e5d7
8. ele2d8a5
9. a4b3 d7c5
10. b I a3 c5b3
11 . a2b3 e7e5
12. f4e5 f6e4
13. e5d6 e8c8
14. b3b4 a5a6
15. b4b5 a6a5

Sebastian Thrun

16. b2b4 a5a4
17. b5c6 a4c6
18. gl f3 d8d6
19. d4a7 f5g4
20. c2c4 c8d7
21. b4b5 c6c7
22. d2d3 d6d3
23. b5b6 c7c6
24. e2d3 e4f2
25. d3c3 g4f3
26. g2f3 f2h 1
27. clb2 c6f3
28. a7a4 d7e7
29. a3c2 hi f2
30. b2a3 e7f6

31 . a3f8 f2e4
32. c3b2 h8f8
33. a4d7 f3f5
34. d7b7 f5e5
35. b2cl f8e8
36. b7d5 e5h2
37. ala7 e8e6
38. d5d8 f6g6
39. b6b7 e6d6
40. d8a5 d6c6
41 . a5b4 h2b8
42. a7a8 e4c3
43. c2d4 c6f6
44. b4e7 c3a2
45. cldl a2c3

46. d I c2 b8h2
47. c2c3 f6b6
48. e7e4 g6h6
49. d4f5 h6g5
50. e4e7 g5g4
51. f5h6 g7h6
52. e7d7 g4h5
53. d7d I h5h4
54. d I d4 h4h3
55. d4b6 h2e5
56. b6d4 e5e6
57. c3d2 e6f5
58. e3e4 f5 g5
59. d4e3 g5e3
60. d2e3 f7f5

61 . e4f5 h3g4 65. a8e8 e6d7
62. f5f6 h6h5
66. e8e7 d7d8
63. b7b8q g4f5 67. f4c7
64. b8f4 f5e6
final board

Figure 3: NeuroChess against GNU-Chess. NeuroChess plays white. Parameters: Both
players searched to depth 3, which could be extended by quiescence search to at most 11.
The evaluation network had no hidden units. Approximately 90% of the training boards
were sampled from expert play.

network V (175 input units, 0 to 80 hidden units, and one output units). To evaluate the level
of play, NeuroChess plays against GNU-Chess in regular time intervals. Both players employ
the same search mechanism which is adopted from GNU-Chess. Thus far, experiments lasted
for 2 days to 2 weeks on I to 20 SUN Sparc Stations.
A typical game is depicted in Fig. 3. This game has been chosen because it illustrates both
the strengths and the shortcomings of the NeuroChess approach. The opening of NeuroChess
is rather weak. In the first three moves NeuroChess moves its queen to the center of the
board.' NeuroChess then escapes an attack on its queen in move 4, gets an early pawn
advantage in move 12, attacks black's queen pertinaciously through moves 15 to 23, and
successfully exchanges a rook. In move 33, it captures a strategically important pawn, which,
after chasing black's king for a while and sacrificing a knight for no apparent reason, finally
leads to a new queen (move 63). Four moves later black is mate. This game is prototypical.
As can be seen from this and various other games, NeuroChess has learned successfully to
protect its material, to trade material, and to protect its king. It has not learned, however, to
open a game in a coordinated way, and it also frequently fails to play short.endgames even
if it has a material advantage (this is due to the short planning horizon). Most importantly, it
still plays incredibly poor openings, which are often responsible for a draw or a loss. Poor
openings do not surprise, however, as TD propagates values from the end of a game to the
beginning.
Table I shows a performance comparison of NeuroChess versus GNU-Chess, with and
without the explanation-based learning strategy. This table illustrates that NeuroChess wins
approximately 13% of all games against GNU-Chess, if both use the same search engine. It
'This is because in the current version NeuroChess still heavily uses expert games for sampling.
Whenever a grand-master moves its queen to the center of the board, the queen is usually safe, and there
is indeed a positive correlation between having the queen in the center and winning in the database.
NeuroChess falsely deduces that having the queen in the center is good. This effect disappears when
the level of self-play is increased, but this comes at the expense of drastically increased training time,
since self-play requires search.

Learning to Play the Game of Chess

# of games
100
200
500
1000
1500
2000
2400

GNU depth 2, NeuroChess depth 2
Back-propagation
EBNN
1
0
6
2
35
13
73
85
130
135
190
215
239
316

1075

GNU depth 4, NeuroChess depth 2
Back-propagation
EBNN
0
0
0
0
I
0
2
1
3
3
3
8
II
3

Table 1: Performance ofNeuroChess vs. GNU-Chess during training. The numbers show the
total number of games won against GNU-Chess using the same number of games for testing
as for training. This table also shows the importance of the explanation-based learning
strategy in EBNN. Parameters: both learners used the original GNU-Chess features, the
evaluation network had 80 hidden units and search was cut at depth 2, or 4, respectively (no
quiescence extensions).
also illustrates the utility of explanation-based learning in chess.

6 Discussion
This paper presents NeuroChess, an approach for learning to play chess from the final
outcomes of games. NeuroChess integrates TD, inductive neural network learning and
a neural network version of explanation-based learning. The latter component analyzes
games using knowledge that was previously learned from expert play. Particular care has
been taken in the design of an appropriate feature representation, sampling methods, and
parameter settings. Thus far, NeuroChess has successfully managed to beat GNU-Chess in
several hundreds of games. However, the level of play still compares poorly to GNU-Chess
and human chess players.
Despite the initial success, NeuroChess faces two fundamental problems which both might
weB be in the way of excellent chess play. Firstly, training time is limited, and it is to
be expected that excellent chess skills develop only with excessive training time. This is
particularly the case if only the final outcomes are considered. Secondly, with each step of
TO-learning NeuroChess loses information. This is partially because the features used for
describing chess boards are incomplete, i.e., knowledge about the feature values alone does
not suffice to determine the actual board exactly. But, more importantly, neural networks have
not the discriminative power to assign arbitrary values to all possible feature combinations.
It is therefore unclear that a TD-like approach will ever, for example, develop good chess
openmgs.
Another problem of the present implementation is related to the trade-off between knowledge
and search. It has been well recognized that the ul timate cost in chess is determi ned by the ti me
it takes to generate a move. Chess programs can generally invest their time in search, or in the
evaluation of chess boards (search-knowledge trade-off) [3] . Currently, NeuroChess does a
poor job, because it spends most of its time computing board evaluations. Computing a large
neural network function takes two orders of magnitude longer than evaluating an optimized
linear evaluation function (like that of GNU-Chess). VLSI neural network technology offers
a promising perspective to overcome this critical shortcoming of sequential neural network
simulations.

1076

Sebastian Thrun

Acknowledgment
The author gratefully acknowledges the guidance and advise by Hans Berliner, who provided
the features for representing chess boards, and without whom the current level of play would
be much worse. He also thanks Tom Mitchell for his suggestion on the learning methods,
and Horst Aurisch for his help with GNU-Chess and the database.

References
[I] Thomas S. Anantharaman. A Statistical Study of Selective Min-Max Search in Computer Chess.
PhD thesis, Carnegie Mellon University, School of Computer Science, Pittsburgh, PA, 1990.
Technical Report CMU-CS-90-173.
[2] R. E. Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, 1957.
[3] Hans J. Berliner, Gordon Goetsch, Murray S. Campbell, and Carl Ebeling. Measuring the
performance potential of chess programs. Artificial Intelligence, 43:7-20, 1990.
[4] Justin A. Boyan. Generalization in reinforcement learning: Safely approximating the value
function. In G. Tesauro, D. Touretzky, and T. Leen, editors, Advances in Neural Information
Processing Systems 7, San Mateo, CA, 1995. Morgan Kaufmann. (to appear).
[5] Gerald Dejong and Raymond Mooney. Explanation-based learning: An alternative view. Machine Learning, 1(2): 145-176, 1986.
[6] Michael Gherrity. A Game-Learning Machine. PhD thesis, University of California, San Diego,
1993.
[7] Tom M. Mitchell, Rich Keller, and Smadar Kedar-Cabelli. Explanation-based generalization: A
unifying view. Machine Learning, 1(1 ):47-80, 1986.
[8] Tom M. Mitchell and Sebastian Thrun. Explanation based learning: A comparison of symbolic
and neural network approaches. In Paul E. Utgoff, editor, Proceedings of the Tenth International
Conference on Machine Learning, pages 197-204, San Mateo, CA, 1993. Morgan Kaufmann.
[9] Tom M. Mitchell and Sebastian Thrun. Explanation-based neural network learning for robot
control. In S. J. Hanson, J. Cowan, and C. L. Giles, editors, Advances in Neural Information
Processing Systems 5, pages 287-294, San Mateo, CA, 1993. Morgan Kaufmann.
[10] A. L. Samuel. Some studies in machine learning using the game of checkers. IBM Journal on
research and development, 3:210-229, 1959.
[11] Johannes Schafer. Erfolgsorientiertes Lemen mit Tiefensuche in Bauemendspielen. Technical
report, UniversiUit Karlsruhe, 1993. (in German).
[12] Nikolaus Schraudolph, Pater Dayan, and Terrence J. Sejnowski. Using the TD(lambda) algorithm
to learn an evaluation function for the game of go. In Advances in Neural Information Processing
Systems 6, San Mateo, CA, 1994. Morgan Kaufmann.
[13] Patrice Simard, Bernard Victorri, Yann LeCun, and John Denker. Tangent prop -a formalism for
specifying selected invariances in an adaptive network. In J. E. Moody, S. J. Hanson, and R. P.
Lippmann, editors, Advances in Neural Information Processing Systems 4, pages 895-903, San
Mateo, CA, 1992. Morgan Kaufmann.
[14] Richard S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning,
3,1988.
[15] Prasad Tadepalli. Planning in games using approximately learned macros. In Proceedings of the
Sixth International Workshop on Machine Learning, pages 221-223, Ithaca, NY, 1989. Morgan
Kaufmann.
[16] Gerald J. Tesauro. Practical issues in temporal difference learning. Machine Learning, 8, 1992.
[17] Sebastian Thrun and Anton Schwartz. Issues in using function approximation for reinforcement learning. In M. Mozer, P. Smolensky, D. Touretzky, J. Elman, and A. Weigend, editors,
Proceedings of the 1993 Connectionist Models Summer School, Hillsdale, NJ, 1993. Erlbaum
Associates.

"
1008,1994,Multidimensional Scaling and Data Clustering,,1008-multidimensional-scaling-and-data-clustering.pdf,Abstract Missing,"Multidimensional Scaling and Data Clustering

Thomas Hofmann & Joachim Buhmann
Rheinische Friedrich-Wilhelms-U niversitat
Institut fur Informatik ill, Romerstra6e 164
D-53117 Bonn, Germany
email:{th.jb}@cs.uni-bonn.de

Abstract
Visualizing and structuring pairwise dissimilarity data are difficult combinatorial optimization problems known as multidimensional scaling or pairwise data clustering.
Algorithms for embedding dissimilarity data set in a Euclidian space, for clustering
these data and for actively selecting data to support the clustering process are discussed
in the maximum entropy framework. Active data selection provides a strategy to discover
structure in a data set efficiently with partially unknown data.

1 Introduction
Grouping experimental data into compact clusters arises as a data analysis problem in psychology, linguistics, genetics and other experimental sciences. The data which are supposed
to be clustered are either given by an explicit coordinate representation (central clustering)
or, in the non-metric case, they are characterized by dissimilarity values for pairs of data
points (pairwise clustering). In this paper we study algorithms (i) for embedding non-metric
data in a D-dimensional Euclidian space, (ii) for simultaneous clustering and embedding of
non-metric data, and (iii) for active data selection to determine a particular cluster structure
with minimal number of data queries. All algorithms are derived from the maximum entropy
principle (Hertz et al., 1991) which guarantees robust statistics (Tikochinsky et al., 1984).
The data are given by a real-valued, symmetric proximity matrix D E R NXN , 'Dkl being
the pairwise dissimilarity between the data points k, l. Apart from the symmetry constraint
we make no further assumptions about the dissimilarities, i.e., we do not require D being a
metric. The numbers 'Dkl quite often violate the triangular inequality and the dissimilarity of
a datum to itself could be finite.

2

Statistical Mechanics of Multidimensional Scaling

Embedding dissimilarity data in a D-dimensional Euclidian space is a non-convex optimization problem which typically exhibits a large number of local minima. Stochastic search
methods like simulated annealing or its deterministic variants have been very successfulJy

460

Thomas Hofmann. Joachim Buhmann

applied to such problems. The question in multidimensional scaling is to find coordinates
{Xi }i~1 in a D-dimensional Euclidian space with minimal embedding costs
N

H MDS

[I Xi -

1 '""'
= 2N
L.,

Xk 12 - 'Dik ]2 .

(1)

i,k=1
Without loss of generality we shift the center of mass in the origin <2::= I Xk = 0).
In the maximum entropy framework the coordinates {Xi} are regarded as random variables
which are distributed according to the Gibbs distribution P ( { Xj} ) = exp( - f3 (H MDS - F). The
inverse temperature f3 = 1/T controls the expected embedding costs (HMDS) (expectation values are denoted by (.). To calculate the free energy F for H MDS we approximate the coupling
term 2 2:~""k=1 'DikxiXk/N ~ 2:[:1 xihi with the mean fields hi = 4 2:~= 1 'Dik(Xk}/N.
Standard t~chniques to evaluate the free energy F yield the equations

J J II
' 00

Z(HMDS)

00

dy

rv

- ' 00

- 00

f)

F(H MDS )

D

L

2

dR.d,d' exp (-f3NF),

(2)

J

(3)

d,d'=1
N

R.~d' - f3~ Lin

d,d'=1

i=1

00

dXjexp (-f3f(Xi)) '

- 00

N

f(Xi)

IXil4 -

~IXiI2 L 'Dik + 4xTR.xi + xT (hi -

4Y)?
k=1
The integral in Eq. (2) is dominated by the absolute minimum of F in the limit N
Therefore, we calculate the saddle point equations

(4)
~ 00.

N

R.

=

~L

((Xjxf) + l(l x iI 2)I)
i=1
I Xi exp( -f3f(Xj)dx i
I exp( -f3 f(Xj)dxi .

and

(5)

0

(6)

Equation (6) has been derived by differentiating F with respect to hi. I denotes the D x D
unit matrix. In the low temperature limit f3 ~ 00 the integral in (3) is dominated by the
minimum of f(Xi) . Therefore, a new estimate of (Xi) is calculated minimizing f with respect
to Xi. Since all explicit dependencies between the Xi have been eliminated, this minimization
can be performed independently for all i, 1 ~ i ~ N.
In the spirit of the EM algorithm for Gaussian mixture models we suggest the following
algorithm to calculate a meanfield approximation for the multidimensional scaling problem.
initialize (Xi)(O) randomly; t
while

2:::':1 I(Xi )(t ) -

(Xi)(t-I)I

E- step: estimate
M-step: calculate

>

(Xi) (t+l)
n (t),

= O.
t:

as a function of

h~t) and determine

(Xi)( t ) ,
y (t)

RY) ,

such

that the centroid condition is satisfied.

y(t ),

h~ t)

Multidimensional Scaling and Data Clustering

461

This algorithm was used to determine the embedding of protein dissimilarity data as shown in
Fig. 1d. The phenomenon that the data clusters are arranged in a circular fashion is explained
by the lack of small dissimilarity values. The solution in Fig. Id is about a factor of two
better than the embedding found by a classical MDS program (Gower, 1966). This program
determines a (N - 1)- space where the ranking of the dissimilarities is preserved and uses
principle component analysis to project this tentative embedding down to two dimensions.
Extensions to other MDS cost functions are currently under investigation.

3

Multidimensional Scaling and Pairwise Clustering

Embedding data in a Euclidian space precedes quite often a visual inspection by the data
analyst to discover structure and to group data into clusters. The question arises how both
problems, the embedding problem and the clustering problem, can be solved simultaneously.
The second algorithm addresses the problem to embed a data set in a Euclidian space such
that the clustering structure is approximated as faithfully as possible in the maximum entropy
sense by the clustering solution in this embedding space. The coordinates in the embedding
space are the free parameters for this optimization problem.
Clustering of non-metric dissimilarity data, also called pairwise clustering (Buhmann, Hofmann, 1994a), is a combinatorial optimization problem which depends on Boolean assignments Miv E {a, I} of datum i to cluster lJ. The cost function for pairwise clustering with
J( clusters is
If
1
N
N
(7)
E~:(M) =
2 N
MkvMlv'Dkl with
v=1 Pv
k=! 1=1

L

LL

In the meanfield approach we approximate the Gibbs distribution P( Ej;) corresponding
to the original cost function by a family of approximating distributions. The distribution
which represents most accurately the statistics of the original problem is determined by
the minimum of the Kullback-Leibler divergence to the original Gibbs distribution. In the
pairwise clustering case we introduce potentials {Ekv } for the effective interactions, which
define a set of cost functions with non-interacting assignments.
K

N

L L Mk 1jEkl;.

?<).; (M, {Ekv }) =

(8)

v=1 k=1

The optimal potentials derived from this minimization procedure are

{?kv} = arg min 'DKL (PO(E~' )IIP(E~)),

(9)

{?kv}

where PO(E9{) is the Gibbs distribution corresponding to E~., and 'DKL(?II?) is the KLdivergence. This method is equivalent to minimizing an upper bound on the free energy
(Buhmann, Hofmann, 1994b),
F(E~:) ::; Fo(E~. )

+ (VK)o,

with

VA"" = Ej; - ?~""

(10)

(')0 denoting the average over all configurations of the cost function without interactions.
Correlations between assignment variables are statistically independent for PO( E9(), i.e.,
(MkvA11v)0 = (M kv )0(A11v )0. The averaged potential VI\, therefore, amounts to
K

(Vrd =

1

N

LL
v=1 k ,I=1

(Mkl;) (Mlv) 2 vN'Dk1 P

K

N

L L(A1kv)Eklj,
v=1 k=1

(11)

462

Thomas Hofmann. Joachim Buhmann

the subscript of averages being omitted for conciseness. The expected assignment variables
are
(12)
Minimizing the upper bound yields
(13)
The ""optimal"" potentials

[i~' =

1

N

IN)

(

N L(Mkv ) 'Dik - 2 N L(M1v)D kl
1v
k=1
Pv
1=1
J

(14)

depend on the given distance matrix, the averaged assignment variables and the cluster
probabilities. They are optimal in the sense, that if we set
(15)
the N * K stationarity conditions (13) are fulfilled for every i E {I, ... , N}, 11 E {I, ... , K}. A
simultaneous solution ofEq. (15) with (12) constitutes a necessary condition for a minimum
of the upper bound for the free energy :F.
The connection between the clustering and the multidimensional scaling problem is established, if we restrict the potentials [iv to be of the form IXi - Yvf with the centroids
YII = 2:~=1 Mkl/Xv/ 2::=1 Mkv. We consider the coordinates Xi as the variational parameters. The additional constraints restrict the family of approximating distributions, defined
by ?9"". to a subset. Using the chain rule we can calculate the derivatives of the upper bound
(10), resulting in the exact stationary conditions for Xi,
K

N

'""
co
~ (Mia )(Mja ) (~Cia

co
-~Civ)Ya

a,v=1

=

K

jv ) x
(MjoJ(M
N
a,v=1
Pa

'~
"" '~
""
j=1

N ( (Xk - Ya) a(Mka)
(~[ia - ~[ir/) [(Mia)! + ~
Oxi T)

1(Xj - Ya),

(16)

where ~[iOt = ?ia - [tao The derivatives a(Mka) /Oxi can be exactly calculated, since they
are given as the solutions of an linear equation system with N x K unknowns for every Xi. To
reduce the computational complexity an approximation can be derived under the assumption
ay 0/ / aXj ~ O. In this case the right hand side of (16) can be set to zero in a first order
approximation yielding an explicit formula for Xi,
K

KiXi

~ ~ L(Miv) (11Yv1l
v=1

K
2 -

[tv) (Yv - L(Mia)Ya) ,

(17)

a=1

with the covariance matrix Ki = ((yyT)j - (Y)i(Y)T) and (Y)i = 2:~=1 (Miv)Y v'
The derived system of transcendental equations given by (12), (17) and the centroid condition explicitly reflects the dependencies between the clustering procedure and the Euclidian
representation. Solving these equations simultaneously leads to an efficient algorithm which

Multidimensional Scaling and Data Clustering

a

463

.

4tHB

b

HB

HG,H~

HA

GGI

GP~

~

GGI~

MY

~

HBX,
HF, HE
GP

HG~~~

~

~
HBX,HF,HE
~.

GGGI
???
? [l}?,faitt\tvJqJ~!;t

. .?. ?. ,.'?..,.? ... .
~llt GP

c

GGI
0

420

~GGG
x

x

HAfo

++

Random Selection
380

?re

+
+

d

x~

HB
+

?

340

MY

---+

HG,HE,HF

300

# of selected Do,

Figure 1: Similarity matrix of 145 protein sequences of the globin family (a): dark gray levels
correspond to high similarity values; (b): clustering with embedding in two dimensions; (c):
multidimensional scaling solution for 2-dimensional embedding; (d): quality of clustering
solution with random and active data selection of 'D ik values. eKe has been calculated on the
basis of the complete set of 'Di k values.
interleaves the multidimensional scaling process and the clustering process and which avoids
an artificial separation into two uncorrelated processes . The described algorithm for simultaneous Euclidian embedding and data clustering can be used for dimensionality reduction,
e.g., high dimensional data can be projected to a low dimensional subspace in a nonlinear
fashion which resembles local principle component analysis (Buhmann, Hofmann, 1994b).
Figure (l) shows the clustering result for a real-world data set of 145 protein sequences. The
similarity values between pairs of sequences are determined by a sequence alignment program
which takes biochemical and structural information into account. The sequences belong to
different protein families like hemoglobin, myoglobin and other globins; they are abbreviated
with the displayed capital letters. The gray level visualization of the dissimilarity matrix with
dark values for similar protein sequences shows the formation of distinct ""squares"" along the
main diagonal. These squares correspond to the discovered partition after clustering. The
embedding in two dimensions shows inter-cluster distances which are in consistent agreement
with the similarity values of the data. In three and four dimensions the error between the

464

Thomas Hofmann. Joachim Buhmann

given dissimilarities and the constructed distances is further reduced. The results are in good
agreement with the biological classification.

4

Active Data Selection for Data Clustering

Active data selection is an important issue for the analysis of data which are characterized
by pairwise dissimilarity values. The size of the distance matrix grows like the square of
the number of data 'points'. Such a O(N2) scaling renders the data acquisition process
expensive. It is, therefore, desirable to couple the data analysis process to the data acquisition
process, i.e., to actively query the supposedly most relevant dissimilarity values. Before
addressing active data selection questions for data clustering we have to discuss the problem
how to modify the algorithm in the case of incomplete data.
If we want to avoid any assumptions about statistical dependencies, it is impossible to infer
unknown values and we have to work directly with the partial dissimilarity matrix. Since the
data enters only in the (re-)ca1culation of the potentials in (14), it is straightforward to appropriately modify these equations. All sums are restricted to terms with known dissimilarities
and the normalization factors are adjusted accordingly.
Alternatively we can try to explicitly estimate the unknown dissimilarity values based on
a statistical model. For this purpose we propose two models, relying on a known group
structure of the data. The first model (I) assumes that all dissimilarities between a point
i and points j belonging to a group G ~ are i.i.d. random variables with the probability
density Pi/1 parameterized by eiw In this scheme a subset of the known dissimilarities of
i and j to other points k are used as samples for the estimation of V ij . The selection
of the specific subset is determined by the clustering structure. In the second model (II)
we assume that the dissimilarities between groups G v, G ~ are i.i.d. random variables with
density PV/1 parameterized by e,IW The parameters ev~ are estimated on the basis of all
known dissimilarities {Vij E V} between points from G v and G~.
The assignments of points to clusters are not known a priori and have to be determined in the
light of the (given and estimated) data. The data selection strategy becomes self-consistent
if we interpret the mean fields (.I""vfiv) of the clustering solution as posterior probabilities for
the binary assignment variables. Combined with a maximum likelihood estimation for the
unknown parameters given the posteriors, we arrive at an EM-like iteration scheme with the
E-step replaced by the clustering algorithm.
The precise form of the M-Step depends on the parametric form of the densities Pi~ or PI/~'
respectively. In the case of Gaussian distributions the M-Step is described by the following
estimation equations for the location parameters
(I),

with 1T:j~ = 1+~vl' ((Mil/){Mj~)

+ (l\tfi~)(Mjv)).

(II),

(18)

Corresponding expressions are derived

for the standard deviations at) or a~'~, respectively. In the case of non-normal distributions
the empirical mean might still be a good estimator of the location parameter, though not
necessarily a maximum likelihood estimator. The missing dissimilarities are estimated by
the following statistics, derived from the empirical means.
- (I)

Dij

K

=

'""""

~ (l\tfiv)(JVfj~)

1/,11=)

i\[

- (I)

J. i~mi~

JY

1~

+ N jvmjv
- (I)
+ N.
}V

(I),

D~~)
!}

= '"""" ."".ij
m- (I)
""11/1
~

11-:5:~

'v~

(II) ,

(19)

Multidimensional Scaling and Data Clustering

465

2600 r - - r -........-.----,--........-.-----,.,

L,
'\.,

2400

\c,

~---,-- ...

\

2200

""""-!

1\

\t:
Ac t i ve Da t~:L--,
Se 1 ec t ion
\ _______________________ _

2000

o

400

BOO

1200

# of selected dissimilarities

Figure 2: Similarity matrix of 54 word fragments generated by a dynamic programming
algorithm. The clustering costs in the experiment with active data selection requires only half
as much data as a random selection strategy.

=

with Nil'
E'D.kE'D(i11k11)' For model (I) we have used a pooled estimator to exploit the
data symmetry. The iteration scheme finally leads to estimates (jill or (j'lt' respectively for the
parameters and Dij for all unknown dissimilarities.
Criterion for Active Data Selection: We will use the expected reduction in the variance of
the free energy Fo as a score, which should be maximized by the selection criterion. Fo is
given by Fo(D) = -~ E;;:', log E;~l exp( -{3?i/l(D)). If we query a new dissimilarity
D ij the expected reduction of the variance of the free energy is approximated by

aFO]2 V [D .. _ D .. ]
~ .. = 2 [aDij
tJ
tJ
t)

(20)

The partial derivatives can be calculated exactly by solving a system of linear equations with
N x [ ..: unknowns. Alternatively a first order approximation in f /I = O( 1/ N P,/) yields
(21)

This expression defines a relevance measure of Dij for the clustering problem since a Dij
value contributes to the clustering costs only if the data i and j belong to the same cluster.
Equation (21) summarizes the mean-field contributions aFo/aDij ~ a(H)o/aDjj .
To derive the final form of our scoring function we have to calculate an approximation of
the variance in Eq. (20) which measures the expected squared error for replacing the true
value Dij with our estimate D ij . Since we assumed statistical independence the variances
are additive V [Dij - Dij] = V [Dij] + V [Dij]. The total population variance is a sum
of inner- and inter-cluster variances, that can be approximated by the empirical means and
by the empirical variances instead of the unknown parameters of Pill or P'lt'. The sampling
variance of the statistics Dij is estimated under the assumption, that the empirical means ifl'ill

466

Thomas Hofmann, Joachim Buhmann

or mVJ.l respectively are uncorrelated. This holds in the hard clustering limit. We arrive at
the following final expression for the variances of model (II)

v [Vij-Dij ]

~

L1TYJl[(Dij-mvJl)2+(I+I: 1T:JJ.l
V~Jl

Vk1EV

1Tkl(j~Jl)l
VJl

(22)

For model (I) a slightly more complicated formula can be derived. Inserting the estimated
variances into Eq. (20) leads to the final expression for our scoring function.
To demonstrate the efficiency of the proposed selection strategy, we have compared the
clustering costs achieved by active data selection with the clustering costs resulting from
randomly queried data. Assignments int the case of active selection are calculated with
statistical model (I). Figure 1d demonstrates that the clustering costs decrease significantly
faster when the selection criterion (20) is implemented. The structure of the clustering
solution has been completely inferred with about 3300 selected V ik values. The random
strategy requires about 6500 queries for the same quality. Analogous comparison results for
linguistic data are summarized in Fig. 2. Note the inconsistencies in this data set reflected by
smallVik values outside the cluster blocks (dark pixels) or by the large Vik values (white
pixels) inside a block.
Conclusion: Data analysis of dissimilarity data is a challenging problem in molecular biology, linguistics, psychology and, in general, in pattern recognition. We have presented
three strategies to visualize data structures and to inquire the data structure by an efficient
data selection procedure. The respective algorithms are derived in the maximum entropy
framework for maximal robustness of cluster estimation and data embedding. Active data
selection has been shown to require only half as much data for estimating a clustering solution
of fixed quality compared to a random selection strategy. We expect the proposed selection
strategy to facilitate maintenance of genome and protein data bases and to yield more robust
data prototypes for efficient search and data base mining.
Acknowledgement: It is a pleasure to thank M. Vingron and D. Bavelier for providing the
protein data and the linguistic data, respectively. We are also grateful to A. Polzer and H.J.
Warneboldt for implementing the MDS algorithm. This work was partially supported by the
Ministry of Science and Research of the state Nordrhein-Westfalen.

References
Buhmann, J., Hofmann, T. (l994a). Central and Pairwise Data Clustering by Competitive
Neural Networks. Pages 104-111 of"" Advances in Neural Infonnation Processing
Systems 6. Morgan Kaufmann Publishers.
Buhmann, J., Hofmann, T. (1994b). A Maximum Entropy Approach to Pairwise Data
Clustering. Pages 207-212 of"" Proceedings of the International Conference on Pattern
Recognition, Hebrew University, Jerusalem, vol. II. IEEE Computer Society Press.
Gower, J. C. (1966). Some distance properties of latent root and vector methods used in
multivariate analysis. Biometrika, 53, 325-328.
Hertz, J., Krogh, A., Palmer, R. G. (1991). Introduction to the Theory ofNeural Computation.
New York: Addison Wesley.
Tikochinsky, y, Tishby, N.Z., Levine, R. D. (1984). Alternative Approach to MaximumEntropy Inference. Physical Review A, 30, 2638-2644.

"
1009,1994,An experimental comparison of recurrent neural networks,,1009-an-experimental-comparison-of-recurrent-neural-networks.pdf,Abstract Missing,"An experimental comparison
of recurrent neural networks
Bill G. Horne and C. Lee Giles?
NEe Research Institute
4 Independence Way
Princeton, NJ 08540
{horne.giles}~research.nj.nec.com

Abstract
Many different discrete-time recurrent neural network architectures have been proposed. However, there has been virtually no
effort to compare these arch:tectures experimentally. In this paper
we review and categorize many of these architectures and compare
how they perform on various classes of simple problems including
grammatical inference and nonlinear system identification.

1

Introduction

In the past few years several recurrent neural network architectures have emerged.
In this paper we categorize various discrete-time recurrent neural network architectures, and perform a quantitative comparison of these architectures on two problems: grammatical inference and nonlinear system identification.

2

RNN Architectures

We broadly divide these networks into two groups depending on whether or not the
states of the network are guaranteed to be observable. A network with observable
states has the property that the states of the system can always be determined from
observations of the input and output alone. The archetypical model in this class
.. Also with UMIACS, University of Maryland, College Park, MD 20742

698

Bill G. Horne, C. Lee Giles

Table 1: Terms that are weighted in various single layer network architectures. Ui
represents the ith input at the current time step, Zi represents the value of the lh
node at the previous time step.
Architecture
First order
High order
Bilinear
Quadratic

bias
x
x

Ui

Zi

x

x

x
x

x
x

UiUj

ZiUj

ZiZj

x

x
x
x

x

was proposed by Narendra and Parthasarathy [9]. In their most general model, the
output of the network is computed by a multilayer perceptron (MLP) whose inputs
are a window of past inputs and outputs, as shown in Figure la. A special case of
this network is the Time Delay Neural Network (TDNN), which is simply a tapped
delay line (TDL) followed by an MLP [7]. This network is not recurrent since there
is no feedback; however, the TDL does provide a simple form of dynamics that
gives the network the ability model a limited class of nonlinear dynamic systems.
A variation on the TDNN, called the Gamma network, has been proposed in which
the TDL is replaced by a set of cascaded filters [2]. Specifically, if the output of
one of the filters is denoted xj(k), and the output of filter i connects to the input
of filter j, the output of filter j is given by,

xj(k + 1) = I-'xi(k) + (l-I-')xj(k).
In this paper we only consider the case where I-' is fixed, although better results can
be obtained if it is adaptive.
Networks that have hidden dynamics have states which are not directly accessible
to observation. In fact, it may be impossible to determine the states of a system
from observations of it's inputs and outputs alone. We divide networks with hidden dynamics into three classes: single layer networks, multilayer networks, and
networks with local feedback.
Single layer networks are perhaps the most popular of the recurrent neural network
models. In a single layer network, every node depends on the previous output of
all of the other nodes. The function performed by each node distinguishes the
types of recurrent networks in this class. In each of the networks, nodes can be
characterized as a nonlinear function of a weighted sum of inputs, previous node
outputs, or products of these values. A bias term may also be included. In this
paper we consider first-order networks, high-order networks [5], bilinear networks,
and Quadratic networks[12]. The terms that are weighted in each of these networks
are summarized in Table 1.
Multilayer networks consist of a feedforward network coupled with a finite set of
delays as shown in Figure lb. One network in this class is an architecture proposed
by Robinson and Fallside [11], in which the feedforward network is an MLP. Another
popular networks that fits into this class is Elman's Simple Recurrent Network
(SRN) [3]. An Elman network can be thought of as a single layer network with an
extra layer of nodes that compute the output function, as shown in Figure lc.
In locally recurrent networks the feedback is provided locally within each individual

An Experimental Comparison of Recurrent Neural Networks

699

MLP

Figure 1: Network architectures: (a) Narendra and Parthasarathy's Recurrent Neural Network, (b) Multilayer network and (c) an Elman network.

node, but the nodes are connected together in a feed forward architecture. Specifically, we consider nodes that have local output feedback in which each node weights
a window of its own past outputs and windows of node outputs from previous layers.
Networks with local recurrence have been proposed in [1, 4, 10].

3
3.1

Experimental Results
Experimental methodology

In order to make the comparison as fair as possible we have adopted the following
methodology.
?

?

?

Resources. We shall perform two fundamental comparisons. One in which the
number of weights is roughly the same for all networks, another in which the
number of states is equivalent. In either case, we shall make these numbers large
enough that most of the networks can achieve interesting performance levels.
Number of weights. For static networks it is well known that the generalization
performance is related to the number of weights in the network. Although this
theory has never been extended to recurrent neural networks, it seems reasonable
that a similar result might apply. Therefore, in some experiments we shall try
to keep the number of weights approximately equal across all networks.
Number of states. It can be argued that for dynamic problems the size of the
state space is a more relevant measure for comparison than the number of
weights. Therefore, in some experiments we shall keep the number of states
equal across all networks.
Vanilla learning. Several heuristics have been proposed to help speed learning
and improve generalization of gradient descent learning algorithms. However,
such heuristics may favor certain architectures. In order to avoid these issues,
we have chosen simple gradient descent learning algorithms.
Number of simulations. Due to random initial conditions, the recurrent
neural network solutions can vary widely. Thus, to try to achieve a statistically
significant estimation of the generalization of these networks, a large number of
experiments were run.

700

Bill G. Horne, C. Lee Giles

o

stan

);::===:====,O'l+------ll
o
o

Figure 2: A randomly generated six state finite state machine.

3.2

Finite state machines

We chose two finite state machine (FSM) problems for a comparison of the ability of
the various recurrent networks to perform grammatical inference. The first problem
is to learn the minimal, randomly generated six state machine shown in Figure 2.
The second problem is to infer a sixty-four state finite memory machine [6] described
by the logic function

y(k) = u(k - 3)u(k)

+ u(k -

3)y(k - 3) + u(k)u(k - 3)Y(k - 3)

where u(k) and y(k) represent the input and output respectively at time k and x
represents the complement of x.
Two experiments were run. In the first experiment all of the networks were designed
such that the number of weights was less than, but as close to 60 as possible. In the
second experiment, each network was restricted to six state variables, and if possible,
the networks were designed to have approximately 75 weights. Several alternative
architectures were tried when it was possible to configure the architecture differently
and yield the same number of weights, but those used gave the best results.
A complete set of 254 strings consisting of all strings of length one through seven is
sufficient to uniquely identify both ofthese FSMs. For each simulation, we randomly
partitioned the data into a training and testing set consisting of 127 strings each.
The strings were ordered lexographically in the training set.
For each architecture 100 runs were performed on each problem. The on-line Back
Propagation Through Time (BPTT) algorithm was used to train the networks.
Vanilla learning was used with a learning rate of 0.5. Training was stopped at 1000
epochs. The weights of all networks were initialized to random values uniformly
distributed in the range [-0.1,0.1]. All states were initialize to zeros at the beginning of each string except for the High Order net in which one state was arbitrarily
initialized to a value of 1.
Table 2 summarizes the statistics for each experiment. From these results we draw
the following conclusions.
?

?

The bilinear and high-order networks do best on the small randomly generated
machine, but poorly on the finite memory machine. Thus, it would appear that
there is benefit to having second order terms in the network, at least for small
finite state machine problems.
Narendra and Parthasarathy's model and the network with local recurrence do
far better than the other networks on the problem of inferring the finite memory

An Experimental Comparison of Recurrent Neural Networks

701

Table 2: Percentage classification error on the FSM experiment for (a) networks with
approximately the same number of weights, (b) networks with the same number of
state variables. %P = The percentage of trials in which the training set was learned
perfectly, #W = the number of weights, and #S = the number of states.
F5M

Architecture t

N&P
TDNN
RND

Gamma
First Order
High Order
Bilinear
Quadratic
Mullilayer
Elman
Local

N&P
TDNN
FMM

Gamma
First Order
High Order
Bilinear
Quadratic
Multilayer
Elman
Local

training
mean
2 .8
12.5
19.6
12.9
0.8
1.3
12.9
19 .4
3.5
2. 8
0 .0
6.9
7.7
4 .8
5.3
9 .5
32.5
36. 7
12.0
0.1

error
( std)

(M)
(2.1)

(H)
(6.9)

(1.5)
(2 . 7)
(13.4)
(13 .6)

~5.~~
1.5
~0 . 2 ~
(2 .1 )
(2 .2)
(3 .0)

(4.0)
(10 .4)
(10.8)
(11.9)
(12.5)
' (0.3)

testing error
(std)
mea.n
16.9
(8 .6)
33.8
(U)
24 .8
(3 .2)
26 .5
(9 .0)
6 .2
(6 .1 )
5 .7
(6 .1)
17.7
(14 .1)
23 .4
( 13.5)
12.7
~9.7.6!~
26 .7
0 .1
15 .8
15.7
16 .0
26 .0
25 . 8
40.5
43 .5
24 .9
1.0

~1 .~~

(3 .2)
(3.3)
(6 .5)
( 5. 1 )
(7 .0)
(7 .3)
(8.5)
(7 .9)
( 3 .0)

'YoP
22
0
0
0
60
46
12
6
27
4
99
0
0
1
1
0
0
0
5
97

#W
56
56
56
48
50
55
45
54
55
60

#5
8
8
8
6
5
5
3
4
6
20

56
56
56
48
50
55
45

8
8
8
6
5
5
3
4
6
20

54

55
60

(a)
F5M

Architecture tt

N&P
TDNN
RND

Gamma
First Order
High Order
Bilinear
Quadratic
Mullilayer
Elman
Local

N&P
TDNN
FMM

Gamma
Firs t Order
High Order
Bilinear
Quadratic
MullUayer
Elman
Local

tra.lnlng
mea.n
4 .6
11 . 7
19.0
12.9
0 .3
0 .6
0 .2
15. 4
3.5
13.9

0 .1
6 .8
9 .0
4 .8
1.2
2 .6
12.6
38.1
12.8
15 .3

error
( std)

( 8.~~
( 2.0)

(H)
( 6.9)
( 0 .5)
( 0 .9)
( 0 .5)
(14 . 1)
( 5.5 )

( 405)

( 0 .8)

( 1.7)
(2.9)
(3 .0)

( 1.7)
( 402)
(17.3)
(12.6)

~H.:~
3 .8

testIng
mea.n
14.1
34.3
25 .2
26 .5
4 .6
4 .4
3.2
19.9
12.7
20.2
0 .3
16.2
14.9
16.0
25.1
20 .3
26.1
42.8
27.6
22.2

error
( std)
(11 .3 )
( 3 .9)
(3.1)
(9 .0)
( 5 .1)

( U)
( 2 .6)

(lU)
( 9 .1)

( 5.7)

( 1.4)
( 2 .9)
(2 .8)
(6 .5)
( 5 .1)
( 7 .2)
(12 .8)
( 9.2)
(10 .7)

( 409)

'YoP
38
0
0
0
79
55
83
16
27
0

#W
73
73
H
48
H
78
216
76
55
26

#5
6
6
6
6
6
6
6
6
6
6

97
0
0
1
31
21
13
0
8
0

73
73
73
48
H
78
216
76
55
26

6
6
6
6
6
6
6
6
6
6

(b)
tThe TDNN and Gamma network both had 8 input taps and 4 hidden layer nodes. For
the Gamma network, I' = 0.3 (RND) and I' = 0.7 (FMM). Narendra and Parthasarathy's
network had 4 input and output taps and 5 hidden layer nodes. The High-order network
used a ""one-hot"" encoding of the input values [5]. The multilayer network had 4 hidden
and output layer nodes. The locally recurrent net had 4 hidden layer nodes with 5 input
and 3 output taps, and one output node with 3 input and output taps.
ttThe TDNN, Gamma network, and N arendra and Parthasarathy's network all had 8
hidden layer nodes. For the Gamma network, I' = 0.3 (RND) and I' = 0.7 (FMM). The
High-order network again used a ""one-hot"" encoding of the input values. The multilayer
network had 5 hidden and 6 output layer nodes. The locally recurrent net had 3 hidden
layer nodes and one output layer node, all with only one input and output tap.

702

Bill G. Horne, C. Lee Giles

machine when the number of states is not constrained. It is not surprising that
the former network did so well since the sequential machine implementation of
a finite memory machine is similar to this architecture [6]. However, the result
for the locally recurrent network was unexpected.
? All of the recurrent networks do better than the TDNN on the small random
machine. However, on the finite memory machine the TDNN does surprisingly
well, perhaps because its structure is similiar to Narendra and Parthasarathy's
network which was well suited for this problem.
? Gradient-based learning algorithms are not adequate for many of these architectures. In many cases a network is capable of representing a solution to a
problem that the algorithm was not able to find. This seems particularly true
for the Multilayer network.
? Not surprisingly, an increase in the number of weights typically leads to overtraining. Although, the quadratic network, which has 216 weights, can consistently find solutions for the random machine that generalize well even though
there are only 127 training samples.
? Although the performance on the training set is not always a good indicator of'
generalization performance on the testing set, we find that if a network is able
to frequently find perfect solutions for the training data, then it also does well
on the testing data.
3.3

Nonlinear system identification

In this problem, we train the network to learn the dynamics of the following set of
equations proposed in [8]

zl(k)

Z2 ( k )
1

+

y(k)

+ 2z2(k)

l+z~(k)

zl(k+l)

=

+u

(k)

zl(k)Z2(k)
+ u (k)
1 + z~(k)
zl(k) + z2(k)

based on observations of u( k) and y( k) alone.
The same networks that were used for the finite state machine problems were used
here, except that the output node was changed to be linear instead of sigmoidal
to allow the network to have an appropriate dynamic range. We found that this
caused some stability problems in the quadratic and locally recurrent networks. For
the fixed number of weights comparison, we added an extra node to the quadratic
network, and dropped any second order terms involving the fed back output. This
gave a network with 64 weights and 4 states. For the fixed state comparison,
dropping the second order terms gave a network with 174 weights. The locally
recurrent network presented stability problems only for the fixed number of weights
comparison. Here, we used a network that had 6 hidden layer nodes and one output
node with 2 taps on the inputs and outputs each, giving a network with 57 weights
and 16 states. In the Gamma network a value of l' 0.8 gave the best results.

=

The networks were trained with 100 uniform random noise sequences of length 50.
Each experiment used a different randomly generated training set. The noise was

An Experimental Comparison of Recurrent Neural Networks

703

Table 3: Normalized mean squared error on a sinusoidal test signal for the nonlinear
system identification experiment.
Archi teet ure
N&P
TDNN
Gamma
First Order
High Order
Bilinear
Quadratic
Multilayer
Elman
Local

Fixed

#

weights

0.101
0.160
0.157
0.105
1.034
0.118
0.108
0.096
0.115
0.117

Fixed

#

states

0.067
0.165
0.151
0.105
1.050
0.111
0.096
0.084
0.115
0.123

uniformly distributed in the range [-2.0,2.0], and each sequence started with an
initial value of Xl(O) = X2(0) = O. The networks were tested on the response to
a sine wave of frequency 0.04 radians/second. This is an interesting test signal
because it is fundamentally different than the training data.
Fifty runs were performed for each network. BPTT was used for 500 epochs with a
learning rate of 0.002. The weights of all networks were initialized to random values
uniformly distributed in the range [-0.1,0.1].
Table 3 shows the normalized mean squared error averaged over the 50 runs on the
testing set. From these results we draw the following conclusions.
?

?
?

?

4

The high order network could not seem to match the dynamic range of its output
to the target, as a result it performed much worse than the other networks. It is
clear that there is benefit to adding first order terms since the bilinear network
performed so much better.
Aside from the high order network, all of the other recurrent networks performed
better than the TDNN, although in most cases not significantly better.
The multilayer network performed exceptionally well on this problem, unlike the
finite state machine experiments. We speculate that the existence of target output at every point along the sequence (unlike the finite state machine problems)
is important for the multilayer network to be successful.
Narendra and Parthasarathy's architecture did exceptionally well, even though
it is not clear that its structure is well matched to the problem.

Conclusions

We have reviewed many discrete-time recurrent neural network architectures and
compared them on two different problem domains, although we make no claim that
any of these results will necessarily extend to other problems.
Narendra and Parthasarathy's model performed exceptionally well on the problems
we explored. In general, single layer networks did fairly well, however it is important
to include terms besides simple state/input products for nonlinear system identification. All of the recurrent networks usually did better than the TDNN except

704

Bill G. Home, C. Lee Giles

on the finite memory machine problem. In these experiments, the use of averaging
filters as a substitute for taps in the TDNN did not seem to offer any distinct advantages in performance, although better results might be obtained if the value of
J.I. is adapted.
We found that the relative comparison of the networks did not significantly change
whether or not the number of weights or states were held constant. In fact, holding
one of these values constant meant that in some networks the other value varied
wildly, yet there appeared to be little correlation with generalization.
Finally, it is interesting to note that though some are much better than others,
many of these networks are capable of providing adequate solutions to two seemingly
disparate problems.

Acknowledgements
We would like to thank Leon Personnaz and Isabelle Rivals for suggesting we perform the experiments with a fixed number of states.

References
[1] A.D. Back and A.C. Tsoi. FIR and IIR synapses, a new neural network architecture for time series modeling. Neural Computation, 3(3):375-385, 1991.
[2] B. de Vries and J .C. Principe. The gamma model: A new neural model for
temporal processing. Neural Networks, 5:565-576, 1992.
[3] J .L. Elman. Finding structure in time. Cognitive Science, 14:179-211, 1990.
[4] P. Frasconi, M. Gori, and G. Soda. Local feedback multilayered networks.
Neural Computation, 4:120-130, 1992.
[5] C.L. Giles, C .B. Miller, et al. Learning and extracting finite state automata
with second-order recurrent neural networks. Neural Computation, 4:393-405,
1992.
[6] Z. Kohavi. Switching and finite automata theory. McGraw-Hill, NY, 1978.
[7] K.J. Lang, A.H. Waibel, and G.E . Hinton. A time-delay neural network architecture for isolated word recognition. Neural Networks, 3:23-44, 1990.
[8] K.S. Narendra. Adaptive control of dynamical systems using neural networks.
In Handbook of Intelligent Control, pages 141-183. Van Nostrand Reinhold,
NY, 1992.
[9] K.S. Narendra and K. Parthasarathy. Identification and control of dynamical
systems using neural networks. IEEE Trans. on Neural Networks, 1:4-27, 1990.
[10] P. Poddar and K.P. Unnikrishnan. Non-linear prediction of speech signals
using memory neuron networks. In Proc. 1991 IEEE Work. Neural Networks
for Sig. Proc., pages 1-10. IEEE Press, 1991.
[11] A.J. Robinson and F. Fallside. Static and dynamic error propagation networks
with application to speech coding. In NIPS, pages 632-641, NY, 1988. AlP.
[12] R.L . Watrous and G.M. Kuhn . Induction of finite-state automata using
second-order recurrent networks. In NIPS4, pages 309-316, 1992.

"
101,1988,Training Multilayer Perceptrons with the Extended Kalman Algorithm,,101-training-multilayer-perceptrons-with-the-extended-kalman-algorithm.pdf,Abstract Missing,"133

TRAINING MULTILAYER PERCEPTRONS WITH THE
EXTENDED KALMAN ALGORITHM
Sharad Singhal and Lance Wu
Bell Communications Research, Inc.
Morristown, NJ 07960

ABSTRACT
A large fraction of recent work in artificial neural nets uses
multilayer perceptrons trained with the back-propagation
algorithm described by Rumelhart et. a1. This algorithm
converges slowly for large or complex problems such as
speech recognition, where thousands of iterations may be
needed for convergence even with small data sets. In this
paper, we show that training multilayer perceptrons is an
identification problem for a nonlinear dynamic system which
can be solved using the Extended Kalman Algorithm.
Although computationally complex, the Kalman algorithm
usually converges in a few iterations. We describe the
algorithm and compare it with back-propagation using twodimensional examples.

INTRODUCTION
Multilayer perceptrons are one of the most popular artificial neural net
structures being used today. In most applications, the ""back propagation""
algorithm [Rllmelhart et ai, 1986] is used to train these networks. Although
this algorithm works well for small nets or simple problems, convergence is
poor if the problem becomes complex or the number of nodes in the network
become large [Waibel et ai, 1987]. In problems sllch as speech recognition,
tens of thousands of iterations may be required for convergence even with
relatively small elata-sets. Thus there is much interest [Prager anel Fallsiele,
1988; Irie and Miyake, 1988] in other ""training algorithms"" which can
compute the parameters faster than back-propagation anel/or can handle much
more complex problems.
In this paper, we show that training multilayer perceptrons can be viewed as
an identification problem for a nonlinear dynamic system. For linear dynamic
Copyright 1989. Bell Communications Research. Inc.

134

Singhal and Wu

systems with white input and observation noise, the Kalman algorithm
[Kalman, 1960] is known to be an optimum algorithm. Extended versions of
the Kalman algorithm can be applied to nonlinear dynamic systems by
linearizing the system around the current estimate of the parameters.
Although computationally complex, this algorithm updates parameters
consistent with all previously seen data and usually converges in a few
iterations. In the following sections, we describe how this algorithm can be
applied to multilayer perceptrons and compare its performance with backpropagation using some two-dimensional examples.

THE EXTENDED KALMAN FILTER
In this section we briefly outline the Extended Kalman filter. Mathematical
derivations for the Extended Kalman filter are widely available in the
literature [Anderson and Moore, 1979; Gelb, 1974] and are beyond the scope
of this paper.
Consider a nonlinear finite dimensional discrete time system of the form:

x(n+l) = In(x(n? + gn(x(n?w(n),
den) = hn(x(n?+v(n).

(1)

Here the vector x (n) is the state of the system at time n, w (n) is the input,
den) is the observation, v(n) is observation noise and In('), gn('), and h n(')
are nonlinear vector functions of the state with the subscript denoting possible
dependence on time. We assume that the initial state, x (0), and the
sequences {v (n)} and {w (n)} are independent and gaussian with

E [x (O)]=x(O), E {[x (O)-x (O)][x (O)-i(O?)I} = P(O),
E [w (n)] = 0, E [w (n )w t (l)] = Q (n )Onl'
E[v(n)] = 0, E[v(n)vt(l)] = R(n)onb

(2)

where Onl is the Kronecker delta. Our problem is to find an estimate i (n +1)
of x (n +1) given d (j) , O<j <n. We denote this estimate by i (n +11 n).
If the nonlinearities in (1) are sufficiently smooth, we can expand them llsing
Taylor series about the state estimates i (n In) and i (n In -1) to obtain

In(x(n? = I"" (i(n In? + F(n)[x(n)-i(n In)] + ...
gn(x(n? = gil (i(n In? + ... = C(n) + ...
hn(x(n? = hll(i(n In-I? + J-f1(n)[x(n)-i(n In-1)] +
where
C(ll) = gn(i(n Ill?,
din (x)
F (ll ) = - - - .-ax

x = .i (II III)

, I-P

dh ll (x)
(n ) = --.,--Ox

(3)
x=i(IIII1-1)

i.e. G (n) is the value of the function g"" (.) at i (n In) and the ij th
components of F (n) and H' (n) are the partial derivatives of the i th
components of f II ( . ) and hll (-) respectively with respect to the j th component
of x (n) at the points indicated. Neglecting higher order terms and assuming

Training Multilayer Perceptrons

knowledge of i (n In) and i (n In-I), the system in (3) can be approximated
as
x(n+l) = F(n)x(n) + G(n)w(n)
z (n ) = HI (n )x (n )+ v (n) + y (n ),

+ u(n)

(4)

n>O

where

(5)

u(n) = /n(i(n In? - F(n)i(n In)
y(n) = hn(i(n In-I? - H1(n)i(n In-1).

It can be shown [Anderson and Moore, 1979] that the desired estimate

i (n + 11 n) can be obtained by the recursion

i(n+1In) =/n(i(n In?
i(n In) = i(n In-I) + K(n)[d(n) - hn(i(n In-1?]
K(n) = P(n In-I)H(n)[R(n)+HI(n)P(n In-I)H(n)tl
P(n+Iln) = F(n)P(n In)FI(n) + G(n)Q(n)G1(n)
P(n In) = P(n In-I) - K(n)HI(n)P(n In-I)

(6)
(7)
(8)
(9)
(10)

with P(11 0) = P (0). K (n) is known as the Kalman gain. In case of a linear
system, it can be shown that P(n) is the conditional error covariance matrix
associated with the state and the estimate i (n +1/ n) is optimal in the sense
that it approaches the conditional mean E [x (n + 1) Id (0) ... d (n)] for large
n . However, for nonlinear systems, the filter is not optimal and the estimates
can only loosely be termed conditional means.

TRAINING MULTILAYER PERCEPTRONS
The network under consideration is a L layer perceptronl with the i th input
of the k th weight layer labeled as :J-l(n), the jth output being zjk(n) and the
weight connecting the i th input to the j th output being (}i~j' We assume that
the net has m inputs and I outputs. Thresholds are implemented as weights
connected from input nodes 2 with fixed unit strength inputs . Thus, if there
are N (k) nodes in the k th node layer, the total number of weights in the
system is
L

M = ~N(k-l)[N(k)-l].

(11)

k=1

Although the inputs and outputs are dependent on time 11, for notational
brevity, we wil1 not show this dependence unless explicitly needed .
l.

We use the convention that the number of layers is equal to the number of weight layers . Thus
we have L layers of Wl'iglrls labeled 1 ?
L and I ~ + I layer s of /lodes (including the input and
output nodes) labeled O ? . . L . We will refer to the kth weight layer or the kth node layer
unless the cont ext is clear.

2.

We adopt the convention that the 1st input node is the threshold. i.e.
the j th output node from the k th weight layer.

lit., is

the threshold for

135

136

Singhal and Wu

In order to cast the problem in a form for recursive estimation, we let the
weights in the network constitute the state x of the nonlinear system, i.e.

x = [Ob,Ot3 ... 0k(O),N(l)]t.

(12)

The vector x thus consists of all weights arranged in a linear array with
dimension equal to the total number of weights M in the system. The system
model thus is

x(n+l)=x(n) n>O,
den) = zL(n) + v(n) = hn(x(n),zO(n))

+ v(n),

(13)
(14)

where at time n, zO(n) is the input vector from the training set, d (n) is the
corresponding desired output vector, and ZL (n) is the output vector
produced by the net. The components of h n (.) define the nonlinear
relationships between the inputs, weights and outputs of the net. If r(?) is the
nonlinearity used, then ZL (n) = h n(x (n ),zO(n)) is given by

zL(n) = r{(OL)tr{(OL-l)tr ... r{(OlyzO(n)}? .. }}..

(15)

where r applies componentwise to vector arguments. Note that the input
vectors appear only implicitly through the observation function h n ( . ) in (14).
The initial state (before training) x (0) of the network is defined by populating
the net with gaussian random variables with a N(x(O),P(O)) distribution where
(0) and P (0) reflect any apriori knowledge about the weights. In the absence
of any such knowledge, a N (0,1/f. I) distribution can be used, where f. is a
small number and I is the identity matrix. For the system in (13) and (14),
the extended Kalman filter recursion simplifies to

x

i(I1+1) = i(n) + K(n)[d(n) - hn(i(n),zO(n))]
K (n) = P(n)H (n )[R (n )+H' (n )P(n )H(n )]-1
Pen +1) = P(n) - K (n )Ht (n)P (n)

(16)
(17)
(18)

where P(n) is the (approximate) conditional error covariance matrix .
Note that (16) is similar to the weight update equation in back-propagation
with the last term [ZL - h n (x ,ZO)] being the error at the output layer.
However, unlike the delta rule used in back-propagation, this error is
propagated to the weights through the Kalman gain K (n) which updates each
weight through the entire gradient matrix H (n) and the conditional error
covariance matrix P (n ). In this sense, the Kalman algorithm is not a local
training algorithm . However, the inversion required in (17) has dimension
equal to the llumber of outputs I, 110t the number of weights M, and thus
does not grow as weights arc added to the problem.

EXAMPLES AND RESULTS
To evaluale the Olltpul and the convergence properties of the extended
Kalman algorithm. we constructed mappings using two-dimensional inputs
with two or four outputs as shown in Fig. 1. Limiting the input vector to 2
dimensions allows liS to visualize the decision regiolls ohtained by the net and

Training Multilayer Perceptrons

to examine the outputs of any node in the net in a meaningful way. The xand y-axes in Fig. 1 represent the two inputs, with the origin located at the
center of the figures. The numbers in the figures represent the different
output classes.

2
-

-

1

t------+-----I

1

2

I
(a) REGIONS

(b) XOR

Figure 1. Output decision regions for two problems

The training set for each example consisted of 1000 random vectors uniformly
filling the region . The hyperbolic tangent nonlinearity was used as the
nonlinear element in the networks. The output corresponding to a class was
set to 0.9 when the input vector belonged to that class, and to -0.9 otherwise.
During training, the weights were adjusted after each data vector was
presented. Up to 2000 sweeps through the input data were used with the
stopping criteria described below to examine the convergence properties. The
order in which data vectors were presented was randomized for each sweep
through the data. In case of back-propagation, a convergence constant of 0.1
was used with no ""momentum"" factor. In the Kalman algorithm R was set to
I ?e-k / 50 , where k was the iteration number through the data. Within each
iteration, R was held constant.
The Stopping Criteria

Training was considered complete if anyone of the following
satisfied:

con~itions

was

a.

2000 sweeps through the input data were used,

h.

the RMS (root mean squared) error at the output averaged over all
training data during a sweep fell below a threshold 11' or

c.

the error reduction 8 after the i th sweep through the data fell below a
threshold I::., where 8; = !3b;_1 + (l-,B) Iei-ei_l I. Here !3 is some
positive constant less than unity, and ei is the error defined in b.
In our simulations we set ;3 = 0.97, II = 10-2 and 12 = 10- 5 ?

137

138

Singhal and Wu

Example 1 - Meshed, Disconnected Regions:

Figure l(a) shows the mapping with 2 disconnected, meshed regions
surrounded by two regions that fill up the space. We used 3-layer perceptrons
with 10 hidden nodes in each hidden layer to Figure 2 shows the RMS error
obtained during training for the Kalman algorithm and back-propagation
averaged over 10 different initial conditions. The number of sweeps through
the data (x-axis) are plotted on a logarithmic scale to highlight the initial
reduction for the Kalman algorithm. Typical solutions obtained by the
algorithms at termination are shown in Fig. 3. It can be seen that the Kalman
algorithm converges in fewer iterations than back-propagation and obtains
better solutions.
1

0.8
Average 0.6
RMS
Error 0.4

backprop

0.2
Kalman

0
2

1

5

10

20
50 100 200
No. of Iterations

500 10002000

Figure 2. Average output error during training for Regions problem using the
Kalman algorithm and backprop

I

I

(a)
(b)
Figure 3. Typical solutions for Regions problem using (a) Kalman algorithm
and (h) hackprop.

Training Multilayer Perceptrons

Example 2 - 2 Input XOR:

Figure 1(b) shows a generalized 2-input XOR with the first and third
quadrants forming region 1 and the second and fourth quadrants forming
region 2. We attempted the problem with two layer networks containing 2-4
nodes in the hidden layer. Figure 4 shows the results of training averaged
over 10 different randomly chosen initial conditions. As the number of nodes
in the hidden layer is increased, the net converges to smaller error values.
When we examine the output decision regions, we found that none of the nets
attempted with back-propagation reached the desired solution. The Kalman
algorithm was also unable to find the desired solution with 2 hidden nodes in
the network. However, it reached the desired solution with 6 out of 10 initial
conditions with 3 hidden nodes in the network and 9 out of 10 initial
conditions with 4 hidden nodes. Typical solutions reached by the two
algorithms are shown in Fig. 5. In all cases, the Kalman algorithm converged
in fewer iterations and in all but one case, the final average output error was
smaller with the Kalman algorithm.
1
0.8
Average 0.6
RMS
Error 0.4
Kalman 3 nodes

0.2

Kalman 4 nodes

0

1

2

5

10

50 100 200
20
No. of Iterations

500 10002000

Figure 4. Average output error during training for XOR problem using the
Kalman algorithm and backprop

CONCLUSIONS
In this paper, we showed that training feed-forward nets can be viewed as a
system identification problem for a nonlinear dynamic system. For linear
dynamic systems, the Kalman tllter is known to produce an optimal estimator.
Extended versions of the Kalman algorithm can be used to train feed-forward
networks. We examined the performance of the Kalman algorithm using
artifkially constructed examples with two inputs and found that the algorithm
typically converges in a few iterations. We also llsed back-propagation on the
same examples and found that invariably, the Kalman algorithm converged in

139

140

Singhal and Wu

l
2

1

~

1

2

I

2

I

I

(a)

(b)

Figure 5. Typical solutions for XOR problem using (a) Kalman algorithm and

(b) backprop.
fewer iterations. For the XOR problem, back-propagation failed to converge
on any of the cases considered while the Kalman algorithm was able to find
solutions with the same network configurations.
References

[1]

B. D. O. Anderson and J. B. Moore, Optimal Filtering, Prentice Hall,
1979.

[2]

A. Gelb, Ed., Applied Optimal Estimation, MIT Press, 1974.

[3]

B. Irie, and S. Miyake, ""Capabilities of Three-layered Perceptrons,""
Proceedings of the IEEE International Conference on Neural Networks,
San Diego, June 1988, Vol. I, pp. 641-648.

[4]

R. E. Kalman, ""A New Approach to Linear Filtering and Prediction
Problems,"" 1. Basic Eng., Trans. ASME, Series D, Vol 82, No.1, 1960,
pp.35-45.

[5]

R. W. Prager and F. Fallside, ""The Modified Kanerva Model for
Automatic Speech Recognition,"" in 1988 IEEE Workshop on Speech
Recognition, Arden House, Harriman NY, May 31-Jllne 3,1988.

[6]

D. E. Rumelharl, G. E. Hinton and R. J. Williams, ""Learning Internal
Representations by Error Propagation,"" in D. E. Rllmelhart and
J. L. McCelland (Eds.), Parallel Distributed Processing: Explorations in
the Microstructure oj' Cognition. Vol 1: Foundations. MIT Press, 1986.

[7J

A. Waibel, T. Hanazawa, G. Hinton, K. Shikano and K . Lang
""Phoneme Recognition Using Time-Delay Neural Networks,"" A 1R
internal Report TR-I-0006, October 30, 1987.

"
1010,1994,Interference in Learning Internal Models of Inverse Dynamics in Humans,,1010-interference-in-learning-internal-models-of-inverse-dynamics-in-humans.pdf,Abstract Missing,"Interference in Learning Internal
Models of Inverse Dynamics in Humans

Reza Shadmehr; Tom Brashers-Krug, and Ferdinando Mussa-lvaldi t
Dept. of Brain and Cognitive Sciences
M. I. T., Cambridge, MA 02139
reza@bme.jhu.edu, tbk@ai.mit.edu, sandro@parker.physio.nwu.edu

Abstract
Experiments were performed to reveal some of the computational
properties of the human motor memory system. We show that
as humans practice reaching movements while interacting with a
novel mechanical environment, they learn an internal model of the
inverse dynamics of that environment. Subjects show recall of this
model at testing sessions 24 hours after the initial practice. The
representation of the internal model in memory is such that there
is interference when there is an attempt to learn a new inverse
dynamics map immediately after an anticorrelated mapping was
learned. We suggest that this interference is an indication that
the same computational elements used to encode the first inverse
dynamics map are being used to learn the second mapping. We
predict that this leads to a forgetting of the initially learned skill.

1

Introduction

In tasks where we use our hands to interact with a tool, our motor system develops
a model of the dynamics of that tool and uses this model to control the coupled
dynamics of our arm and the tool (Shadmehr and Mussa-Ivaldi 1994). In physical
systems theory, the tool is a mechanical analogue of an admittance, mapping a force
as input onto a change in state as output (Hogan 1985). In this framework, the
?Currently at Dept. Biomedical Eng, Johns Hopkins Univ, Baltimore, MD 21205
tCurrently at Dept. Physiology, Northwestern Univ Med Sch (M211), Chicago, IL 60611

1118

Reza Shadmehr, Tom Brashers-Krug, Ferdinando Mussa-Ivaldi

Figure 1: The experimental setup. The robot is
a very low friction planar mechanism powered by
two torque motors that act on the shoulder and
elbow joints. Subject grips the end-point of the
robot which houses a force transducer and moves
the hand to a series of targets displayed on a monitor facing the subject (not shown) . The function of
the robot is to produce novel force fields that the
subject learns to compensate for during reaching
movements.

model developed by the motor control system during the learning process needs to
approximate an inverse of this mapping . This inverse dynamics map is called an
internal model of the tool.
We have been interested in understanding the representations that the nervous
system uses in learning and storing such internal models. In a previous work we
measured the way a learned internal model extrapolated beyond the training data
(Shadmehr and Mussa-Ivaldi 1994). The results suggested that the coordinate system of the learned map was in intrinsic (e.g., joint or muscles based) rather than in
extrinsic (e.g., hand based) coordinates. Here we present a mathematical technique
to estimate the input-output properties of the learned map. We then explore the
issue of how the motor memory might store two maps which have similar inputs
but different outputs.

2

Quantifying the internal model

In our paradigm, subjects learn to control an artificial tool: the tool is a robot
manipulandum which has torque motors that can be programmed to produce a
variety of dynamical environments (Fig. 1). The task for the subject is to grasp
the end-effector and make point to point reaching movements to a series of targets.
The environments are represented as force fields acting on the subject's hand, and a
typical case is shown in Fig. 2A. A typical experiment begins with the robot motors
turned off. In this ""null"" environment subjects move their hand to the targets in a
smooth, straight line fashion. When the force field is introduced, the dynamics of the
task change and the hand trajectory is significantly altered (Shadmehr and MussaIvaldi 1994). With practice (typically hundreds of movements), hand trajectories
return to their straight line path. We have suggested that practice leads to formation
of an internal model which functions as an inverse dynamics mapping, i.e., from a
desired trajectory (presumably in terms of hand position and velocity, Wolpert et
al. 1995) to a prediction of forces that will be encountered along the trajectory. We
designed a method to quantify these forces and estimate the output properties of
the internal model.
If we position a force transducer at the interaction point between the robot and the
subject, we can write the dynamics of the four link system in Fig. 1 in terms of the

Interference in Learning Internal Models of Inverse Dynamics in Humans

following coupled vector differential equations:
Ir(P)P + Gr(p,p)p = E(p,p)

III (q)q + GII(q, q)q

= C(q, q, q*(t?

1119

+ J'{ F

(1)

- f; F

(2)

where I and G are inertial and Corriolis/centripetal matrix functions, E is the
torque field produced by the robot's motors, i.e., the environment, F is the force
measured at the handle of the robot, C is the controller implemented by the motor
system of the subject, q*(t) is the reference trajectory planned by the motor system
of the subject, J is the Jacobian matrix describing the differential transformation
of coordinates from endpoint to joints, q and p are joint positions of the subject
and the robot, and the subscripts sand r denote subject or robot matrices.

?

In the null environment, i.e., E = in Eq. (1), a solution to this coupled system
is q = q*(t) and the arm follows the reference trajectory (typically a straight hand
path with a Gaussian tangential velocity profile). Let us name the controller which
accomplishes this task C = Co in Eq. (2). When the robot motors are producing a
force field E # 0, it can be shown that the solution is q = q*(t) if and only if the
new controller in Eq. (2) is C = C1 = Co + f[ J;T E. The internal model composed
by the subject is C 1 - Co, i.e., the change in the controller after some training
period. We can estimate this quantity by measuring the change in the interaction
force along a given trajectory before and after training. If we call these functions
Fo and FI, then we have:
Fo(q, q, ij, q*(t?
J;T(Co - IlIq - Gllq)
(3)

FI(q,q,ij,q*(t?
JII-T(Co+f;J;TE-Illq-Gllq)
(4)
The functions Fo and FI are impedances of the subject's arm as viewed from the
interaction port. Therefore, by approximating the difference FI - F o, we have an
estimate of the change in the controller. The crucial assumption is that the reference
trajectory q*(t) does not change during the training process.
In order to measure Fo, we had the subjects make movements in a series of environments. The environments were unpredictable (no opportunity to learn) and
their purpose was to perturb the controller about the reference trajectory so we
could measure Fo at neighboring states. Next, the environment in Fig. 2A was
presented and the subject given a practice period to adapt. After training, FI was
estimated in a similar fashion as Fo. The difference between these two functions was
calculated along all measured arm trajectories and the results were projected onto
the hand velocity space. Due to computer limitations, only 9 trajectories for each
target direction were used for this approximation. The resulting pattern of forces
were interpolated via a sum of Gaussian radial basis functions, and are shown in
Fig. 2B. This is the change in the impedance of the arm and estimates the inputoutput property of the internal model that was learned by this subject. We found
that this subject, which provided some of best results in the test group, learned to
change the effective impedance of his arm in a way that approximated the imposed
force field. This would be a sufficient condition for the arm to compensate for the
force field and allow the hand to follow the desired trajectory. An alternate strategy
might have been to simply co-contract arm muscles: this would lead to an increased
stiffness and an ability to resist arbitrary environmental forces. Figure 2B suggests
that practice led to formation of an internal model specific to the dynamics of the
imposed force field.

Reza Shadmehr, Tom Brashers-Krug, Ferdinando Mussa-Ivaldi

1120

A
-200

0

200

..... _<...-)

...

B

"","",,-<...-)

Figure 2: Quantification of the change in impedance of a subject's arm after learning a
force field. A: The force field produced by the robot during the training period. B: The
change in the subject's arm impedance after the training period, i.e., the internal model.

2.1

Formation of the internal model in long-term memory

Here we wished to determine whether subjects retained the internal model in longterm motor memory. We tested 16 naive subjects. They were instructed to move
the handle of the robot to a sequence of targets in the null environment. Each
movement was to last 500 ? 50 msec . They were given visual feedback on the
timing of each movement. After 600 movements, subjects were able to consistently
reach the targets in proper time. These trajectories constituted a baseline set.
Subjects returned the next day and were re-familiarized with the timing of the
task. At this point a force field was introduced and subjects attempted to perform the exact task as before: get to the target in proper time. A sequence of 600
targets was given. When first introduced, the forces perturbed the subject's trajectories, causing them to deviate from the straight line path. As noted in previous
work (Shadmehr and Mussa-Ivaldi 1994), these deviations decreased with practice.
Eventually, subject's trajectories in the presence of the force field came to resemble
those of the baseline, when no forces were present. The convergence of the trajectories to those performed at baseline is shown for all 16 subjects in Fig. 3A. The
timing performance of the subjects while moving in the field is shown in Fig. 3B.
In order to determine whether subjects retained the internal model of the force
field in long-term memory, we had them return the next day (24 to 30 hours later)
and once again be tested on a force field. In half of the subjects, the force field
presented was one that they had trained on in the previous day (call this field 1).
In the other half, it was a force field which was novel to the subjects, field 2. Field
2 had a correlation value of -1 with respect to field 1 (i.e., each force vector in
field 2 was a 180 degree rotation of the respective vector in field 1). Subjects who
were tested on a field that they had trained on before performed significantly better
(p < 0.01) than their initial performance (Fig. 4A), signifying retention. However,
those who were given a field that was novel performed at naive levels (Fig. 4B).
This result suggested that the internal model formed after practice in a given field
was (1) specific to that field: performance on the untrained field was no better than

1121

Interference in Learning Internal Models of Inverse Dynamics in Humans

0.9
0.85

?-; 0.9

.~ 0.8

~

:?
~ 0.75

8

0.7

i

08

~

0.7

0.85

A

0

100

200

300

400

500

600

0.6

B

0

100

Movemen1 N!mber

200

300

400

500

600

Movement Number

Figure 3: Measures of performance during the training period (600 movements) for 16
naive subjects. Short breaks (2 minutes) were given at intervals of 200 movements. A :
Mean ? standard error (SE) of the correlation coefficient between hand trajectory in a
null environment (called baseline trajectories, measured before exposure to the field) , and
trajectory in the force field. Hand trajectories in the field converge to that in the null field
(i.e. , become straight, with a bell shaped velocity profile). B: Mean ? SE of the movement
period to reach a target. The goal was to reach the target in 0.5 ? 0.05 seconds.

I

0.8

1.,

1

0.75

., 0.9

E
i=

A

I:: \~ , ~ ',
:::;; 0.6

;

~iIJIJ
,l,ll,lI""
0)
""T' I,."", 1f11T'1

Y!1y~

o

100

E
i=

200

300

400

500

600

0.7

~ 0.65
E

~

0.8

:::;;

0.55

B

0

Movement Number

100

200

300

400

500

600

Movement Number

Figure 4: Subjects learned an internal model specific to the field and retained it in longterm memory. A: Mean ? standard error (SE) of the movement period in the force field
(called field 1) during initial practice session (upper trace) and during a second session
24-30 hours after the initial practice (lower trace). B: Movement period in a different
group of subjects during initial training (dark line) in field 1 and test in an anti-correlated
field (called field 2) 24-30 hours later (gray line).

performance recorded in a separate set of naive subjects who were given than field
in their initial training day; and (2) could be retained, as evidenced by performance
in the following day.
2.2

Interference effects of the motor memory

In our experiment the ""tool"" that subjects learn to control is rather unusual , nevertheless, subjects learn its inverse dynamics and the memory is used to enhance
performance 24 hours after its initial acquisition. We next asked how formation
of this memory affected formation of subsequent internal models. In the previous
section we showed that when a subject returns a day after the initial training, although the memory of the learned internal model is present , there is no interference
(or decrement in performance) in learning a new, anti-correlated field . Here we
show that when this temporal distance is significantly reduced, the just learned

1122

Reza Shadmehr, Tom Brashers-Krug, Ferdinanda Mussa-Ivaldi

200

300

400

Movement Number

Figure 5: Interference in sequential learning of two uncorrelated force fields: The lower
trace is the mean and standard error of the movement periods of a naive group of subjects
during initial practice in a force field (called field 1). The upper trace is the movement period of another group of naive subjects in field 1, 5 minutes after practicing 400 movements
in field 2, which was anti-correlated with field 1.

model interferes with learning of a new field.
Seven new subjects were recruited. They learned the timing of the task in a null
environment and in the following day were given 400 targets in a force field (called
field 1). They showed improvement in performance as before. After a short break
(5-10 minutes in which they walked about the lab or read a magazine), they were
given a new field: this field was called field 2 and was anti-correlated with respect
to field 1. We found a significant reduction (p < 0.01) in their ability to learn field
2 (Fig. 5) when compared to a subject group which had not initially trained in field
1. In other words, performance in field 2 shortly after having learned field 1 was
significantly worse than that of naives. Subjects seemed surprised by their inability
to master the task in field 2. In order to demonstrate that field 2 in isolation was
no more difficult to learn than field 1, we had a new set of subjects (n = 5) initially
learn field 2, then field 1. Now we found a very large decrement in learn ability of
field 1.
One way to explain the decrement in performance shown in Fig. 5 is to assume that
the same ""computational elements"" that represented the internal model of the first
field were being used to learn the second field.! In other words, when the second field
was given, because the forces were opposite to the first field, the internal model was
badly biased against representing this second field: muscle torque patterns predicted
for movement to a given target were in the wrong direction.
In the connectionist literature this is a phenomenon called temporal interference
(Sutton 1986). As a network is trained, some of its elements acquire large weights
and begin to dominate the input-output transformation. When a second task is
presented with a new and conflicting map (mapping similar inputs to different outputs), there are large errors and the network performs more poorly than a ""naive""
network. As the network attempts to learn the new task, the errors are fed to each
element (i.e., pre-synaptic input). This causes most activity in those elements that
1 Examples of computational elements used by the nervous system to model inverse
dynamics of a mechanical system were found by Shidara et al. (1993), where it was shown
that the firing patterns of a set of Purkinje cells in the cerebellum could be reconstructed
by an inverse dynamic representation of the eye.

Interference in Learning Internal Models of Inverse Dynamics in Humans

1123

had the largest synaptic weight. If the learning algorithm is Hebbian , i.e., weights
change in proportion to co-activation of the pre- and the post-synaptic element,
then the largest weights are changed the most , effectively causing a loss of what
was learned in the first task . Therefore, from a computational stand point, we
would expect that the internal model of field 1 as learned by our subjects should be
destroyed by learning of field 2. Evidence for ""catastrophic interference"" in these
subjects is presented elsewhere in this volume (Brashers-Krug et al. 1995).
The phenomenon of interference in sequential learning of two stimulus-response
maps has been termed proactive interference or negative transfer in the psychological
literature. In humans, interference has been observed extensively in verbal tasks
involving short-term declarative memory (e.g., tasks involving recognition of words
in a list or pairing of non-sense syllables, Bruce 1933, Melton and Irwin 1940,
Sears and Hovland 1941). It has been found that interference is a function of the
similarity of the stimulus-response maps in the two tasks: if the stimulus in the new
learning task requires a response very different than what was recently learned, then
there is significant interference. Interestingly, it has been shown that the amount of
interference decreases with increased learning (or practice) on the first map (Siipola
and Israel 1933).
In tasks involving procedural memory (which includes motor learning, Squire 1986),
the question of interference has been controversial: Although Lewis et al. (1949)
reported interference in sequential learning of two motor tasks which involved moving levers in response to a set of lights, it has been suggested that the interference
that they observed might have been due to cognitive confusion (Schmidt 1988).
In another study, Ross (1974) reported little interference in subjects learning her
motor tasks.
We designed a task that had little or no cognitive components. We found that
shortly after the acquisition of a motor memory, that memory strongly interfered
with learning of a new, anti-correlated input-output mapping. However, this interference was not significant 24 hours after the memory was initially acquired . One
possible explanation is that the initial learning has taken place in a temporary and
vulnerable memory system. With time and/or practice, the information in this
memory had transferred to long-term storage (Brashers-Krug et al. 1995) .
Brain imaging studies during motor learning suggest that as subjects become more
proficient in a motor task, neural fields in the motor cortex display increases in
activity (Grafton et al. 1992) and new fields are recruited (Kawashima et al. 1994) .
It has been reported that when a subject attempts to learn two new motor tasks
successively (in this case the tasks consisted of two sequences of finger movements),
the neural activity in the motor cortex is lower for the second task , even when the
order ofthe tasks is reversed (Jezzard et al. 1994). It remains to be seen whether this
decrement in neural activity in the motor cortex is correlated with the interference
observed when subjects attempt to learn two different input-output mappings in
succession (Gandolfo et al. 1994) .
References
Brashers-Krug T , Shadmehr R, Todorov E (1995) Catastrophic interference in human
motor learning. Adv Neural Inform Proc Syst, vol 7, in press.

1124

Reza Shadmehr, Tom Brashers-Krug, Ferdinando Mussa-Ivaldi

Bruce RW (1933) Conditions of transfer of training. J Exp Psychol 16:343-361.
French, R. (1992) Semi-distributed Representations and Catastrophic Forgetting in Connectionist Networks, Connection Science 4:365-377.
Grafton ST et al. (1992) Functional anatomy of human procedural learning determined
with regional cerebral blood flow and PET. J Neurosci 12:2542-2548.
Gandolfo F, Shadmehr R, Benda B, Bizzi E (1994) Adaptive behavior ofthe monkey motor
system to virtual environments. Soc Neurosci Abs 20(2):1411.
Hogan N (1985) Impedance control: An approach for manipulation: Theory. J Dynam
Sys Meas Cont 107:1-7.
Jezzard P et al. (1994) Practice makes perfect: A functional MRI study oflong term motor
cortex plasticity. 2nd Ann Soc. Magnetic Res., p. 330.
Kawashima R, Roland PE, O'Sullivan BT (1994) Fields in human motor areas involved
in preparation for reaching, actual reaching, and visuomotor learning: A PET study. J
Neurosci 14:3462-3474.
Lewis D, Shephard AH, Adams JA (1949) Evidences of associative interference in psychomotor performance. Science 110:271-273.
Melton AW, Irwin JM (1940) The influence of degree of interpolated learning on retroactive
inhibition and the overt transfer of specific responses. Amer J Psychol 53:173-203.
Ross D (1974) Interference in discrete motor tasks: A test of the theory. PhD dissertation,
Dept. Psychology, Univ. Michigan, Ann Arbor.
Schmidt RA (1988) Motor Control and Learning: A Behavioral Emphasis. Human Kinetics
Books, Champaign IL, pp. 409-411.
Sears RR, Hovland CI (1941) Experiments on motor conflict. J Exp Psychol 28:280-286.
Shadmehr R, Mussa-Ivaldi FA (1994) Adaptive representation of dynamics during learning
of a motor task. J Neuroscience, 14(5):3208- 3224.
Shidara M, Kawano K, Gomi H, Kawato M (1993) Inverse dynamics model eye movement
control by Purkinje cells in the cerebellum. Nature 365:50-52.
Siipola EM, Israel HE (1933) Habit interference as dependent upon stage oftraining. Amer
J Psychol 45:205-227.
Squire LR (1986) Mechanisms of memory. Science 232:1612-1619.
Sutton RS (1986) Two problems with backpropagation and other steepest-descent learning
procedures for networks. Proc 8th Cognitive Sci Soc, pp. 823-831.
Wolpert DM, Ghahramani Z, Jordan MI (1995) Are arm trajectories planned in kimenatic
or dynamic coordinates? An adaptation stUdy. Exp Brain Res, in press.

"
1011,1994,Active Learning with Statistical Models,,1011-active-learning-with-statistical-models.pdf,Abstract Missing,"Active Learning with Statistical Models

David A. Cohn, Zoubin Ghahramani, and Michael I. Jordan
cohnQpsyche.mit.edu. zoubinQpsyche.mit.edu. jordan~syche.mit.edu
Department of Brain and Cognitive Sciences
Massachusetts Institute of Technology
Cambridge, MA 02139

Abstract
For many types of learners one can compute the statistically ""optimal"" way to select data. We review how these techniques have
been used with feedforward neural networks [MacKay, 1992; Cohn,
1994] . We then show how the same principles may be used to select
data for two alternative, statistically-based learning architectures:
mixtures of Gaussians and locally weighted regression. While the
techniques for neural networks are expensive and approximate, the
techniques for mixtures of Gaussians and locally weighted regression are both efficient and accurate.

1

ACTIVE LEARNING - BACKGROUND

An active learning problem is one where the learner has the ability or need to
influence or select its own training data. Many problems of great practical interest
allow active learning, and many even require it.
We consider the problem of actively learning a mapping X - Y based on a set of
training examples {(Xi,Yi)}~l' where Xi E X and Yi E Y. The learner is allowed
to iteratively select new inputs x (possibly from a constrained set), observe the
resulting output y, and incorporate the new examples (x, y) into its training set.
The primary question of active learning is how to choose which x to try next.
There are many heuristics for choosing x based on intuition, including choosing
places where we don't have data, where we perform poorly [Linden and Weber,
1993], where we have low confidence [Thrun and Moller, 1992], where we expect it

706

David Cohn, Zoubin Ghahramani, Michael I. Jordon

to change our model [Cohn et aI, 1990], and where we previously found data that
resulted in learning [Schmidhuber and Storck, 1993].
In this paper we consider how one may select x ""optimally"" from a statistical
viewpoint. We first review how the statistical approach can be applied to neural
networks, as described in MacKay [1992] and Cohn [1994]. We then consider two
alternative, statistically-based learning architectures: mixtures of Gaussians and
locally weighted regression. While optimal data selection for a neural network is
computationally expensive and approximate, we find that optimal data selection for
the two statistical models is efficient and accurate.

2

ACTIVE LEARNING - A STATISTICAL APPROACH

We denote the learner's output given input x as y(x). The mean squared error of
this output can be expressed as the sum of the learner's bias and variance. The
variance 0'3 (x) indicates the learner's uncertainty in its estimate at x. 1 Our goal
will be to select a new example x such that when the resulting example (x, y) is
added to the training set, the integrated variance IV is minimized:

IV =

J0'3

P (x)dx.

(1)

Here, P(x) is the (known) distribution over X. In practice, we will compute a
Monte Carlo approximation of this integral, evaluating 0'3 at a number of random
points drawn according to P(x).
Selecting x so as to minimize IV requires computing 0-3, the new variance at x given
(x, y). Until we actually commit to an x, we do not know what corresponding y we
will see, so the minimization cannot be performed deterministically.2 Many learning
architectures, however, provide an estimate of PWlx) based on current data, so we
can use this estimate to compute the expectation of 0-3. Selecting x to minimize
the expected integrated variance provides a solid statistical basis for choosing new
examples.

2.1

EXAMPLE: ACTIVE LEARNING WITH A NEURAL
NETWORK

In this section we review the use of techniques from Optimal Experiment Design
(OED) to minimize the estimated variance of a neural network [Fedorov, 1972;
MacKay, 1992; Cohn, 1994] . We will assume we have been given a learner y = fwO,
a training set {(Xi, yd}f;l and a parameter vector til that maximizes a likelihood
measure. One such measure is the minimum sum squared residual

52 =

~
m

f

(Yi - Y(Xi))2.

i=l

lUnless explicitly denoted, fI and O'~ are functions of x. For simplicity, we present our
results in the univariate setting. All results in the paper extend easily to the multivariate
case.
2This contrasts with related work by Plutowski and White [1993], which is concerned
with filtering an existing data set.

Active Learning with Statistical Models

707

The estimated output variance of the network is

O'~ ~ S2
y

(Oy(X ?) T(02 S2) (Oy(X?)
OW 2

ow

-1

OW

The standard OED approach assumes normality and local linearity. These assumptions allow replacing the distribution P(ylx) by its estimated mean y(x) and
variance
The expected value of the new variance, iT~, is then:

S2.

-2)...... 2
(O'g
...... O'g -

x)
S2O'~(x,
+ O'~(x)'

[MacKay, 1992].

(2)

where we define

_(

0' y

_) =
x, x -

S2 (OY(X?)T
(02S2)-1
(Oy(X?)
ow
ow2
ow?

For empirical results on the predictive power of Equation 2, see Cohn [1994] .
The advantages of minimizing this criterion are that it is grounded in statistics,
and is optimal given the assumptions. Furthermore, the criterion is continuous
and differentiable. As such, it is applicable in continuous domains with continuous
action spaces, and allows hillclimbing to find the ""best"" x.
For neural networks, however, this approach has many disadvantages. The criterion
relies on simplifications and strong assumptions which hold only approximately.
Computing the variance estimate requires inversion of a Iwl x Iwl matrix for each
new example, and incorporating new examples into the network requires expensive
retraining. Paass and Kindermann [1995] discuss an approach which addresses some
of these problems.

3

MIXTURES OF GAUSSIANS

The mixture of Gaussians model is gaining popularity among machine learning practitioners [Nowlan, 1991; Specht, 1991; Ghahramani and Jordan, 1994]. It assumes
that the data is produced by a mixture of N Gaussians gi, for i = 1, ... , N. We
can use the EM algorithm [Dempster et aI, 1977] to find the best fit to the data,
after which the conditional expectations of the mixture can be used for function
approximation.
For each Gaussian gi we will denote the estimated input/output means as JLx,i and
JLy,i and estimated covariances as O';,i'
and O'xy,i. The conditional variance of
y given x may then be written

O';,i

We will denote as ni the (possibly fractional) number of training examples for which
gi takes responsibility:

David Cohn, Zoubin Ghahramani, Michael I. Jordon

708

For an input x, each 9i has conditional expectation

Yi = J.Ly,i
A

xy -,i ( X + -0-2
0-

x,i

J.Lx,i ) ,

o-~

.=

y,J

2

Yi

0.
~

and variance (1'~,i:

.)2) .

1 + x - J.Lx,~

((

n'

0- 2 .
XI'

t

These expectations and variances are mixed according to the prior probability that
9i has of being responsible for x:

. = h.( ) _
h,_~x-

P(xli)
.
2:j=l P(xlj)
N

For input x then, the conditional expectation
variance may be written:

Y of the

resulting mixture and its

N

Y=

L hi Yi,
i:::l

In contrast to the variance estimate computed for a neural network, here o-~ can be
computed efficiently with no approximations.

3.1

ACTIVE LEARNING WITH A MIXTURE OF GAUSSIANS

We want to select x to minimize ( Cr~). With a mixture of Gaussians, the model's
estimated distribution of ii given x is explicit:

P(ylx)

N

N

i=l

i=l

= L hiP(ylx, i) = L hiN(Yi(X), o-;lx,i(X)),

=

where hi hi (x). Given this, calculation of ( Cr~) is straightforward: we model the
change in each 9i separately, calculating its expected variance given a new point
sampled from P(ylx, i) and weight this change by hi. The new expectations combine
to form the learner's new expected variance
(3)
where the expectation can be computed exactly in closed form:

Active Learning with Statistical Models

4

709

LOCALLY WEIGHTED REGRESSION

We consider here two forms of locally weighted regression (LWR): kernel regression
and the LOESS model [Cleveland et aI, 1988]. Kernel regression computes y as an
average of the Yi in the data set, weighted by a kernel centered at x. The LOESS
model performs a linear regression on points in the data set, weighted by a kernel
centered at x. The kernel shape is a design parameter: the original LOESS model
uses a ""tricubic"" kernel; in our experiments we use the more common Gaussian
hi(x) == hex - Xi) = exp( -k(x - xd 2),
where k is a smoothing constant. For brevity, we will drop the argument x for hi(x),
and define n = L:i hi. We can then write the estimated means and covariances as:

L:ihiXi
2
L:i hi(Xi- x )2
, Ux =
, Uxy = Lihi(Xi-X)(Yi-J.Ly)
n
n
n
_ L:i hiYi 2 _ Li hi(Yi - J.Ly)2 2 _ 2 u;y
J.Ly , Uy , Uyl x - Uy - - 2 .
n
n
~
We use them to express the conditional expectations and their estimated variances:
J.Lx =

kernel:
LOESS:

Y

,_
Y - J.Ly

= J.Ly,

+ ~( X q2

%

4.1

u
= -1!..
2

u?y

),...? __
J.Lx,
Y
V

(4)

n

u;lx (1 + (x n

J.Lx)2)

u;

(5)

ACTIVE LEARNING WITH LOCALLY WEIGHTED
REGRESSION

Again we want to select x to minimize (iT~) . With LWR, the model's estimated
distribution of y given x is explicit:

P(ylx) = N(y(x), u;lxCx))
The estimate of (iT~) is also explicit. Defining
kernel, the learner's expected new variance is

1.
kerne.

h as the weight assigned to x by the

(-2)
_ (iT~)
uy - --n+h

where the expectation can be computed exactly in closed form:

(6)

710

5

David Cohn, Zoubin Ghahramani, Michael 1. Jordon

EXPERIMENTAL RESULTS

Below we describe two sets of experiments demonstrating the predictive power of
the query selection criteria in this paper. In the first set, learners were trained on
data from a noisy sine wave. The criteria described in this paper were applied to
predict how a new training example selected at point x would decrease the learner's
variance. These predictions, along with the actual changes in variance when the
training points were queried and added, are plotted in Figure 1.

o.

- .- - . _. - predicted change
- - actual change

-0.5

o.

- ._.-. - .? predicted change
- - actual
.

9""8rl97"".
\
-"" i

~.

0.2

0.4

0.6

0.8

-0.2

,""

0.2

.-.- -.-.- predicted change
- - actual change

.
0.4

0.6

0.8

Figure 1: The upper portion of each plot indicates each learner's fit to noisy sinusoidal data. The lower portion of each plot indicates predicted and actual changes
in the learner's average estimated variance when x is queried and added to the
training set, for x E [0,1]. Changes are not plotted to scale with learners' fits.

In the second set of experiments, we a:pplied the techniques of this paper to learning
the kinematics of a two-joint planar arm (Figure 2; see Cohn [1994] for details).
Below, we illustrate the problem using the LOESS algorithm.
An example of the correlation between predicted and actual changes in variance
on this problem is plotted in Figure 2. Figure 3 demonstrates that this correlation may be exploited to guide sequential query selection. We compared a
LOESS learner which selected each new query so as to minimize expected variance

Active Learning with Statistical Models

711

with LOESS learners which selected queries according to various heuristics. The
variance-minimizing learner significantly outperforms the heuristics in terms of both
variance and MSE.
0 .025r--..---...,......-~---.----...---,---."",

o
o

0

0.02
o

~

c::

0.015
o

.~

~

0.01

til

~

""iii

0.005

::I

~

0
-0.005
o

-?$.01 -0.005

0
0.005 0,01 0.015
predicted delta variance

0.02

0.025

Figure 2: (left) The arm kinematics problem. (right) Predicted vs. actual changes
in model variance for LOESS on the arm kinematics problem. 100 candidate points
are shown for a model trained with 50 initial random examples. Note that most
of the potential queries produce very little improvement , and that the algorithm
successfully identifies those few that will help most.

0.2
0.1
3
VarianceO.04

MSE

0.02

0.01

0.3

0.004

0.1
50 100 150 200 250 300 350 400 450 500
training examples

50 100 150200 250 300 350 400 450 500
training examples

Figure 3: Variance and MSE for a LOESS learner selecting queries according to
the variance-minimizing criterion discussed in this paper and according to several
heuristics . ""Sensitivity"" queries where output is most sensitive to new data, ""Bias""
queries according to a bias-minimizing criterion, ?Support"" queries where the model
has the least data support. The variance of ""Random"" and ""Sensitivity"" are off the
scale. Curves are medians over 15 runs with non-Gaussian noise.

712

6

David Cohn. Zouhin Ghahramani. Michael 1. Jordon

SUMMARY

Mixtures of Gaussians and locally weighted regression are two statistical models
that offer elegant representations and efficient learning algorithms. In this paper
we have shown that they also offer the opportunity to perform active learning in an
efficient and statistically correct manner. The criteria derived here can be computed
cheaply and, for problems tested, demonstrate good predictive power.
Acknowledgements

This work was funded by NSF grant CDA-9309300, the McDonnell-Pew Foundation,
ATR Human Information Processing Laboratories and Siemens Corporate Research.
We thank Stefan Schaal for helpful discussions about locally weighted regression .
References
W. Cleveland, S. Devlin, and E. Grosse. (1988) Regression by local fitting. Journal of
Econometrics 37:87-114.
D. Cohn, 1. Atlas and R. Ladner. (1990) Training Connectionist Networks with Queries
and Selective Sampling. In D. Touretzky, ed., Advances in Neural Information Processing
Systems 2, Morgan Kaufmann.
D. Cohn. (1994) Neural network exploration using optimal experiment design. In J . Cowan
et al., eds., Advances in Neural Information Processing Systems 6. Morgan Kaufmann.
A. Dempster, N. Laird and D. Rubin. (1977) Maximum likelihood from incomplete data
via the EM algorithm. J. Royal Statistical Society Series B, 39:1-38.
V. Fedorov. (1972) Theory of Optimal Experiments. Academic Press, New York.
Z. Ghahramani and M. Jordan. (1994) Supervised learning from incomplete data via an
EM approach. In J. Cowan et al., eds., Advances in Neural Information Processing Systems
6. Morgan Kaufmann.
A. Linden and F. Weber. (1993) Implementing inner drive by competence reflection. In
H. Roitblat et al., eds., Proc. 2nd Int. Conf. on Simulation of Adaptive Behavior, MIT
Press, Cambridge.
D. MacKay. (1992) Information-based objective functions for active data selection, Neural
Computation 4( 4): 590-604.
S. Nowlan. (1991) Soft Competitive Adaptation: Neural Network Learning Algorithms
based on Fitting Statistical Mixtures. CMU-CS-91-126, School of Computer Science,
Carnegie Mellon University, Pittsburgh, PA.
Paass, G., and Kindermann, J . (1995) . Bayesian Query Construction for Neural Network
Models. In this volume.
M. Plutowski and H. White (1993). Selecting concise training sets from clean data. IEEE
Transactions on Neural Networks, 4, 305-318.
S. Schaal and C. Atkeson. (1994) Robot Juggling: An Implementation of Memory-based
Learning. Control Systems Magazine, 14(1):57-71.
J. Schmidhuber and J . Storck. (1993) Reinforcement driven information acquisition in
nondeterministic environments. Tech. Report, Fakultiit fiir Informatik, Technische Universitiit Munchen.
D. Specht. (1991) A general regression neural network. IEEE Trans. Neural Networks,
2(6):568-576.
S. Thrun and K. Moller. (1992) Active exploration in dynamic environments. In J. Moody
et aI., editors, Advances in Neural Information Processing Systems 4. Morgan Kaufmann.

"
1012,1994,A Rapid Graph-based Method for Arbitrary Transformation-Invariant Pattern Classification,,1012-a-rapid-graph-based-method-for-arbitrary-transformation-invariant-pattern-classification.pdf,Abstract Missing,"A Rapid Graph-based Method for
Arbitrary Transformation-Invariant
Pattern Classification
Alessandro Sperduti
Dipartimento di Informatica
Universita di Pisa
Corso Italia 40
56125 Pisa, ITALY

David G. Stork
Machine Learning and Perception Group
Ricoh California Research Center
2882 Sand Hill Road # 115
Menlo Park, CA USA 94025-7022

perso~di.unipi.it

stork~crc.ricoh.com

Abstract
We present a graph-based method for rapid, accurate search
through prototypes for transformation-invariant pattern classification. Our method has in theory the same recognition accuracy as
other recent methods based on ''tangent distance"" [Simard et al.,
1994], since it uses the same categorization rule. Nevertheless ours
is significantly faster during classification because far fewer tangent distances need be computed. Crucial to the success of our
system are 1) a novel graph architecture in which transformation
constraints and geometric relationships among prototypes are encoded during learning, and 2) an improved graph search criterion,
used during classification. These architectural insights are applicable to a wide range of problem domains. Here we demonstrate that
on a handwriting recognition task, a basic implementation of our
system requires less than half the computation of the Euclidean
sorting method.

1

INTRODUCTION

In recent years, the crucial issue of incorporating invariances into networks for pattern recognition has received increased attention, most especially due to the work of

666

Alessandro Sperduti, David G. Stork

Simard and his colleagues. To a regular hierachical backpropagation network Simard
et al. [1992] added a Jacobian network, which insured that directional derivatives
were also learned. Such derivatives represented directions in feature space corresponding to the invariances of interest, such as rotation, translation, scaling and
even line thinning. On small training sets for a function approximation problem,
this hybrid network showed performance superior to that of a highly tuned backpropagation network taken alone; however there was negligible improvement on
large sets. In order to find a simpler method applicable to real-world problems,
Simard, Le Cun & Denker [1993] later used a variation of the nearest neighbor
algorithm, one incorporating ""tangent distance"" (T-distance or D T ) as the classification metric - the smallest Euclidean distance between patterns after the optimal
transformation. In this way, state-of-the-art accuracy was achieved on an isolated
handwritten character task, though at quite high computational complexity, owing
to the inefficient search and large number of Euclidean and tangent distances that
had to be calculated.
Whereas Simard, Hastie & Saeckinger [1994] have recently sought to reduce this
complexity by means of pre-clustering stored prototypes, we here take a different
approach, one in which a (graph) data structure formed during learning contains
information about transformations and geometrical relations among prototypes.
Nevertheless, it should be noted that our method can be applied to a reduced
(clustered) training set such as they formed, yielding yet faster recognition. Simard
[1994] recently introduced a hierarchical structure of successively lower resolution
patterns, which speeds search only if a minority of patterns are classified more
accurately by using the tangent metric than by other metrics. In contrast, our
method shows significant improvement even if the majority or all of the patterns
are most accurately classified using the tangent distance.
Other methods seeking fast invariant classification include Wilensky and
Manukian's scheme [1994]. While quite rapid during recall, it is more properly
considered distortion (rather than coherent transformation) invariant. Moreover,
some transformations such as line thinning cannot be naturally incorporated into
their scheme. Finally, it appears as if their scheme scales poorly (compared to
tangent metric methods) as the number of invariances is increased.
It seems somewhat futile to try to improve significantly upon the recognition accuracy of the tangent metric approach - for databases such as NIST isolated
handwritten characters, Simard et al. [1993] reported accuracies matching that
of humans! Nevertheless, there remains much that can be done to increase the
computational efficiency during recall. This is the problem we address.

2

TRANSFORMATION INVARIANCE

In broad overview, during learning our method constructs a labelled graph data
structure in which each node represents a stored prototype (labelled by its category)
as given by a training set, linked by arcs representing the T-distance between them.
Search through this graph (for classification) takes advantage of the graph structure
and an improved search criterion. To understand the underlying computations, we
must first consider tangent space.

Graph-Based Method for Arbitrary Transformation-Invariant Pattern Classification

667

Figure 1: Geometry of tangent space. Here, a three-dimensional feature space
contains the ""current"" prototype, Pc, and the subspace consisting of all patterns
obtainable by performing continuous transformations of it (shaded). Two candidate
prototypes and a test pattern, T, as well as their projections onto the T-space of
Pc are shown. The insert (above) shows the progression of search through the
corresponding portion of the recognition graph. The goal is to rapidly find the
prototype closest to T (in the T-distance sense), and our algorithm (guided by the
minimum angle OJ in the tangent space) finds that P 2 is so closer to T than are
either PI or Pc (see text).

Figure 1 illustrates geometry of tangent space and the relationships among the fundamental entities in our trained system. A labelled (""current"") trained pattern is
represented by Pc, and the (shaded) surface corresponds to patterns arising under
continuous transformations of Pc. Such transformations might include rotation,
translation, scaling, line thinning, etc. Following Simard et al. [1993], we approximate this surface in the vicinity of Pc by a subspace - the tangent space or T -space
of Pc - which is spanned by ""tangent"" vectors, whose directions are determined by
infinitessimally transforming the prototype Pc. The figure shows an ortho-normal
basis {TVa, TV b}, which helps to speed search during classification, as we shall see.
A test pattern T and two other (candidate) prototypes as well as their projections
onto the T-space of Pc are shown.

668

3

Alessandro Sperduti, David G. Stork

THE ALGORITHMS

Our overall approach includes constructing a graph (during learning), and searching
it (for classification). The graph is constructed by the following algorithm:

Graph construction
Initialize N = # patterns; k = # nearest neighbors; t = # invariant transformations
Begin Loop For each prototype Pi (i = 1 ~ N)
? Compute a t-dimensional orthonormal basis for the T -space of Pi
? Compute (""one-sided"") T-distance of each of the N - 1 prototypes
P j (j i- i) using Pi'S T-space
? Represent Pj.l (the projection of P j onto the T-space of Pi) in the
tangent orthonormal frame of Pi
? Connect Pi to each of its k T-nearest neighbors, storing their associated normalized projections Ph
End Loop
During classification, our algorithm permits rapid search through prototypes. Thus
in Figure 1, starting at Pc we seek to find another prototype (here, P2) that is
closer to the test point T . After P2 is so chosen, it becomes the current pattern,
and the search is extended using its T-space. Graph search ends when the closest
prototype to T is found (Le., closest in a T-distance sense).
We let D~ denote the current minimum tangent distance. Our search algorithm is:

Graph search
Input Test
Initialize
?
?
?
Do

pattern T
Choose initial candidate prototype, Po
SetPc~Po

Set D~ ~ DT(P c , T), i.e., the T-distance ofT from Pc
T.L?P~

? For each prototype P j connected to Pc compute cos(Oj) = IT.Ll.L
? Sort these prototypes by increasing values of OJ and put them into a
candidate list
? Pick P j from the top of the candidate list
? In T-space of Pj, compute DT(P j , T)
If DT(P j , T) < D~ then Pc ~ P j and D~ ~ DT(P j , T)
otherwise mark P j as a ""failure"" (F), and pick next prototype from
the candidate list
Until Candidate list empty

Return D~ or the category label of the optimum prototype found

Graph-Based Method for Arbitrary Transformation-Invariant Pattern Classification

Dr

4.91

3.70

3.61

3.03

669

2.94

Figure 2: The search through the ""2"" category graph for the T-nearest stored
prototype to the test pattern is shown (N = 720 and k = 15 nearest neighbors).
The number of T-distance calculations is equal to the number of nodes visited plus
the number offailures (marked F); Le., in the case shown 5 + 26 = 31. The backward
search step attempt is thwarted because the middle node has already been visited
(marked M). Notice in the prototypes how the search is first a downward shift, then
a counter-clockwise rotation - a mere four steps through the graph.
Figure 2 illustrates search through a network of ""2"" prototypes. Note how the Tdistance of the test pattern decreases, and that with only four steps through the
graph the optimal prototype is found.
There are several ways in which our search technique can be incorporated into a
classifier. One is to store all prototypes, regardless of class, in a single large graph
and perform the search; the test pattern is classified by the label of the optimal
prototype found. Another, is to employ separate graphs, one for each category, and
search through them (possibly in parallel); the test is classified by the minimum
T-distance prototype found. The choice of method depends upon the hardware
limitations, performance speed requirements, etc. Figure 3 illustrates such a search
through a ""2"" category graph for the closest prototype to a test pattern ""5."" We
report below results using a single graph per category, however.

3.1

Computational complexity

If a graph contains N prototypes with k pointers (arcs) each, and if the patterns are
of dimension m, then the storage requirement is O(N((t + 1) . m 2 + kt)). The time
complexity of training depends upon details of ortho-normalization, sorting, etc.,
and is of little interest anyway. Construction is more than an order of magnitude
faster than neural network training on similar problems; for instance construction
of a graph for N = 720 prototypes and k = 100 nearest neighbors takes less than

Alessandro Sperduti, David G. Stork

670

[ZJ[ZJ[2J[2]
Dr

5.10

5.09

5.01

4.93

4.90

Figure 3: The search through a ""2"" category graph given a ""5"" test pattern. Note
how the search first tries to find a prototype that matches the upper arc of the
""5,"" and then one possessing skew or rotation. For this test pattern, the minimum
T-distance found for the ""5"" category (3.62) is smaller than the one found for the
""2"" category shown here (4.22), and indeed for any other category. Thus the test
pattern is correctly classified as a ""5.""

20 minutes on a Sparc 10.
The crucial quantity of interest is the time complexity for search. This is, of course,
problem related, and depends upon the number of categories, transformation and
prototypes and their statistical properties (see next Section). Worst case analyses
(e.g., it is theoretically conceivable that nearly all prototypes must be visited) are
irrelevant to practice.
We used a slightly non-obvious search criterion at each step, the function cos(Oj),
as shown in Figure 1. Not only could this criterion be calculated very efficiently
in our orthonormal basis (by using simple inner products), but it actually led to
a slightly more accurate search than Euclidean distance in the T-space - perhaps
the most natural choice of criterion. The angle OJ seems to guide the ""flow"" of the
search along transformation directions toward the test point.

4

Simulations and results

We explored the search capabilities of our system on the binary handwritten digit
database of Guyon, et al. [1991J. We needed to scale all patterns by a linear factor
(0.833) to insure that rotated versions did not go outside the 16 x 16 pixel grid. As
required in all T-space methods, the patterns must be continuous valued (Le., here
grayscale); this was achieved by convolution with a spatially symmetric Gaussian
having a = .55 pixels. We had 720 training examples in each of ten digit categories;
the test set consisted of 1320 test patterns formed by transforming independent
prototypes in all meaningful combinations of the t = 6 transformations (four spatial
directions and two rotation senses).
We compared the Euclidean sorting method of Simard et al. [1993J to our graph

Graph-Based Method for Arbitrary Transformation-Invariant Pattern Classification

1.00

671

______-----:::::::::::::==---10. 6
?

0.4 u

.c

~
'""

u

0.2

...'

',-.
-

error

---~.

o
50

100

150

200

""

.. - ................ --

250

300

350

~

e

~

0
400

Computational complexity
(equivalent number of T-distance calculations)

Figure 4: Comparison of graph-based (heavy lines) and standard Euclidean sorting
searches (thin lines). Search accuracy is the percentage of optimal prototypes found
on the full test set of 1320 patterns in a single category (solid lines). The average
search error is the per pattern difference between the global optimum T -distance and
the one actually found, averaged over the non-optimal prototypes found through the
search (dashed lines). Note especially that for the same computational complexity,
our method has the same average error, but that this average is taken over a much
smaller number of (non-optimal) prototypes. For a given criterion search accuracy,
our method requires significantly less computation. For instance, if 90% of the
prototypes must be found for a requisite categorization accuracy (a typical value
for asymptotically high recognition accuracy), our graph-based method requires less
than half the computation of the Euclidean sorting method.

based method using the same data and transformations, over the full range of
relevant computational complexities. Figure 4 summarizes our results. For our
method, the computational complexity is adjusted by the number of neighbors
inspected, k. For their Euclidean sorting method, it is adjusted by the percentage
of Euclidean nearest neighbors that were then inspected for T -distance. We were
quite careful to employ as many computational tricks and shortcuts on both methods
we could think of. Our results reflect fairly on the full computational complexity,
which was dominated by tangent and Euclidean distance calculations.
We note parenthetically that many of the recognition errors for both methods could
be explained by the fact that we did not include the transformation of line thinning
(solely because we lacked the preprocessing capabilities); the overall accuracy of
both methods will increase when this invariance is also included.

5

CONCLUSIONS AND FUTURE WORK

We have demonstrated a graph-based method using tangent distance that permits search through prototypes significantly faster than the most popular current
approach. Although not shown above, ours is also superior to other tree-based

672

Alessandro Sperduli. David G. Stork

methods, such as k-d-trees, which are less accurate. Since our primary concern was
reducing the computational complexity of search (while matching Simard et al.'s
accuracy), we have not optimized over preprocessing steps, such as the Gaussian
kernel width or transformation set. We note again that our method can be applied
to reduced training sets, for instance ones pruned by the method of Simard, Hastie
& Saeckinger [1994]. Simard's [1994] recent method - in which low-resolution
versions of training patterns are organized into a hierarchical data structure so
as to reduce the number of multiply-accumulates required during search - is in
some sense ""orthogonal"" to ours. Our graph-based method will work with his lowresolution images too, and thus these two methods can be unified into a hybrid
system.
Perhaps most importantly, our work suggests a number of research avenues. We
used just a single (""central"") prototype Po to start search; presumably having
several candidate starting points would be faster. Our general method may admit
gradient descent learning of parameters of the search criterion. For instance, we can
imagine scaling the different tangent basis vectors according to their relevance in
guiding correct searches as determined using a validation set. Finally, our approach
may admit elegant parallel implementations for real-world applications.
Acknowledgements

This work was begun during a visit by Dr. Sperduti to Ricoh CRC. We thank I.
Guyon for the use of her database of handwritten digits and Dr. K. V. Prasad for
assistance in image processing.
References
1. Guyon, P. Albrecht, Y. Le Cun, J. Denker & W. Hubbard. (1991) ""Comparing

different neural network architectures for classifying handwritten digits,"" Proc. of
the Inter. Joint Conference on Neural Networks, vol. II, pp. 127-132, IEEE Press.
P. Simard. (1994) ""Efficient computation of complex distance metrics using hierarchical filtering,"" in J. D. Cowan, G. Tesauro and J. Alspector (eds.) Advances in
Neural Information Processing Systems-6 Morgan Kaufmann pp. 168-175.
P. Simard, B. Victorrio, Y. Le Cun & J. Denker. (1992) ""Tangent Prop - A formalism for specifying selected invariances in an adaptive network,"" in J. E. Moody, S.
J . Hanson and R. P. Lippmann (eds.) Advances in Neural Information Processing
Systems-4 Morgan Kaufmann pp. 895-903.
P. Y. Simard, Y. Le Cun & J. Denker. (1993) ""Efficient Pattern Recognition Using
a New Transformation Distance,"" in S. J. Hanson, J. D. Cowan and C. L. Giles
(eds.) Advances in Neural Information Processing Systems-5 Morgan Kaufmann
pp.50-58.
P. Y. Simard, T. Hastie & E. Saeckinger. (1994) ""Learning Prototype Models for
Tangent Distance,"" Neural Networks for Computing Snowbird, UT (April, 1994).
G. D. Wilensky & N. Manukian. (1994) ""Nearest Neighbor Networks: New Neural
Architectures for Distortion-Insensitive Image Recognition,"" Neural Networks for
Computing Snowbird, UT (April, 1994).

"
1013,1994,Ocular Dominance and Patterned Lateral Connections in a Self-Organizing Model of the Primary Visual Cortex,,1013-ocular-dominance-and-patterned-lateral-connections-in-a-self-organizing-model-of-the-primary-visual-cortex.pdf,Abstract Missing,"Ocular Dominance and Patterned Lateral
Connections in a Self-Organizing Model of the
Primary Visual Cortex
Joseph Sirosh and Risto Miikkulainen

Department of Computer Sciences
University of Texas at Austin, Austin, 'IX 78712
email:

sirosh.risto~cs.utexas.edu

Abstract
A neural network model for the self-organization of ocular dominance and
lateral connections from binocular input is presented. The self-organizing
process results in a network where (1) afferent weights of each neuron organize into smooth hill-shaped receptive fields primarily on one of the retinas, (2) neurons with common eye preference form connected, intertwined
patches, and (3) lateral connections primarily link regions of the same eye
preference. Similar self-organization of cortical structures has been observed experimentally in strabismic kittens. The model shows how patterned lateral connections in the cortex may develop based on correlated
activity and explains why lateral connection patterns follow receptive field
properties such as ocular dominance.

1 Introduction
Lateral connections in the primary visual cortex have a patterned structure that closely
matches the response properties of cortical cells (Gilbert and Wiesel 1989; Malach et al.1993).
For example, in the normal visual cortex, long-range lateral connections link areas with similar orientation preference (Gilbert and Wiesel 1989). Like cortical response properties, the
connectivity pattern is highly plastic in early development and can be altered by experience
(Katz and Callaway 1992). In a cat that is brought up squint-eyed from birth, the lateral connections link areas with the same ocular dominance instead of orientation (Lowel and Singer
1992). Such patterned lateral connections develop at the same time as the orientation selectivity and ocular dominance itself (Burkhalter et al.1993; Katz and Callaway 1992). Together,

110

Joseph Sirosh, Risto Miikkulainen

these observations suggest that the same experience-dependent process drives the development of both cortical response properties and lateral connectivity.
Several computational models have been built to demonstrate how orientation preference,
ocular dominance, and retinotopy can emerge from simple self-organizing processes (e.g.
Goodhill1993; Miller 1994; Obermayer et al.1992; von der Malsburg 1973). These models
assume that the neuronal response properties are primarily determined by the afferent connections, and concentrate only on the self-organization of the afferent synapses to the cortex. Lateral interactions between neurons are abstracted into simple mathematical functions
(e.g. Gaussians) and assumed to be uniform throughout the network; lateral connectivity is not
explicitly taken into account. Such models do not explicitly replicate the activity dynamics
of the visual cortex, and therefore can make only limited predictions about cortical function.
We have previously shown how Kohonen's self-organizing feature maps (Kohonen 1982)
can be generalized to include self-organizing lateral connections and recurrent activity dynamics (the Laterally Interconnected Synergetically Self-Organizing Map (LISSOM); Sirosh
and Miikkulainen 1993, 1994a), and how the algorithm can model the development of ocular dominance columns and patterned lateral connectivity with abstractions of visual input.
LISSOM is a low-dimensional abstraction of cortical self-organizing processes and models a
small region of the cortex where all neurons receive the same input vector. This paper shows
how realistic, high-dimensional receptive fields develop as part of the self-organization, and
scales up the LISSOM approach to large areas of the cortex where different parts of the cortical network receive inputs from different parts of the receptor surface. The new model shows
how (1) afferent receptive fields and ocular dominance columns develop from simple retinal images, (2) input correlations affect the wavelength of the ocular dominance columns and
(3) lateral connections self-organize cooperatively and simultaneously with ocular dominance
properties. The model suggests new computational roles for lateral connections in the cortex,
and suggests that the visual cortex maybe maintained in a continuously adapting equilibrium
with the visual input by co adapting lateral and afferent connections.

2

The LISSOM Model of Receptive Fields and Ocular Dominance

The LISSOM network is a sheet of interconnected neurons (figure 1). Through afferent connections, each neuron receives input from two ""retinas"". In addition, each neuron has reciprocal excitatory and inhibitory lateral connections with other neurons. Lateral excitatory connections are short-range, connecting only close neighbors. Lateral inhibitory connections run
for long distances, and may even implement full connectivity between neurons in the network.
Neurons receive afferent connections from broad overlapping patches on the retina called
anatomical receptive fields, or RFs. The N x N network is projected on to each retina of
R x R receptors, and each neuron is connected to receptors in a square area of side s around
the projections. Thus, neurons receive afferents from corresponding regions of each retina.
Depending on the location of the projection, the number of afferents to a neuron from each
retina could vary from
x ~s (at the comers) to s x s (at the center).

ts

The external and lateral weights are organized through an unsupervised learning process. At
each training step, neurons start out with zero activity. The initial response TJij of neuron (i, j)

Ocular Dominance and Patterned Lateral Connections

Loft _ . .

111

fllgIIl Roll . .

Figure 1: The Receptive-Field LISSOM architecture. The afferent and lateral connectionsof a single
neuron in the liSSOM network are shown. All connection weights are positive.

is based on the scalar product
TJij

=

(T

(L

eabJJij ,ab

+

a,b

L

(1)

eCdJJij,Cd) ,

c,d

where eab and ecd are the activations of retinal receptors (a, b) and (c, d) within the receptive
fields of the neuron in each retina, JJij,ab and JJij,cd are the corresponding afferent weights,
and (T is a piecewise linear approximation of the familiar sigmoid activation function. The
response evolves over time through lateral interaction. At each time step, the neuron combines the above afferent activation I:: eJJ with lateral excitation and inhibition:
TJij(t)

=

(T

(L eJJ + L
""Ie

Eij,kITJkl(t -

1) - L
""Ii

k,1

Iij,klTJkl(t -

1)) ,

(2)

k,1

where Eij,kl is the excitatory lateral connection weight on the connection from neuron (k, l)
to neuron (i, j), Iij,kl is the inhibitory connection weight, and TJkl (t - 1) is the activity of
neuron (k, I) during the previous time step. The constants ""Ie and ""Ii determine the relative
strengths of excitatory and inhibitory lateral interactions. The activity pattern starts out diffuse and spread over a substantial part of the map, and converges iteratively into stable focused
patches of activity, or activity bubbles. After the-activity has settled, typically in a few iterations of equation 2, the connection weights of each neuron are modified. Both afferent and
lateral weights adapt according to the same mechanism: the Hebb rule, normalized so that the
sum of the weights is constant:
(

Wij,mn t

r ) _

+ vt

-

+

Wij,mn(t)
CtTJijXmn
'""""
( )
wmn [Wij ,mn t
CtTJijXmn

+

1'

(3)

where TJij stands for the activity of neuron (i, j) in the final activity bubble, Wij,mn is the afferent or lateral connection weight (JJ, E or I), Ct is the learning rate for each type of connection
(Ct a for afferent weights, Ct E for excitatory, and Ct I for inhibitory) and X mn is the presynaptic
activity for afferent, TJ for lateral).

(e

Joseph Sirosh, Risto Miikkulainen

112

""
(a) Random Initial Weights

(b) Monocular RF

(c) Binocular RF

Figure 2: Self-organization of the afferent input weights into receptive fields. The afferent weights
of a neuron at position (42,39) in a 60 x 60 network are shown before (a) and after self-organization
(b). This particular neuron becomes monocular with strong connections to the right eye, and weak connections to the left. A neuron at position (38, 23) becomes binocular with appoximately equal weights
to both eyes (c).
Both excitatory and inhibitory lateral connections follow the same Hebbian learning process and strengthen by correlated activity. The short-range excitation keeps the activity of
neighboring neurons correlated, and as self-organization progresses, excitation and inhibition strengthen in the vicinity of each neuron. At longer distances, very few neurons have
correlated activity and therefore most long-range connections become weak. Such weak connections are eliminated, and through weight normalization, inhibition concentrates in a closer
neighborhood of each neuron. As a result, activity bubbles become more focused and local,
weights change in smaller neighborhoods, and receptive fields become better tuned to local
areas of each retina.
The input to the model consists of gaussian spots of ""light"" on each retina:
t
_
((x
<""x,y - exp -

- xd 2 + (y - Yi)2)
u2

(4)

where ex,y is the activation of receptor (x, V), u 2 is a constant determining the width of the
spot, and (Xi,Yi): 0 ~ xi, Yi < R its center. At each input presentation, one spot is randomly
placed at (Xi ,Yi) in the left retina, and a second spot within a radius of p x RN of (Xi, yd
in the right retina. The parameter p E [0, 1] specifies the spatial correlations between spots
in the two retinas, and can be adjusted to simulate different degrees of correlations between
images in the two eyes.

3

Simulation results

To see how correlation between the input from the two eyes affects the columnar structures
that develop, several simulations were run with different values of p. The afferent weights of
all neurons were initially random (as shown in figure 2a), with the total strength to both eyes
being equal.
Figures 2b,c show the final afferent receptive fields of two typical neurons in a simulation
with p = 1. In this case, the inputs were uncorrelated, simulating perfect strabismus. In
the early stages of such simulation, some of the neurons randomly develop a preference for
one eye or the other. Nearby neurons will tend to share the same preference because lateral

Ocular Dominance and Patterned Lateral Connections

(a) Connections of a Monocular Neuron

113

(b) Connections of a Binocular Neuron

Figure 3: Ocular dominance and lateral connection patterns. The ocular dominance of a neuron is
measured as the difference in total afferent synaptic weight from each eye to the neuron. Each neuron
is labeled with a grey-scale value (black ~ white) that represents continuously changing eye preference from exclusive left through binocular to exclusive right. Small white dots indicate the lateral input
connections to the neuron marked with a big white dot. (a) The surviving lateral connections of a left
monocular neuron predominantly link areas of the same ocular dominance. (b) The lateral connections
of a binocular neuron come from both eye regions.

excitation keeps neural activity partially correlated over short distances. As self-organization
progresses, such preferences are amplified, and groups of neurons develop strong weights to
one eye. Figure 2b shows the afferent weights of a typical monocular neuron.
The extent of activity correlations on the network detennines the size of the monocular neuronal groups. Farther on the map, where the activations are anticorrelated due to lateral inhibition, neurons will develop eye preferences to the opposite eye. As a result, alternating
ocular dominance patches develop over the map, as shown in figure 3. 1 In areas between ocular dominance patches, neurons will develop approximately equal strengths to both eyes and
become binocular, like the one shown in figure 2e.
The width and number of ocular dominance columns in the network (and therefore, the wavelength of ocular dominance) depends on the input correlations (figure 4). When inputs in the
two eyes become more correlated (p < 1), the activations produced by the two inputs in the
network overlap closely and activity correlations become shorter range. By Hebbian adaptation, lateral inhibition concentrates in the neighborhood of each neuron, and the distance at
which activations becomes anticorrelated decreases. Therefore, smaller monocular patches
develop, and the ocular dominance wavelength decreases. Similar dependence was very recently observed in the cat primary visual cortex (LoweI1994). The LISSOM model demonstrates that the adapting lateral interactions and recurrent activity dynamics regulate the wavelength, and suggests how these processes help the cortex develop feature detectors at a scale
1 For a thorough treatment of the mathematical principles underlying the development of ocular dominance columns, see (GoodhillI993; Miller et al.1989; von der Malsburg and Singer 1988).

114

Joseph Sirosh, Risto Miikkulainen

-0
-0

(a) Strabismic case

(b ) Normal case

Figure 4: Ocular dominance wavelength in strabismic and normal models. In the strabismic case,
there are no between-eye correlations (p = 1), and broad ocular dominance columns are produced (a) .
With normal, partial between-eye correlations (p = 0.45 in this example), narrower stripes are formed
(b). As a result, there are more ocular dominance columns in the normal case and the ocular dominance
wavelength is smaller.

that matches the input correlations.
As eye preferences develop, left or right eye input tends to cause activity only in the left or
right ocular dominance patches. Activity patterns in areas of the network with the same ocular dominance tend to be highly correlated because they are caused by the same input spot.
Therefore, the long-range lateral connections between similar eye preference areas become
stronger, and those between opposite areas weaker. After the weak lateral connections are
eliminated, the initially wide-ranging connections are pruned, and eventually only connect
areas of similar ocular dominance as shown in figure 3. Binocular neurons between ocular
dominance patches will see some correlated activity in both the neigbboring areas, and maintain connections to both ocular dominance columns (figure 3b).
The lateral connection patterns shown above closely match observations in the primary visual cortex. Lowel and Singer (1992) observed that when between-eye correlations are abolished in kittens by surgically induced strabismus, long-range lateral connections primarily
link areas of the same ocular dominance. However, binocular neurons, located between ocular dominance columns, retained connections to both eye regions. The receptive field model
confinns that such patterned lateral connections develop based on correlated neuronal activity,
and demonstrates that they can self-organize simultaneously with ocular dominance columns.
The model also predicts that the long-range connections have an inhibitory function.

4 Discussion
In LISSOM, evolving lateral interactions and dynamic activity patterns are explicitly modeled. Therefore, LISSOM has several novel properties that set it apart from other selforganizing models of the cortex.
Previous models (e.g. Goodhill1993; Milleret al.1989; Obermayer et al.1992; von der Malsburg 1973) have concentrated only on forming ordered topographic maps where clusters of
adjacent neurons assume similar response properties such as ocular dominance or orientation
preference. The lateral connections in LISSOM, in addition, adapt to encode correlations be-

Ocular Dominance and Patterned Lateral Connections

115

tween the responses. 2 This property can be potentially very useful in models of cortical function. While afferent connections learn to detect the significant features in the input space (such
as ocularity or orientation), the lateral connections can learn correlations between these features (such as Gestalt principles), and thereby form a basis for feature grouping.
As an illustration, consider a single spot of light presented to the left eye. The spot causes disjoint activity patterns in the left-eye-dominant patches. How can these multiple activity patterns be recognized as representing the same spatially coherent entity? As proposed by Singer
et al. (1990), the long-range lateral connections between similar ocular dominance columns
could synchronize cortical activity, and form a coherently firing assembly of neurons. The
spatial coherence of the spot will then be represented by temporal coherence of neural activity. LISSOM can be potentially extended to model such feature binding.
Even after the network has self-organized, the lateral and afferent connections remain plastic
and in a continuously-adapting dynamic equilibrium with the input. Therefore, the receptive
field properties of neurons can dynamically readapt when the activity correlations in the network are forced to change. For example, when a small area of the cortex is set inactive (or
lesioned), the sharply-tuned afferent weight profiles of the neurons surrounding that region
expand in size, and neurons begin to respond to the stimuli that previously activated only the
lesioned area (Sirosh and Miikkulainen 1994b, 1994c). This expansion of receptive fields is
reversible, and when the lesion is repaired, neurons return to their original tuning. Similar
changes occur in response to retinal lesions as well. Such dynamic expansions of receptive
fields have been observed in the visual cortex (Pettet and Gilbert 1992). The LISSOM model
demonstrates that such plasticity is a consequence of the same self-organizing mechanisms
that drive the development of cortical maps.

5

Conclusion

The LISSOM model shows how a single local and unsupervised self-organizing process can
be responsible for the development of both afferent and lateral connection structures in the primary visual cortex. It suggests that this same developmental mechanism also encodes higherorder visual information such as feature correlations into the lateral connections. The model
forms a framework for future computational study of cortical reorganization and plasticity, as
well as dynamic perceptual processes such as feature grouping and binding.
Acknowledgments

This research was supported in part by National Science Foundation under grant #IRI9309273. Computer time for the simulations was provided by the Pittsburgh Supercomputing
Center under grants IRI930005P and TRA940029P.

References
Burkhalter, A., Bernardo, K. L., and Charles, V. (1993). Development of local circuits in
human visual cortex. Journalo/Neuroscience, 13:1916-1931.
Gilbert, C. D., and Wiesel, T. N. (1989). Columnar specificity of intrinsic horizontal and
corticocortical connections in cat visual cortex. Journal 0/ Neuroscience, 9:2432-2442.
2Tbe idea was conceived by von der Malsburg and Singer (1988), but not modeled.

116

Joseph Sirosh, Risto Miikkulainen

Goodhill, G. (1993). Topography and ocular dominance: a model exploring positive correlations. Biological Cybernetics, 69:109-118.
Katz, L. C., and Callaway, E. M. (1992). Development of local circuits in mammalian visual
cortex. Annual Review o/Neuroscience, 15:31-56.
Kohonen, T. (1982). Self-organized formation of topologically correct feature maps. Biolog-

ical Cybernetics, 43:59-69.
Lowel, S. (1994). Ocular dominance column development: Strabismus changes the spacing
of adjacent columns in cat visual cortex. Journal 0/ Neuroscience, 14(12):7451-7468.
Lowel, S., and Singer, W. (1992). Selection of intrinsic horizontal connections in the visual
cortex by correlated neuronal activity. Science, 255:209-212.
Malach, R., Amir, Y., Harel, M., and Grinvald, A (1993). Relationship between intrinsic
connections and functional architecture revealed by optical imaging and in vivo targeted
biocytin injections in the primate striate cortex. Proceedings o/the National Academy

o/Sciences, USA,90:10469-10473.
Miller, K. D. (1994). A model for the development of simple cell receptive fields and the
ordered arrangement of orientation columns through activity-dependent competition between on- and off-center inputs. Journalo/Neuroscience, 14:409-441.
Miller, K. D., Keller, 1. B., and Stryker, M. P. (1989). Ocular dominance column development:
Analysis and simulation. Science, 245:605-615.
Obermayer, K., Blasdel, G. G., and Schulten, K. J. (1992). Statistical-mechanical analysis of
self-organization and pattern formation during the development of visual maps. Physical

Review A, 45:7568-7589.
Pettet, M. W., and Gilbert, C. D. (1992). Dynamic changes in receptive-field size in cat primary visual cortex. Proceedings o/the NationalAcademy 0/ Sciences, USA,89:83668370.
Singer, W., Gray, C., Engel, A, Konig, P., Artola, A, and Bracher, S. (1990). Formation of
cortical cell assemblies. In Cold Spring Harbor Symposia on Quantitative Biology, Vol.
LV, 939-952. Cold Spring Harbor, NY: Cold Spring Harbor Laboratory.
Sirosh, J., and Miikkulainen, R. (1993). How lateral interaction develops in a self-organizing
feature map. In Proceedings o/the IEEE International Conference on Neural Networks
(San Francisco, CA), 1360--1365. Piscataway, NJ: IEEE.
Sirosh, J., and Miikkulainen, R. (1994a). Cooperative self-organization of afferent and lateral
connections in cortical maps. Biological Cybernetics, 71(1):66--78.
Sirosh, 1., and Miikkulainen, R. (1994b). Modeling cortical plasticity based on adapting lateral interaction. In The Neurobiologyo/Computation: Proceedings o/the Annual ComputationalNeuroscience Meeting. Dordrecht; Boston: Kluwer. In Press.
Sirosh, J., and Miikkulainen, R. (1994c). A neural network model oftopographic reorganization following cortical lesions. In Proceedings o/the World Congress on Computational
MediCine, Public Health and BioteChnology (Austin, TX). World Scientific. In Press.
von der Malsburg, C. (1973). Self-organization of orientation-sensitive cells in the striate
cortex. Kybernetik, 15:85-100.
von der Malsburg, C., and Singer, W. (1988). Principles of cortical network organization. In
Rakic, P., and Singer, W., editors, Neurobiology 0/Neocortex, 69-99. New York: Wiley.

"
1014,1994,Associative Decorrelation Dynamics: A Theory of Self-Organization and Optimization in Feedback Networks,,1014-associative-decorrelation-dynamics-a-theory-of-self-organization-and-optimization-in-feedback-networks.pdf,Abstract Missing,"Associative Decorrelation Dynamics:
A Theory of Self-Organization and
Optimization in Feedback Networks

Dawei W. Dong*
Lawrence Berkeley Laboratory
University of California
Berkeley, CA 94720

Abstract
This paper outlines a dynamic theory of development and adaptation in neural networks with feedback connections. Given input ensemble, the connections change in strength according to an
associative learning rule and approach a stable state where the
neuronal outputs are decorrelated . We apply this theory to primary visual cortex and examine the implications of the dynamical
decorrelation of the activities of orientation selective cells by the
intracortical connections. The theory gives a unified and quantitative explanation of the psychophysical experiments on orientation
contrast and orientation adaptation. Using only one parameter , we
achieve good agreements between the theoretical predictions and
the experimental data.

1

Introduction

The mammalian visual system is very effective in detecting the orientations of lines
and most neurons in primary visual cortex selectively respond to oriented lines and
form orientation columns [1) . Why is the visual system organized as such? We
*Present address: Rockefeller University, B272, 1230 York Avenue, NY, NY 10021-6399.

926

Dawei W Dong

believe that the visual system is self-organized, in both long term development and
short term adaptation, to ensure the optimal information processing.
Linsker applied Hebbian learning to model the development of orientation selectivity and later proposed a principle of maximum information preservation in early
visual pathways [2]. The focus of his work has been on the feedforward connections
and in his model the feedback connections are isotropic and unchanged during the
development of orientation columns; but the actual circuitry of visual cortex involves extensive, columnar specified feedback connections which exist even before
functional columns appear in cat striate cortex [3].
Our earlier research emphasized the important role of the feedback connections in
the development of the columnar structure in visual cortex. We developed a theoretical framework to help understand the dynamics of Hebbian learning in feedback networks and showed how the columnar structure originates from symmetry
breaking in the development of the feedback connections (intracortical, or lateral
connections within visual cortex) [4].
Figure 1 illustrates our theoretical predictions. The intracortical connections break
symmetry and develop strip-like patterns with a characteristic wave length which
is comparable to the developed intracortical inhibitory range and the LGN-cortex
afferent range (left). The feedforward (LGN-cortex) connections develop under the
influence of the symmetry breaking development of the intracortical connections.
The developed feedforward connections for each cell form a receptive field which
is orientation selective and nearby cells have similar orientation preference (right) .
Their orientations change in about the same period as the strip-like pattern of the
intracortical connections.

Figure 1: The results of the development of visual cortex with feedback connections. The
simulated cortex consists of 48 X 48 neurons, each of which connects to 5 X 5 other cortical
neurons (left) and receives inputs from 7 X 7 LGN neurons (right). In this figure, white
inclicates positive connections and black inclicates negative connections. One can see that
the change of receptive field's orientation (right) is highly correlated with the strip-like
pattern of intracortical connections (left).

Many aspects of our theoretical predictions agree qualitatively with neurobiological observations in primary visual cortex. Another way to test the idea of optimal

Associative Correlation Dynamics

927

information processing or any self-organization theory is through quantitative psychophysical studies. The idea is to look for changes in perception following changes
in input environments. The psychophysical experiments on orientation illusions
offer some opportunities to test our theory on orientation selectivity.
Orientation illusions are the effects that the perceived orientations of lines are affected by the neighboring (in time or space) oriented stimuli, which have been
observed in many psychophysical experiments and were attributed to the inhibitory
interactions between channels tuned to different orientations [5]. But there is no unified and quantitative explanation. Neurophysiological evidences support our earlier
computational model in which intracortical inhibition plays the role of gain-control
in orientation selectivity [6]. But in order for the gain-control mechanism to be
effective to signals of different statistics, the system has to develop and adapt in
different environments.
In this paper we examine the implication of the hypothesis that the intracortical
connections dynamically decorrelate the activities of orientation selective cells, i.e.,
the intracortical connections are actively adapted to the visual environment, such
that the output activities of orientation selective cells are decorrelated. The dynamics which ensures such decorrelation through associative learning is outlined in the
next section as the theoretical framework for the development and the adaptation
of intracortical connections. We only emphasize the feedback connections in the
following sections and assume that the feedforward connections developed orientation selectivities based on our earlier works. The quantitative comparisons of the
theory and the experiments are presented in section 3.

2

Associative Decorrelation Dynamics

There are two different kinds of variables in neural networks. One class of variables
represents the activity of the nerve cells, or neurons. The other class of variables
describes the synapses, or connections, between the nerve cells. A complete model
of an adaptive neural system requires two sets of dynamical equations, one for each
class of variables, to specify the evolution and behavior of the neural system.
The set of equations describing the change of the state of activity of the neurons is
dVi

adt
-I

= -ViI

+ ~T.
L..J .. v.. + 1I}}

I

(1)

j

in which a is a time constant, Tij is the strength of the synaptic connection from
neuron j to neuron i, and Ii is the additional feedforward input to the neuron besides
those described by the feedback connection matrix nj . A second set of equations
describes the way the synapses change with time due to neuronal activity. The
learning rule proposed here is
B dnj = (V,. - V.')!,
dt

in which B is a time constant and
in the following.

Vi'

I

I}

(2)

is the feedback learning signal as described

The feedback learning signal Vi' is generated by a Hopfield type associative memory
network: Vi' = Lj T/j Vi , in which T/j is the strength of the associative connection

928

Dawei W Dong

from neuron j to neuron i, which is the recent correlation between the neuronal
activities Vi and Vj determined by Hebbian learning with a decay term [4]

B

,dTfj

,

dt = -Iij + ViVj

(3)

in which B' is a time constant. The Vi' and T[j are only involved in learning and
do not directly affect the network outputs.
It is straight forward to show that when the time constants B
dynamics reduces to

dT

B dt

= (1- < VVT ?

> > B' > > a, the

< VIT >

(4)

where bold-faced quantities are matrices and vectors and <> denotes ensemble
average. It is not difficult to show that this equation has a Lyapunov or ""energy""
function
L = Tr(1- < VV T ?(1- < VVT
(5)
which is lower bounded and satisfies

>f

dL

<0

dt -

and

dL =0

dt

-+-

dTij =
dt

0 I'lor at,)
11""

(6)

Thus the dynamics is stable. When it is stable, the output activities are decorrelated ,
<VVT >= 1
(7)
The above equation shows that this dynamics always leads to a stable state where
the neuronal activities are decorrelated and their correlation matrix is orthonormal.
Yet the connections change in an associative fashion - equation (2) and (3) are
almost Hebbian . That is why we call it associative decorrelation dynamics. From information processing point of view, a network, self-organized to satisfy equation (7),
is optimized for Gaussian input ensembles and white output noises [7].

Linear First Order Analysis
In applying our theory of associative decorrelation dynamics to visual cortex to
compare with the psychophysical experiments on orientation illusions, the linear
first-order approximation is used, which is

T = TO + 6T,
V = Va +6V,

TO = 0, 6T ex - < I IT >
Va = I, 6V = TI

(8)

where it is assumed that the input correlations are small. It is interesting to notice
that the linear first-order approximation leads to anti-Hebbian feedback connections: Iij ex - < /i/j > which is guarantteed to be stable around T = 0 [8].

3

Quantitative Predictions of Orientation Illusions

The basic phenomena of orientation illusions are demonstrated in figure 2 (left).
On the top, is the effect of orientation contrast (also called tilt illusion): within the
two surrounding circles there are tilted lines; the orientation of a center rectangle

Associative Correlation Dynamics

929

appears rotated to the opposite side of its surrounding tilt. Both the two rectangles and the one without surround (at the left-center of this figure) are, in fact,
exactly same. On the bottom, is the effect of orientation adaptation (also called
tilt aftereffect): if one fixates at the small circle in one of the two big circles with
tilted lines for 20 seconds or so and then look at the rectangle without surround,
the orientation of the lines of the rectangle appears tilted to the opposite side.
These two effects of orientation illusions are both in the direction of repulsion: the
apparent orientation of a line is changed to increase its difference from the inducing
line. Careful experimental measurements also revealed that the angle with the
inducing line is
100 for maximum orientation adaptation effect [9] but 20 0 for
orientation contrast [10].
<""V

<""V

1

Ol..---~-~-""""';:""'''''''''---'

-90

-45

o

Stimulus orientation

45
(J

90

(degree)

Figure 2: The effects of orientation contrast (upper-left) and orientation adaptation (lowerleft) are attributed to feedback connections between cells tuned to different orientations
(upper-right, network; lower-right, tuning curve).

Orientation illusions are attributed to the feedback connections between orientation selective cells. This is illustrated in figure 2 (right). On the top is the network
of orientation selective cells with feedback connections. Only four cells are shown.
From the left, they receive orientation selective feedforward inputs optimal at -45 0 ,
00 ,45 0 , and 90 0 , respectively. The dotted lines represent the feedback connections
(only the connections from the second cell are drawn). On the bottom is the orientation tuning curve of the feedforward input for the second cell, optimally tuned to
stimulus of 00 (vertical), which is assumed to be Gaussian of width (T = 20 0 ? Because of the feedback connections, the output of the second cell will have different
tuning curves from its feedforward input, depending on the activities of other cells.
For primary visual cortex, we suppose that there are orientation selective neurons
tuned to all orientations. It is more convenient to use the continuous variable e
instead of the index i to represent neuron which is optimally tuned to the orientation
of angle e. The neuronal activity is represented by V(e) and the feedforward input
to each neuron is represented by I(e). The feedforward input itself is orientation

930

Dawei W. Dong

selective: given a visual stimulus of orientation

J(e) =

eo, the input is

e-(9-9 o )2/ q 2

(9)
This kind of the orientation tuning has been measured by experiments (for references, see [6]). Various experiments give a reasonable tuning width around 20?
?(7"" = 20? is used for all the predictions).
Predicted Orientation Adaptation
For the orientation adaptation to stimulus of angle eo, substituting equation (9)
into equation (8), it is not difficult to derive that the network response to stimulus
of angle 0 (vertical) is changed to

V(e)

= e_ 92 / q2 _

ae-(9-9 o )2/ q 2 e-9~/2q2

(10)

in which (7"" is the feedforward tuning width chosen to be 20? and a is the parameter
of the strength of decorrelation feedback.
The theoretical curve of perceived orientation ?(eo) is derived by assuming the
maximum likelihood of the the neural population, i.e., the perceived angle ? is the
angle at which Vee) is maximized. It is shown in figure 3 (right). The solid line is
the theoretical curve and the experimental data come from [9] (they did not give
the errors, the error bars are of our estimation,...., 0.2?). The parameter obtained
through X2 fit is the strength of decorrelation feedback: a = 0.42.
2.0 ;--'""T""""---,--...,.....-----.------,

-

~ 1.5

-

~ 3.0

~

~
II)

II)

CD 1.0

}

.""
~

.""

~

0.5

~

2.0

II)
> 1.0
'il

'il
Q.,

4.0

0.0 f - - - - - - - - - - - - - J

o

10
20
30
40
Surround angle 80 (degree)

50

i:!
II)

Q.,

0.0
0

10
20
30
40
50
Adaptation angle 80 (degree)

Figure 3: Quantitative comparison of the theoretical predictions with the experimental
data of orientation contrast (left) and orientation adaptation (right).

It is very interesting that we can derive a relationship which is independent of the
parameter of the strength of decorrelation feedback a,

(eo - ?m)(3e o - 2?m) = (7""2
(11)
in which eo is the adaptation angle at which the tilt aftereffect is most significant
and ?m is the perceived angle.
Predicted Orientation Contrast
For orientation contrast, there is no specific adaptation angle, i.e., the network has
developed in an environment of all possible angles. In this case, when the surround
is of angle eo, the network response to a stimulus of angle e1 is
Vee) = e-(9-9 1)2/ q 2 _ ae-(9-9 o )2/ 3q 2
(12)

Associative Correlation Dynamics

931

in which fr and a has the same meaning as for orientation adaptation. Again assuming the maximum likelihood, ?(eo), the stimulus angle e1 at which it is perceived
as angle 0, is derived and shown in figure 3 (left). The solid line is the theoretical
curve and the experimental data come from [10] and their estimated error is """" 0.20.
The parameter obtained through X 2 fit is the strength of decorrelation feedback:
a = 0.32.
We can derive the peak position eo, i.e., the surrounding angle
orientation contrast is most significant,

~e~ =
3

fr2

eo

at which the

(13)

For fr = 20 0 , one immediately gets eo = 24 0 ? This is in good agreement with
experiments, most people experience the maximum effect of orientation contrast
around this angle.
Our theory predicts that the peak position of the surround angle for orientation
contrast should be constant since the orientation tuning width fr is roughly the
same for different human observers and is not going to change much for different
experimental setups. But the peak value of the perceived angle is not constant since
the decorrelation feedback parameter a is not necessarily same, indeed, it could be
quite different for different human observers and different experimental setups.

4

Discussion

First, we want to emphasis that in all the comparisons, the same tuning width fr is
used and the strength of decorrelation feedback a is the only fit parameter. It does
not take much imagination to see that the quantitative agreements between the
theory and the experiments are good. Further more, we derived the relationships
for the maximum effects, which are independent of the parameter a and have been
partially confirmed by the experiments.
Recent neurophysiological experiments revealed that the surrounding lines did influence the orientation selectivity of cells in primary visual cortex of the cat [11].
Those single cell experiments land further support to our theory. But one should
be cautioned that the cells in our theory should be considered as the average over
a large population of cells in cortex.
The theory not only explains the first order effects which are dominant in angle
range of 00 to 50 0 , as shown here, but also accounts for the second order effects
which can be seen in 500 to 90 0 range, where the sign of the effects is reversed.
The theory also makes some predictions for which not much experiment has been
done yet, for example, the prediction about how orientation contrast depends on
the distance of surrounding stimuli from the test stimulus [7].
Finally, this is not merely a theory for the development and the adaptation of
orientation selective cells, it can account for effect such as human vision adaptation
to colors as well [7]. We can derive the same equation as Atick etal [12] which agrees
with the experiment on the appearance of color hue after adaptation. We believe
that future psychophysical experiments could give us more quantitative results to
further test our theory and help our understanding of neural systems in general.

932

Dawei W. Dong

Acknowledgements
This work was supported in part by the Director, Office of Energy Research, Division of Nuclear Physics of the Office of High Energy and Nuclear Physics of the
U.S. Department of Energy under Contract No. DE-AC03-76SF00098.

References
[1] Hubel DH, Wiesel TN, 1962 Receptive fields, binocular interactions, and functional
architecture in the cat's visual cortex J Physiol (London) 160, 106- 54. - 1963
Shape and arrangement of columns in cat's striate cortex J Physiol (London) 165,
559-68.
[2] Linsker R, 1986 From basic network principles to neural architecture ... Proc Natl
Acad Sci USA 83, 7508 8390 8779. - , 1989 An application of the principle of maximum information preservation to linear systems Advances in Neural Information
Processing Systems 1, Touretzky DS, ed, Morgan Kaufman, San Mateo, CA 186-94.
[3] Gilbert C, Wiesel T, 1989 Columnar Specificity of intrinsic horizontal and corticocortical connections in cat visual cortex J Neurosci 9(7), 2432-42. Luhmann HJ,
Martinez L, Singer W, 1986 Development of horizontal intrinsic connections in cat
striate cortex Exp Brain Res 63, 443-8.
[4] Dong DW, 1991 Dynamic properties of neural network with adapting synapses Proc
International Joint Conference on Neural Networks, Seattle, 2, 255- 260. - , 1991
Dynamic Properties of Neural Networks Ph D thesis, University Microfilms International, Ann Arbor, ML Dong DW, Hopfield JJ, 1992 Dynamic properties of neural
networks with adapting synapses Network: Computation in Neural Systems, 3(3),
267- 83.
[5] Gibson J J, Radner M, 1937 Adaptation, after-effect and contrast in the perception
of tilted lines J of Exp Psy 20, 453-67. Carpenter RHS, Blakemore C, 1973 Interactions between orientations in human vision Exp Brain Res 18, 287-303. Tolhurst
DJ, Thompson PG, 1975 Orientation illusions and after-effects: Inhibition between
channels Vis Res 15,967-72. Barlow HB, Foldiak P, 1989 Adaptation and decorrelation in the cortex The Computing Neuron, Durbin R, Miall C, Mitchison G, eds,
Addison- Wesley, New York, NY.
[6] Wehmeier U, Dong DW, Koch C, Van Essen DC, 1989 Modeling the mammalian
visual system Methods in Neuronal Modeling: From Synapses to Networks, Koch C,
Segev I, eds, MIT Press, Cambridge, MA 335-60.
[7] Dong DW, 1993 Associative Decorrelation Dynamics in Visual Cortex Lawrence
Berkeley Laboratory Technical Report LBL-34491.
[8] Dong DW, 1993 Anti-Hebbian dynamics and total recall of associative memory Proc
World Congress on Neural Networks, Portland, 2, 275-9.
[9] Campbell FW, Maffei L, 1971 The tilt after-effect: a fresh look Vis Res 11, 833-40.
[10] Westheimer G, 1990 Simultaneous orientation contrast for lines in the human fovea
Vis Res 30, 1913-21.

[11] Gilbert CD, Wiesel TN, 1990 The influence of contextual stimuli on the orientation
selectivity of cells in primary visual cortex of the cat Vis Res 30,1689-701.
[12] Atick JJ, Li Z, Redlich AN, 1993 What does post-adaptation color appearance reveal
about cortical color representation Vis Res 33, 123-9.

"
1015,1994,A Connectionist Technique for Accelerated Textual Input: Letting a Network Do the Typing,,1015-a-connectionist-technique-for-accelerated-textual-input-letting-a-network-do-the-typing.pdf,Abstract Missing,"A Connectionist Technique for Accelerated
Textual Input: Letting a Network Do the Typing

Dean A. Pomerleau
pomerlea@cs.cmu.edu
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213

Abstract
Each year people spend a huge amount of time typing. The text people type
typically contains a tremendous amount of redundancy due to predictable
word usage patterns and the text's structure. This paper describes a
neural network system call AutoTypist that monitors a person's typing and
predicts what will be entered next. AutoTypist displays the most likely
subsequent word to the typist, who can accept it with a single keystroke,
instead of typing it in its entirety. The multi-layer perceptron at the heart
of Auto'JYpist adapts its predictions of likely subsequent text to the user's
word usage pattern, and to the characteristics of the text currently being
typed. Increases in typing speed of 2-3% when typing English prose and
10-20% when typing C code have been demonstrated using the system,
suggesting a potential time savings of more than 20 hours per user per year.
In addition to increasing typing speed, AutoTypist reduces the number of
keystrokes a user must type by a similar amount (2-3% for English, 1020% for computer programs). This keystroke savings has the potential to
significantly reduce the frequency and severity of repeated stress injuries
caused by typing, which are the most common injury suffered in today's
office environment.

1 Introduction
People in general, and computer professionals in particular, spend a huge amount of time
typing. Most of this typing is done sitting in front of a computer display using a keyboard as
the primary input device. There are a number of efforts using artificial neural networks and
other techniques to improve the comfort and efficiency of human-computer communication
using alternative modalities. Speech recognition [Waibel et al., 1988], handwritten character
recognition [LeCun et al., 1989], and even gaze tracking [Baluja & Pomerleau, 1993] have

1040

Dean Pomerleau

the potential to facilitate this communication. But these technologies are still in their infancy,
and at this point cannot approach the speed and accuracy of even a moderately skilled typist
for textual input.
Is there some way to improve the efficiency of standard keyboard-based human-computer
communication? The answer is yes, there are several ways to make typing more efficient.
The first, called the Dvorak keyboard, has been around for over 60 years. The Dvorak
keyboard has a different arrangement of keys, in which the most common letters, E, T, S,
etc., are on the home row right under the typist's fingers. This improved layout requires the
typist's fingers to travel1116th as far, resulting in an average of20% increase in typing speed.
Unfortunately, the de facto standard in keyboards is the inefficient QWERTY configuration,
and people are reluctant to learn a new layout.
This paper describes another approach to improving typing efficiency, which can be used
with either the QWERTY or DVORAK keyboards. It takes advantage of the hundreds of
thousands of computer cycles between the typist's keystrokes which are typically wasted
while the computer idly waits for additional input. By spending those cycles trying to predict
what the user will type next, and allowing the typist to accept the prediction with a single
keystroke, substantial time and effort can be saved over typing the entire text manUally.
There are actually several such systems available today, including a package called ""Autocompletion"" developed for gnu-emacs by the author, and an application called ""Magic
Typist"" developed for the Apple Macintosh by Olduvai Software. Each of these maintains
a database of previously typed words, and suggests completions for the word the user is
currently in the middle of typing, which can be accepted with a single keystroke. While reasonable useful, both have substantial drawbacks. These systems use a very naive technique
for calculating the best completion, simply the one that was typed most recently. In fact,
experiments conducted for this paper indicated that this ""most recently used"" heuristic is
correct only about 40% of the time. In addition, these two systems are annoyingly verbose,
always suggesting a completion if a word has been typed previously which matches the
prefix typed so far. They interrupt the user's typing to suggest a completion even if the
word they suggest hasn't been typed in many days, and there are many other alternative
completions for the prefix, making it unlikely that the suggestion will be correct. These
drawbacks are so severe that these systems frequently decrease the user's typing speed,
rather than increase it.
The Auto'JYpist system described in this paper employs an artificial neural network during the
spare cycles between keystrokes to make more intelligent decisions about which completions
to display, and when to display them.

2 The Prediction Task
To operationalize the goal of making more intelligent decisions about which completions
to display, we have defined the neural networks task to be the following: Given a list of
candidate completions for the word currently being typed, estimate the likelihood that the
user is actually typing each of them. For example, if the user has already types the prefix
""aut"", the word he is trying to typing could anyone of a large number of possibilities,
including ""autonomous"", ""automatic"", ""automobile"" etc. Given a list of these possibilities
taken from a dictionary, the neural network's task is to estimate the probability that each of
these is the word the user will type.
A neural network cannot be expected to accurately estimate the probability for a particular
completion based on a unique representation for each word, since there are so many words

A Connectionist Technique for Accelerated Textual Input

ATTRIBUTE
absolute age
relative age

absolute frequency
relative frequency
typed previous
total length
remaining length
special character match

capitalization match

1041

DESCRIPTION
time since word was last typed
ratio of the words age to age of the
most recently typed alternative
number of times word has been typed
in the past
ratio of the words frequency to that
of the most often typed alternative
1 if user has typed word previously,
ootherwise
the word's length, in characters
the number of characters left after the
prefix to be typed for this word
the percentage of ""special characters""
(Le. not a-z) in this word relative to the
percentage of special characters typed
recently
1 if the capitalization of the prefix the
user has already typed matches the word's
usual capitalization, 0 otherwise.

Table 1: Word attributes used as input to the neural network for predicting word probabilities.
in the English language, and there is only very sparse data available to characterize an
individual's usage pattern for any single word. Instead, we have chosen to use an input
representation that contains only those characteristics of a word that could conceivably have
an impact on its probability of being typed. The attributes we employed to characterize each
completion are listed in Table 1.
These are not the only possible attributes that could be used to estimate the probability of
the user typing a particular word. An additional characteristic that could be helpful is the
word's part of speech (i.e. noun, verb, adjective, etc.). However this attribute is not typically
available or even meaningful in many typing situations, for instance when typing computer
programs. Also, to effectively exploit information regarding a word's part of speech would
require the network to have knowledge about the context of the current text. In effect, it
would require at least an approximate parse tree of the current sentence. While there are
techniques, including connectionist methods [Jain, 1991], for generating parse trees, they
are prone to errors and computationally expensive. Since word probability predictions in
our system must occur many times between each key the user types, we have chosen to
utilize only the easy to compute attributes shown in Table 1 to characterize each completion.

3 Network Processing
The network architecture employed for this system is a feedforward multi-layer perceptron.
Each of the networks investigated has nine input units, one for each of the attributes listed
in Table 1, and a single output unit. As the user is typing a word, the prefix he has typed so
far is used to find candidate completions from a dictionary, which contains 20,000 English
words plus all words the user has typed previously. For each of these candidate completions,
the nine attributes in Table 1 are calculated, and scaled to the range of 0.0 to 1.0. These
values become the activations of the nine units in the input layer. Activation is propagated
through the network to produce an activation for the single output unit, representing the

1042

Dean Pomerleau

probability that this particular candidate completion is the one the user is actually typing.
These candidate probabilities are then used to determine which (if any) of the candidates
should be displayed to the typist, using a technique described in a later section.
To train the network, the user's typing is again monitored. After the user finishes typing a
word, for each prefix of the word a list of candidate completions, and their corresponding
attributes, is calculated. These form the input training patterns. The target activation for
the single output unit on a pattern is set to 1.0 if the candidate completion represented by
that pattern is the word the user was actually typing, and 0.0 if the candidate is incorrect.
Note that the target output activation is binary. As will be seen below, the actual output the
network learns to produce is an accurate estimate of the completion's probability. Currently,
training of the network is conducted off-line, using a fixed training set collected while a
user types normally. Training is performed using the standard backpropagation learning
algorithm.

4 Experiments
Several tests were conducted to determine the ability of multi-layer perceptrons to perform
the mapping from completion attributes to completion probability. In each of the tests,
networks were trained on a set of inputJoutputexemplars collected over one week of a single
subject's typing. During the training data collection phase, the subject's primary text editing
activities involved writing technical papers and composing email, so the training patterns
represent the word choice and frequency distributions associated with these activities. This
training set contained of 14,302 patterns of the form described above.
The first experiment was designed to determine the most appropriate network architecture
for the prediction task. Four architecture were trained on a 10,000 pattern subset of the
training data, and the remaining 4,302 patterns were used for cross validation. The first of
the four architectures was a perceptron, with the input units connected directly to the single
output unit. The remaining three architectures had a single hidden layer, with three, six
or twelve hidden units. The networks with hidden units were fully connected without skip
connections from inputs to output. Networks of three and six hidden units which included
skip connections were tested, but did not exhibit improved performance over the networks
without skip connections, so they are not reported.
Each of the network architectures were trained four times, with different initial random
weights. The results reported are those produced by the best set of weights from these
trials. Note that the variations between trials with a single architecture were small relative
to the variations between architectures. The trained networks were tested on a disjoint set
of 10,040 collected while the same subject was typing another technical paper.
Three different performance metrics were employed to evaluate the performance of these
architectures on the test set. The first was the standard mean squared error (MSE) metric,
depicted in Figure 1. The MSE results indicate that the architectures with six and twelve
hidden units were better able to learn the task than either the perceptron, or the network with
only three hidden units. However the difference appears to be relatively small, on the order
of about 10%.
MSE is not a very informative error metric, since the target output is binary (1 if the
completion is the one the user was typing, 0 otherwise), but the real goal is to predict
the probability that the completion is correct. A more useful measure of performance is
shown in Figure 2. For each of the four architectures, it depicts the predicted probability
that a completion is correct, as measured by the network's output activation value, vs. the

1043

A Connectionist Technique for Accelerated Textual Input

0.095

0.070 ......._ _

Perceptron

3 Hidden
Units

6 Hidden
Units

12 Hidden
Units

Figure 1: Mean squared error for four networks on the task of predicting completion
probability.

actual probability that a completion is correct. The lines for each of the four networks
were generated in the following manner. The network's output response on each of the
10,040 test patterns was used to group the test patterns into 10 categories. All the patterns
which represented completions that the network predicted to have a probability of between
o and 10% of being correct (output activations of 0.0-0.1) were placed in one category.
Completions that the network predicted to have a 10-20% change of being right were placed
in the second category, etc. For each of these 10 categories, the actual likelihood that
a completion classified within the category is correct was calculated by determining the
percent of the completions within that category that were actually correct.
As a concrete example, the network with 6 hidden units produced an output activation
between 0.2 and 0.3 on 861 of the 10,040 test patterns, indicating that on these patterns
it considered there to be a 20-30% chance that the completion each pattern represented
was the word the user was typing. On 209 of these 861 patterns in this category, the
completion was actually the one the user was typing, for a probability of 24.2%. Ideally, the
actual probability should be 25%, half way between the minimum and maximum predicted
probability thresholds for this category. This ideal classification performance is depicted as
the solid 45? line labeled ""Target"" in Figure 2. The closer the line for a given network matches
this 45? line, the more the network's predicted probability matches the actual probability
for a completion. Again, the networks with six and twelve hidden units outperformed the
networks with zero and three hidden units, as illustrated by their much smaller deviations
from the 45? line in Figure 2.
The output activations produced by the networks with six and twelve hidden units reflect
the actual probability that the completion is correct quite accurately. However prediction
accuracy is only half of what is required to perform the final system goal, which recall was
to identify as many high probability completions as possible, so they can be suggested to
the user without requiring him to manually type them. If overall accuracy of the probability
predictions were the only requirement, a network could score quite highly by classifying

1044

Dean Pomerleau

1.00

e
-a

0
.....

.....
.D

Perceptron

0.80

~

~

3 Hidden Units
6 Hidden Units

.D
~

n;;get

0.60

12 Hidden Units

0.40

u

<

0.20
0.00

Figure 2: Predicted vs.
architectures tested.

actual probability of a completion being correct for the four

every pattern into the 10-20% category, since about 15% of the 10,040 completions in the
test set represent the word the user was typing at the time. But a constant prediction of
10-20% probability on every alternative completion would not allow the system to identify
and suggest to the user those individual completions that are much more likely than the other
alternatives.
To achieve the overall system goal, the network must be able to accurately identify as many
high probability completions as possible. The ability of each of the four networks to achieve
this goal is shown in Figure 3. This figures shows the percent of the 10,040 test patterns each
of the four networks classified as having more than a 60% probability of being correct. The
60% probability threshold was selected because it represents a level of support for a single
completion that is significantly higher than the support for all the others. As can be seen in
Figure 3, the networks with hidden units again significantly outperformed the perceptron,
which was able to correctly identify fewer than half as many completions as highly likely.

5

Auto1)rpist System Architecture and Performance

The networks with six and twelve hidden units are able to accurately identify individual
completions that have a high probability of being the word the user is typing. In order
to exploit this prediction ability and speed up typing, we have build an X-window based
application called AutoTypist around the smaller of the two networks. The application
serves as the front end for the network, monitoring the user's typing and identifying likely
completions for the current word between each keystroke. If the network at the core of
AutoTypist identifies a single completion that it is both significantly more probably than all
the rest, and also longer than a couple characters, it will momentarily display the completion
after the current cursor location in whatever application the user is currently typing 1? If the
displayed completion is the word the user is typing, he can accept it with a single keystroke
(The criterion for displaying a completion, and the human interface for AutoTypist, are somewhat
more sophisticated than this description. However for the purposes of this paper, a high level
description is sufficient.

A Connectionist Technique for Accelerated Textual Input

1045

Percent of
6.0
Patterns Classified 5 .0
as over 60%
4.0
Probable
3.0

2.0
1.0

Perceptron

3 Hidden
Units

6 Hidden
Units

12 Hidden
Units

Figure 3: Percent of candidate completions classified as having more than a 60% chance of
being correct for the four architectures tested.

and move on to typing the next word. If the displayed completion is incorrect, he can
continue typing and the completion will disappear.
Quantitative results with the fully integrated Auto1Ypist system, while still preliminary, are
very encouraging. In a two week trial with two subjects, who could type at 40 and 60 wpm
without AutoTypists, their typings speeds were improved by 2.37% and 2.21 % respectively
when typing English text. Accuracy improvements during these trials were even larger,
since spelling mistakes become rare when AutoTypist is doing a significant part of the
typing automatically. When writing computer programs, speed improvements of 12.93%
and 18.47% were achieved by the two test subjects. This larger speedup was due to the
frequent repetition of variable and function names in computer programs, which Auto1Ypist
was able to expedite. Not only is computer code faster to produce with AutoTypist, it is
also easier to understand. AutoTypist encourages the programmer to use long, descriptive
variable and function names, by making him type them in their entirety only once. On
subsequent instances of the same name, the user need only type the first few characters and
then exploitAutoTypist's completion mechanism to type the rest. These speed improvements
were achieved by subjects who are already relatively proficient typists. Larger gains can
be expected for less skilled typists, since typing an entire word with a single keystroke will
save more time when each keystroke takes longer.
Perhaps an even more significant benefit results from the reduced number of keystrokes
Auto1Ypist requires the user to type. During the test trials described above, the two test
subjects had to strike an average of 2.89% fewer keys on the English text, and 16.42% fewer
keys on the computer code than would have been required to type the text out in its entirety.
Clearly this keystroke savings has the potential to benefit typists who suffer from repeated
stress injuries brought on by typing.
Unfortunately it is impossible to quantitatively compare these results with those of the other
completion-based typing aids described in the introduction, since the other systems have
not been quantitatively evaluated. Subjectively, Auto1Ypist is far less disturbing than the

1046

Dean Pomerleau

alternatives, since it only displays a completion when there is a very good chance it is the
correct one.

6

Future Work

Further experiments are required to verify the typing speed improvements possible with
AutoTypist, and to compare it with alternative typing improvement systems. Preliminary
experiments suggest a network trained on the word usage patterns of one user can generalize
to that of other users, but it may be necessary to train a new network for each individual
typist. Also, the experiments conducted for this paper indicate that a network trained on
one type of text, English prose, can generalize to text with quite different word frequency
patterns, C language computer programs. However substantial prediction improvements,
and therefore typing speedup, may be possible by training separate networks for different
types of text. The question of how to rapidly adapt a single network, or perhaps a mixture
of expert networks, to new text types is one which should be investigated.
Even without these extensions, AutoTypist has the potential to greatly improve the comfort
and efficiency of the typing tasks. For people who type English text two hours per workday,
even the conservative estimate of a 2% speedup translates into 10 hours of savings per
year. The potential time savings for computer programming is even more dramatic. A
programmer who types code two hours per workday could potentially save between 52
and 104 hours in a single year by using AutoTypist. With such large potential benefits,
commercial development of the AutoTypist system is also being investigated.
Acknowledgements

I would like to thank David Simon and Martial Hebert for their helpful suggestions, and for
acting as willing test subjects during the development of this system.

References
[Baluja & Pomerleau, 1993] Baluja, S. and Pomerleau, D.A. (1993) Non-Intrusive Gaze
Tracking Using Artificial Neural Networks. In Advances in Neural Information Processing Systems 6, San Mateo, CA: Morgan Kaufmann Publishers.
[Jain,1991] Jain, A.N. (1991) PARSEC: A connectionist learning architecture for parsing
spoken language. Carnegie Mellon University School of Computer Science Technical
Report CMU-CS-91-208.
[LeCun et al., 1989] LeCun, Y., Boser, B., Denker, 1.S., Henderson, D., Howard, R.E.,
Hubbard, W., and Jackel, L.D. (1989) Backpropagation applied to handwritten zip
code recognition. Neural Computation 1(4).
[Waibel et al., 1988] Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., Lang, K. (1988)
Phoneme recognition: Neural Networks vs. Hidden Markov Models. Proceedings from
Int. Conf on Acoustics, Speech and Signal Processing, New York, New York.

"
1016,1994,Connectionist Speaker Normalization with Generalized Resource Allocating Networks,,1016-connectionist-speaker-normalization-with-generalized-resource-allocating-networks.pdf,Abstract Missing,"Connectionist Speaker Normalization
with Generalized
Resource Allocating Networks

Cesare Furlanello
Istituto per La Ricerca
Scientifica e Tecnologica
Povo (Trento), Italy
furlan?lirst. it

Diego Giuliani
Istituto per La Ricerca
Scientifica e Tecnologica
Povo (Trento), Italy
giuliani?lirst.it

Edmondo Trentin
Istituto per La Ricerca
Scientifica e Tecnologica
Povo (Trento), Italy
trentin?lirst.it

Abstract
The paper presents a rapid speaker-normalization technique based
on neural network spectral mapping. The neural network is used
as a front-end of a continuous speech recognition system (speakerdependent, HMM-based) to normalize the input acoustic data from
a new speaker. The spectral difference between speakers can be
reduced using a limited amount of new acoustic data (40 phonetically rich sentences). Recognition error of phone units from the
acoustic-phonetic continuous speech corpus APASCI is decreased
with an adaptability ratio of 25%. We used local basis networks of
elliptical Gaussian kernels, with recursive allocation of units and
on-line optimization of parameters (GRAN model). For this application, the model included a linear term. The results compare
favorably with multivariate linear mapping based on constrained
orthonormal transformations.

1

INTRODUCTION

Speaker normalization methods are designed to minimize inter-speaker variations,
one of the principal error sources in automatic speech recognition. Training a speech
recognition system on a particular speaker (speaker-dependent or SD mode) generally gives better performance than using a speaker-independent system, which is

868

Cesare Furlanello. Diego Giuliani. Edmondo Trentin

trained to recognize speech from a generic user by averaging over individual differences. On the other hand, performance may be dramatically worse when a SD
system ""tailored"" on the acoustic characteristics of a speaker (the reference speaker)
is used by another one (the new or target speaker). Training a SD system for any
new speaker may be unfeasible: collecting a large amount of new training data
is time consuming for the speaker and unacceptable in some applications. Given
a pre-trained SD speech recognition system, the goal of normalization methods is
then to reduce to a few sentences the amount of training data required from a new
speaker to achieve acceptable recognition performance. The inter-speaker variation
of the acoustic data is reduced by estimating a feature vector transformation between the acoustic parameter space of the new speaker and that of the reference
speaker (Montacie et al., 1989; Class et al., 1990; Nakamura and Shikano, 1990;
Huang, 1992; Matsukoto and Inoue, 1992). This multivariate transformation, also
called spectral mapping given the type of features considered in the parameterization of speech data, provides an acoustic front-end to the recognition system.
Supervised speaker normalization methods require that the text of the training utterances required from the new speaker is known, while arbitrary utterances can
be used by unsupervised methods (Furui and Sondhi, 1991). Good performance
have been achieved with spectral mapping techniques based on MSE optimization
(Class et al., 1990; Matsukoto and Inoue, 1992). Alternative approaches presented
estimation of the spectral normalization mapping with Multi-Layer Perceptron neural networks (Montacie et al., 1989; Nakamura and Shikano, 1990; Huang, 1992;
Watrous, 1994).
This paper introduces a supervised speaker normalization method based on neural
network regression with a generalized local basis model of elliptical kernels (Generalized Resource Allocating Network: GRAN model). Kernels are recursively allocated
by introducing the heuristic procedure of (Platt, 1991) within the generalized RBF
schema proposed in (Poggio and Girosi, 1989). The model includes a linear term
and efficient on-line optimization of parameters is achieved by an automatic differentiation technique. Our results compare favorably with normalization by affine
linear transformations based on orthonormal constrained pseudoinverse. In this paper, the normalization module was integrated and tested as an acoustic front-end for
speaker-dependent continuous speech recognition systems. Experiments regarded
phone units recognition with Hidden Markov Model (HMM) recognition systems.
The diagram in Figure 1 outlines the general structure of the experiment with
GRAN normalization modules. The architecture is independent from the specific
speech recognition system and allows comparisons between different normalization
techniques. The GRAN model and a general procedure for data standardization are
described in Section 2 and 3. After a discussion of the spectral mapping problem
in Section 4, the APASCI corpus used in the experiments and the characteristics
of the acoustic data are described in Section 5. The recognition system and the
experiment set-up are detailed in Sections 6-8. Results are presented and discussed
in Section 9.

Connectionist Speaker Normalization with Generalized Resource Allocating Networks

DataBase:
reference phrase

869

phraseS

(Yj } j - I ? ...? ]

Dynamic Time Warping
Training

fx)

I(Xi(t), Yj(t}}
-'

Test

i-I ?...? I

'-------------------""1

Neural Network
supervised training

:

GRAN normalizati

Feature Extraction

Speech Signal
corresponding to phrase S
uttered by a new speaker

Output

Figure 1: System overview

2

THE GRAN MODEL

Feedforward artificial neural networks can be regarded as a convenient realization
of general functional superpositions in terms of simpler kernel functions (Barron
and Barron, 1988). With one hidden layer we can implement a multivariate superposition f(z) = Ef=o cxjKj(z,wj) where Kj is a function depending on an
input vector z and a parameter vector Wj, a general structure which allows to realize flexible models for multivariate regression. We are interested in the schema:
y = H K(x) + Ax + b with input vector x E Rd 1 and estimated output vector y E R 2 . K = (Kj) is a n-dimensional vector of local kernels, H is the
d2 x n real matrix of kernel coefficients, b E R d 2 is an offset term and A is a
d2 x d1 linear term. Implemented kernels are Gaussian, Hardy multiquadrics, inverse of Hardy multiquadrics and Epanenchnikov kernels, also in the NadarayaWatson normalized form (HardIe, 1990). The kernel allocation is based on a
recursive procedure: if appropriate novelty conditions are satisfied for the example (x', y/), a new kernel Kn+1 is allocated and the new estimate Yn+l becomes
Yn+l (x) = Yn(X) + Kn+1 (llx - x'llw)(y' - Yn(X)) (HardIe, 1990). Global properties and rates of convergence for recursive kernel regression estimates are given in
(Krzyzak, 1992). The heuristic mechanism suggested by (Platt, 1991) has been
extended to include the optimization of the weighted metrics as requested in the
generalized versions of RBF networks of (Poggio and Girosi, 1989). Optimization
regards kernel coefficients, locations and bandwidths, the offset term, the coefficient matrix A if considered, and the W matrix defining the weighted metrics in
the input space: IIxll~ = xtwtWx. Automatic differentiation is used for efficient
on-line gradient-descent procedure w.r. t. different error functions (L2, L1, entropy
fit), with different learning rates for each type of parameters.

870

Cesare FurLanello, Diego GiuLiani, Edmondo Trentin

Ij;-::=<p
X -----------+"" Y

TJx

TJy

-1

TJy

x -----------"" Y
Figure 2: Commutative diagram for the speaker normalization problem. The spectral mapping <p between original spaces X and Y is estimated by Ij; = TJy 1 . ip . TJx,
obtained by composition of the neural GRAN mapping ip between PCA spaces X
and Y with the two invertible PCA transformations TJx and TJy.

3

NETWORKS AND PCA TRANSFORMATIONS

The normalization module is designed to estimate a spectral mapping between the
acoustic spaces of two different speakers. Inter-speaker variability is reflected by
significant differences in data distribution in these multidimensional spaces (we considered 8 dimensions); in particular it is important to take into account global data
anisotropy. More generally, it is also crucial to decorrelate the features describing
the data. A general recipe is to apply the well-known Principal Component Analysis (PCA) to the data, in this case implemented from standard numerical routines
based on Singular Value Decomposition of the data covariance matrices. The network was applied to perform a mapping between the new feature spaces obtained
from the PCA transformations, mean translation included (Figure 2).

4

THE SPECTRAL MAPPING PROBLEM

A sound uttered by a speaker is generally described by a sequence offeature vectors
obtained from the speech signal via short-time spectral analysis (Sec. 5). The spectral representations of the same sequence of sounds uttered by two speakers are subject to significant variations (e.g. differences between male and female speakers, regional accents, ... ). To deal with acoustic differences, a suitable transformation (the
spectral mapping) is seeked which performs the ""best"" mapping between the corresponding spectra oftwo speakers. Let Y = (Yl, Y2, ... , YJ) and X = (x 1, X2, ... , XI) be
the spectral feature vector sequences of the same sentence uttered by two speakers,
called respectively the reference and the new speaker. The desired mapping is performed by a function <pC Xi) such that the transformed vector sequence obtained from
X = (Xi) approximates as close as possible the spectral vector sequence Y = (Yi).
To eliminate time differences between the two acoustic realizations, a time warping
function has to be determined yielding pairs C(k) = (i(k),j(k))k=1.. .K of corresponding indexes of feature vectors in X and Y, respectively. The desired spectral

Connectionist Speaker Normalization with Generalized Resource Allocating Networks

87 J

mapping r,o(Xi) is the one which minimizes Ef=l d(Yj(k)' r,o(Xi(k?)) where d(?,?) is a
distorsion measure in the acoustic feature space. To estImate the transformation, a
set of supervised pairs (Xi(k), Yj(k?) is considered. In summary, the training material
considered in the experiments consisted of a set of vector pairs obtained by applying
the Dynamic Time Warping (DTW) algorithm (Sakoe and Chiba, 1978) to a set
of phrases uttered by the reference and the new speaker.

5

THE APASCI CORPUS

The experiments reported in this paper were performed on a portion of APASCI,
an italian acoustic-phonetic continuous speech corpus. For each utterance, text
and phonetic transcriptions were automatically generated (Angelini et al., 1994).
The corpus consists of two portions. The first part, for the training and validation of speaker independent recognition systems, consists of a training set (2140
utterances), a development set (900 utterances) and a test set (860 utterances).
The sets contain, respectively, speech material from 100 speakers (50 males and 50
females), 36 speakers (18 males and 18 females) and 40 speakers (20 males and 20
females). The second portion of the corpus is for training and validation of speaker
dependent recognition systems. It consists of speech material from 6 speakers (3
males and 3 females). Each speaker uttered 520 phrases, 400 for training and 120
for test. Speech material in the test set was acquired in different days with respect
to the training set. A subset of 40 utterances from the training material forms the
adaptation training set, to be used for speaker adaptation/normalization purposes.
For this application, each signal in the corpus was processed to obtain its parametric
representation. The signal was preemphasized using a filter with transfer function
H(z)
1 - 0.95 X z-l, and a 20 ms Hamming window is then applied every 10
ms. For each frame, the normalized log-energy as well as 8 Mel Scaled Cepstral
Coefficients (MSCC) based on a 24-channel filter-bank were computed. Normalization of log-energy was performed by subtracting the maximum log-energy value in
the sentence; for each Mel coefficient, normalization was performed by subtracting
the mean value of the whole utterance. For both MSCC and the log-energy, the
first order derivatives as well as the second order derivatives were computed. For
each frame, all the computed acoustic parameters were combined in a single feature
vector with 27 components.

=

6

THE RECOGNITION SYSTEM

For each of the 6 speakers, a SD HMM recognition system was trained with the 400
utterances available in the APASCI corpus; the systems were bootstrapped with
gender dependent models trained on the gender dependent speech material (1000
utterances for male and 1140 utterances for female). A set of 38 context independent
acoustic-phonetic units was considered. Left-to-right HMMs with three and four
states were adopted for short (i.e. p,t,k,b,d,g) and long (e.g. a,i,u,Q,e) sounds
respectively. Silence, pause and breath were modeled with a single state ergodic
model. The output distribution probabilities were modeled with mixtures of 16
gaussian probability densities, diagonal covariance matrixes. Transitions leaving
the same state shared the same output distribution probabilities.

872

Cesare Furlanello, Diego Giuliani, Edmondo Trentin

Table 1: Phone Recognition Rate (Unit Accuracy %) without normalization

7

TRAINING THE NORMALIZATION MODULES

A set of 40 phrases was considered for each pair (new, re f erence) of speakers to train
the normalization modules. In order to take into account alternative pronunciation,
insertion or deletion of phonemes, pauses between words and other phenomena, the
automatic phonetic transcription and segmentation available in APASCI was used
for each utterance. Given two utterances corresponding to the same phrase, we considered only their segments having the same phonetic transcription. To determine
these segments the DTW algorithm was applied to the phonetic transcription of the
two utterances. The DTW algorithm was applied a second time to the obtained
segments and the resulting optimal alignment paths gave the desired set of vector
pairs. The DTW algorithm was applied only to the 8 MSCC and the other acoustic
parameters were left unmodified.
We trained networks with 8 inputs and 8 outputs. The model included a linear
term: first the linear term was fit to the data, and then the rest of the expansion
was estimated by fitting the residuals of the linear regression. The networks grew
up to 50 elliptical gaussian kernels using dynamic allocation. Kernel coefficients,
locations and bandwidths were optimized using different learning rates for 10 epochs
w.r.t the Ll norm, which proved to be more efficient than the usual L2 norm.

8

THE RECOGNITION EXPERIMENTS

Experiments concerned continuous phone recognition without any lexical and
phonetical constraint (no phone statistic was used).
For all the couples
(new, reference) of speakers in the database, a recognition experiment was performed using 90 (of the 120 available) test utterances from the new speaker with
the SD recognition system previously trained for the reference speaker. On average the test sets consisted of 4770 phone units. The experiments were repeated
transforming the test data with different normalization modules and performance
compared. Results are expressed in terms of insertions (Ins), deletions (Del) and
substitutions (Sub) of phone units made by the recognizer. Unit Accuracy (U A)
and Percent Correct (PC) performance indicators are respectively defined w.r.t.
the total number of units nunih as U A = 100 (1 - (Ins + Del + Sub)/nunit.) and
PC = 100 (1 - (Del + Sub)/nunit.). In Table 1 the baseline speaker dependent
performance for the 6 speaker dependent systems is reported. Row labels indicate the speaker reference model while column labels identify whose target acous-

Connectionist Speaker Normalization with Generalized Resource Allocating Networks

873

Table 2: Phone Recognition Rate (Unit Accuracy %) with NN normalization

tic data are used. Thus U A and PC entries in the main diagonal are for the
same speaker who trained the system while the remaining entries relate to performance obtained with new speakers. We also considered the adaptability ratios for
a U A and P PC (Montacie et al., 1989): Pa (aRT - aRT )/(aRR - aRT) and
Pp = (PRT - PRT )/(PRR - PRT) where aRT indicate accuracy for reference speaker
R and target T without normalization, aRR is the speaker dependent baseline accuracy and apex n indicates normalization. The same notation applies to the percent
correct adaptability ratio pp.

=

9

=

=

RESULTS AND CONCLUSIONS

Normalization experiments have been performed with the set-up described in the
previous Section. The phone recognition rates obtained with normalization modules
based on the GRAN model are reported in Table 2 in terms of Unit Accuracy (dee
Table 1 for the baseline performance). In Table 3 the performance of the GRAN
model (NN) and constrained orthonormal linear mapping (LIN) are compared with
the baseline performance (SD: no adaptation) in terms of both Unit Accuracy and
Percent Correct. The network shows an improvement, as evidenced by the variation
in the Pa and Pp values. Results are reported averaging performance over all the
pairs (new,reference) of speakers (Total column), and considering pairs of speakers
of the same gender and of different genders (Female: only female subjects, Male:
only males, Dill: different genders). An analysis of the adaptability ratios shows
that the effect of the network normalization is higher than with the linear network
for all the 3 subgroups of pairs: p~N = 0.20 vs p~IN = 0.16 for the Female
couples and liN = 0.16 vs p~IN = 0.15 for the Male couples. The improvement is
higher (p~N = 0.28, p~IN = 0.24) for speaker of different genders. Although these
preliminary experiments show only a minor improvement of performance achieved
by the network with respect to linear mappings, we expect that the selectivity of
the network could be exploited using acoustic contexts and code dependent neural
networks.
Acknowledgements
This work has been developed within a grant of the ""Programma Nazionale di
Ricerca per la Bioelettronica"" assigned by the Italian Ministry of University and
Technologic Research to Elsag Bailey. The authors would like to thank B. Angelini,
F. Brugnara, B. Caprile, R. De Mori, D. Falavigna, G. Lazzari and P. Svaizer.

874

Cesare Furlanello, Diego Giuliani, Edmondo Trentin

Table 3: Phone Recognition Rate (%) in terms of both Unit Accuracy, Percent
Correct, and adaptability ratio p.

References

Angelini, B., Brugnara, F., Falavigna, D., Giuliani, D., Gretter, R., and Omologo,
M. (September 1994). Speaker Independent Continuous Speech Recognition Using
an Acoustic-Phonetic Italian Corpus. In Proc. of ICSLP, pages 1391-1394.
Barron, A. R. and Barron, R. L. (1988). Statistical learning networks: a unifying
view. In Symp. on the Interface: Statistics and Computing Science, Reston, VI.
Class, F., Kaltenmeier, A., Regel, P., and Troller, K. (1990). Fast speaker adaptation for speech recognition system. In Proc. of ICASSP 90, pages 1-133-136.
Furui, S. and Sondhi, M. M., editors (1991). Advances in Speech Signal Processing.
Marcel Dekker and Inc.
HardIe, W. (1990). Applied nonparametric regression, volume 19 of Econometric
Society Monographs. Cambridge University Press, New York.
Huang, X. D. (1992). Speaker normalization for speech recognition. In Proc. of
ICASSP 92, pages 1-465-468.
Krzyzak, A. (1992). Global convergence of the recursive kernel regression estimates
with applications in classification and nonlinear system estimation. IEEE Transactions on Information Theory, 38(4):1323-1338.
Matsukoto, H. and Inoue, H. (1992). A piecewise linear spectral mapping for supervised speaker adaptation. In Proc. of ICASSP 92, pages 1-449-452.
Montacie, C., Choukri, K., and Chollet, G. (1989). Speech recognition using temporal decomposition and multi-layer feed-forward automata. In Proc. of ICASSP
89, pages 1-409-412.
Nakamura, S. and Shikano, K. (1990). A comparative study of spectral mapping
for speaker adaptation. In Proc. of ICASSP 90, pages 1-157-160.
Platt, J. (1991). A resource-allocating network for function interpolation. Neural
Computation, 3(2):213-225.
Poggio, T. and Girosi, F. (1989). A theory of networks for approximation and
learning. A.1. Memo No. 1140, MIT.
Sakoe, H. and Chiba, S. (1978). Dynamic programming algorithm optimization for
spoken word recognition. IEEE-A SSP, 26(1):43-49.
Watrous, R. (1994). Speaker normalization and adaptation using second-order connectionist networks. IEEE Trans. on Neural Networks, 4(1):21-30.

"
1017,1994,A Critical Comparison of Models for Orientation and Ocular Dominance Columns in the Striate Cortex,,1017-a-critical-comparison-of-models-for-orientation-and-ocular-dominance-columns-in-the-striate-cortex.pdf,Abstract Missing,"A Critical Comparison of Models for
Orientation and Ocular Dominance
Columns in the Striate Cortex
E. Erwin
Beckman Institute
University of Illinois
Urbana, IL 61801, USA

K. Obermayer
Technische Fakultat
U niversitat Bielefeld
33615 Bielefeld, FRG

K. Schulten
Beckman Institute
University of Illinois
Urbana, IL 61801, USA

Abstract
More than ten of the most prominent models for the structure
and for the activity dependent formation of orientation and ocular dominance columns in the striate cort(>x have been evaluated.
We implemented those models on parallel machines, we extensively
explored parameter space, and we quantitatively compared model
predictions with experimental data which were recorded optically
from macaque striate cortex.
In our contribution we present a summary of our results to date.
Briefly, we find that (i) despite apparent differences, many models
are based on similar principles and, consequently, make similar predictions, (ii) certain ""pattern models"" as well as the developmental
""correlation-based learning"" models disagree with the experimental data, and (iii) of the models we have investigated, ""competitive
Hebbian"" models and the recent model of Swindale provide the
best match with experimental data.

1

Models and Data

The models for the formation and structure of orientation and ocular dominance
columns which we have investigated are summarized in table 1. Models fall into
two categories: ""Pattern models"" whose aim is to achieve a concise description of
the observed patterns and ""developmental models"" which are focussed on the pro-

94

E. Erwin, K. Obermayer, K. Schulten

Class
Pattern
Models

Type
Structural
Models
Spectral
Models

Develop.
Models

Correlation
Based Learning
Competl bve
Hebbian
Other

Model
1. Icecube
2. Pinwheel
3. Gotz
4. Baxter
5. ROJer
6. Niebur
7. Swindale
8. Linsker
9. Miller
10. ~UM-h
11 . SOM-I
12. EN
13. Tanaka
14. Yuille

Reference
Hubel and Wiesel 1977 [~I
Braitenberg and Braitenberg 1979 161
Gotz 1987 (8)
Baxter and Dow 1989 11)
ROJer and Schwartz 1990 J20)
Niebur and Worgotter 1993 (15)
Swindale 1992a (21)
Linsker 1986c J12]
Miller 1989, 1994 113, 14)
Ubennayer, et. al. 1990 P~J
Obermayer, et. al. 1992(17)
Durbin and Mitchison 1990 (7)
Tanaka 1991 [22J
Yuille, et. al. 1992 (23)

Table 1: Models of visual cortical maps which have been evaluated.

cesses underlying their formation. Pattern models come in two varieties, ""structural
models"" and ""spectral models"", which describe orientation and ocular dominance
maps in real and in Fourier space, respectively. Developmental models fall into the
categories ""correlations based learning"", ""competitive Hebbian"" learning and a few
miscellaneous models.
Models are compared with data obtained from macaque striate cortex through optical imaging [2, 3, 4, 16]. Data were recorded from the representation of the parafovea
from the superficial layers of cortex. In the following we will state that a particular
model reproduces a particular feature of the experimental data (i) if there exists a
parameter regime where the model generates appropriate patterns and (ii) if the
phenomena are robust. We will state that a particular model does not reproduce a
certain feature (i) if we have not found an appropriate parameter regime and (ii) if
there exists either a proof or good intuitive reasons that a lllodel cannot reproduce
this feature.
One has to keep in mind, though, that model predictions are compared with a fairly
special set of data. Ocular dominance patterns, e.g., are known to vary between
species and even between different regions within area 17 of an individual. Consequently, a model which does not reproduce certain featurE'S of ocular dominance
or orientation colulllns in the macaque may well describE' those patterns in other
species. Interspecies differences, however, are not. the focus of this contribution;
results of corresponding modelling studies will be reported E'lsewhere.

2

Examples of Organizing Principles and Model Predictions

It has been suggested t.hat the most important principles underlying the pattern of
orientation and ocular dominance are ""continuity"" and ""diversity"" [7. 19, 21]. Continuity, because early image processing is often local in fE'atnre space, and diversity,
because, e.g., the visual system may want to avoid perceptual scotomata. The continuity and diversity principles underlie almost all dE'scriptive and developmental

A Critical Comparison of Models for Orientation and Ocular Dominance Columns

95

Figure 1: Typical patterns of orientation preferences as they are predicted by six
of the models list.ed in Table 1. Orientation preferences are coded by gray values,
where black - whit.e denotes preferences for vertical _ horizont.al - vertical. Top
row (left to right): Models 7, 11, 9. Bottom row (left to right) Models 5, 12, 8.

models, but. maps which comply with t.hese principles often differ in qualitat.ive ways:
The icecube model, e.g., obeys bot.h principles but. contains no singularities in the
orient.ation preference map and no branching of ocular dominance bands. Figure 1
shows orientat.ion maps generated by six different. algorithms taken from Tab. 1.
Although all pat.t.erns are consist.ent. wit.h the continuit.y and diversity const.raints,
closer comparison reveals differences. Thus additional element.s of organization must
be considered.
It has been suggested that maps are characterized by local correlations and global
disorder. Figure 2 (left) shows as an exam pIe two- point correlation functions of
orientation maps. The autocorrelation function [17] of one of the Cartesian coordinat.es of t.he orientation vector is plotted as a function of cortical distance. The
fact. that all correlation functions decay indicates that the orientation map exhibits
global disorder. Global disorder is predicted by all models except. the early pattern models 6, 8 and 9. Figure 2 (right) shows the corresponding power spectra.
Bandpass-like spectra which are typical for the experiment.al data [16] are well predicted by models 10- 12. Interestingly, they are not predicted by model 9, which
also fails reproducing the Mexican-hat shaped correlation functions (bold lines),
and model 13.
Based on the fact that. experimental maps are characterized by a bandpass-like
power spectrum it has been suggested that orientation maps may be organized

96

E. Erwin, K. Obermayer, K. Schulten

1.0 . . . _ - - - - - - - - - - - - - ,

--

1,0

"" '0

.~

?

1,6

S...

0,4

~

-0.5

.................- _ __....-_
10
20
30

~----:.""_

o

distance (normalized)

_4

40

1,8

&

0,2

0,0

0

5

10

15

20

distance (normalized)

Figure 2: Left: Spatial autocorrelation functions for one of the cartesian coordinates of the orientat.ion vector. Aut.ocorrelation functions were averaged over all
directions. Right: Complex power spectra of orientation maps. Power was averaged over all directions of the wave vector. Modelnumhers as in Tab. 1.

according to four principles [15]: continuity, diversity, homogeneity and isotropy.
If those principles are implemented using bandpass filtered noise the resulting
maps [15, 21] indeed share many properties with the experimental data. Above
principles alone, however, are not sufficient: (i) There are models such as model ?5
which are based on those principles but generate different patterns, (ii) homogeneity and isotropy are hardly ever fulfilled ([16] and next paragraph), and (iii) those
principles cannot. account for correlations between maps of various response properties [16].
Maps of orientation and ocular dominance in the macaque are anisotropic, i.e. ,
there exist preferred directions along which orientation and ocular dominance slabs
align [16]. Those anisotropies can emerge due to different mechanisms: (i) spontaneous symmetry breaking, (ii) model equations, which are not rotation invariant,
and (iii) appropriately chosen boundary conditions. Figure 3 illustrates mechanisms (ii) and (iii) for model 11. Bot.h mechanisms indeed predict anisotropic
pat.terns, however, preferred directions of orientation and ocular dominance align in
both cases (fig. 3, left and center). This is not true for the experimental data, where
preferred directions tend to be orthogonal [16]. Ort.hogonal preferred directions can
be generated by llsing different neighborhood funct.ions for different components of
the feature vector (fig. 3, right). However, this is not a satisfactory solution, and
the issue of anisotropies is still unsolved.
The pattern of orientation preference in the area 17 of the macaque exhibits four
local elements of organization: linear zones, singularit.ies, saddle point.s and fractures [16]. Those element.s are correctly predict.ed by most. of the pat.t,ern models,
except models 1- 3, and they appear in the maps generated by models 10- 14. Interestingly' models 9 and 13 predict very few linear zones, which is related to the
fact. that those models generate orientat.ion maps with lowpass-like power spect.ra.
Another important property of orientation maps is that orientation preferences and
their spatial layout across cortex are not correlated which each other. One conse-

A Critical Comparison of Models for Orientation and Ocular Dominance Columns

:.~ .
.~;

+

+

+

' .~,!-.

+

97

.

. 4-

+

Figure 3: Anisotropic orientation and ocular dominance maps generated by model
11. The figure shows Fourier spectra [17] of orientation (top row) and ocular dominance maps (bottom row). Left: Maps generated with an elliptic neighborhood
function (case (ii), see text); Center: Maps generated using circular input layers and an elliptical cortical sheet (case (iii), see text), Right: Maps generated
with different, elliptic neighborhood functions for orientation preference and ocular
dominance. '+' symbols indicate the locations of the origin.
quence is that there exist singularities, near which the curl of the orientation vector
field does not vanish (fig. 4, left). This rules out a class of pattern models where the
orientation map is derived from the gradient of a potential function, model 5. Figure 4 (right) shows another consequence of this property. In those figures cortical
area is plotted against the angular difference between the iso-orientation lines and
the local orientation preference. The even distribution found in the experimental
data is correctly predicted by models 1,6, 7 and 10-12. Model 8, however, predicts
preference for large difference angles while model 9 - over a wide range of parameters
- predicts preference for small difference angles (bold lines).
Finally, let us consider correlations between the patterns of orientation preference
and ocular dominance. Among the more prominent relationships present in macaque
data are [3, 16,21]: (i) Singularities are aligned with the centers of ocular dominance
bands, (ii) fractures are either aligned or run perpendicular, and (iii) iso-orientation
bands in linear zones intersect ocular dominance bands at approximately right angles. Those relationships are readily reproduced only by models 7 and 10- 12. For
model 9 reasonable orientation and ocular dominance patterns have not been generated at the same time. It would seem as if the parameter regime where reasonable
orientation columns emerge is incompatible with the parameter regime where ocular
dominance patterns are formed.

98

E. Erwin, K. Obermayer, K. Schulten

e'""
'0""

C+-I

0J.5

0.10

u

bO

'""

O.
=
~

8.

0. . +----.--_ _-....-_--1

03060

90

difference angle ( degrees)
Figure 4: Left: This singularity is an example of a feature in the experimental
data which is not allowed by model 5. The arrows indicat.e orientation vectors,
whose angular component is twice the value of the local orientation preference.
Right: Percentage of area as a function of the angular difference bet.ween preferred
orient.ation and t.he local orientation gradient vector. Model numbers as in Table 1.

3

The Current Status of the Model Comparison Project

Lack of space prohibit.s a detailed discussion of our findings hut we have summarized
the current status of our project in Tables 2 and 3. Given the models list.ed in
Tab. 1 and given the properties of t.he orientation and ocular dominance patt.erns
in macaque striate cortex listed in Tables 2 and 3 it is models 7 and 10-12 which
currently are in best agreement with the data. Those models, however, are fairly
abstract. and simplified, and they cannot easily be extended to predict receptive
field structure. Biological realism and predictions about. receptive fields are the
advantages of models 8 and 9. Those models, however, cannot account for the
observed orientation patterns. It. would, therefore, be of high interest, if elements
of both approaches could be combined to achieve a better description of the dat.a.
The main conclusion, however, is that there are now enough data available to allow
a better evaluation of model approaches than just by visual comparison of the
generated pat.terns. It. is our hope, that future studies will address at least those
propert.ies of t.he patterns which are known and well described, some of which are
list.ed in Tables 2 and 3. In case of developmental models more stringent tests
require experiments which (i) monitor the actual time-course of pattern formation,
and which (ii) study pattern development under experimentally modified conditions
(deprivation experiments). Currently there is not enough data available to constrain
models but the experiments are under way [5, 10, 11, 18].

Acknowledgements
VVe are very much indebted to Drs. Linsker, Tanaka and Yuille for sharing modelling
data. E.E. thanks t.he Beckman Institute for support.. K.O. thanks ZiF (Universitat
Bielefeld) for it.s hospitality. Computing time on a CM-2 and a CM-5 was made
available by NCSA.

A Critical Comparison of Models for Orientation and Ocular Dominance Columns

no.

1
2
3
4
5
6

7
8
9
10
11

12
13
14

disorder

bandpass

linear
zones

-

+
+
+
+
+
+
+
+

+
+
+
+
+
+
+

+2
+
+
+
+
+
+
+
+
+
+

-

+
+
+
-

?

-

+
+
+
?

Properties of OR Maps
saddle sing.
fracto indep.
points ?1/2
coord.

-

+
+
+
+
+
+
+
+
+
+
+
+
+

-

+
+2
+
+
+
+
+
+
+
+
+
+

-

-

+1
+1
+1
+
+
+1
+1
+1
+1
+

+

-

high
spec.
n
n
n
n

+
+

-

-

+

-

n

-/+
+
+
+
+

+
+
+
+
+

?

n

99

amsotropy

n
n

URbias
n
n
n
n
n
n
n
n
n

+
+
+

+
+
+

n
n

n
n

+
n
n

+
+
+
+

Table 2: Evaluation of orientation (OR) map models. Properties of the experimental maps include (left to right): global disorder; bandpass-like power spectra; the
presence of linear zones in roughly 50% of the map area; the presence of saddle
points, singularities (?1/2 with equal densities), and fractures; independence between cortical and orientation preference coordinates; a distribution favoring high
values of orientation specificity; global anisotropy; and a possible orientation bias.
Symbols: '+': There exists a parameter regime in which a model generates maps
with this property; '-': The model cannot reproduce this property; ""n': The model
makes no predictions; ""?': Not enough data available. 1 Models agree with the data
only if one assumes that fractures are loci of rapid orientation change rather than
real discontinuities. 20 ne of several cases.

References
[1] W. T. Baxter and B. M. Dow. Bioi. Cybern. , 61:171-182, 1989.
[2] G. G. Blasdel. J. Neurosci., 12:3115-3138, 1992.
[3] G. G. Blasdel. J. Neurosci., 12:3139-3161,1992.

[4] G. G. Blasdel and G. Salama. Nature, 321:579- 585, 1986.
[5] T. Bonhoeffer, D. Kim, and W. Singer. Soc. Neurosci. Abs., 19:1800, 1993.
[6] V. Braitenberg and C. Braitenberg. Bioi. Cybern., 33:179- 186, 1979.
[7] R. Durbin and G. Mitchison. Nature, 343:341-344, 1990.
[8] K. G. Gotz. Bioi. Cybern., 56:107-109, 1987.
[9] D. Rubel and T. N. Wiesel. Proc. Roy. Soc. Lond. B, 198:1-59, 1977.
[10] D. Rubel, T. N. Wiesel, and S. LeVay. Phil. Trans. Roy. Soc. Lond. B, 278:377409, 1977.

[11] D. Kim and T. Bonhoeffer. Soc. Neurosci. Abs., 19:1800, 1993.
[12] R. Linsker. Proc. Nat. Acad. Sci., USA, 83:8779-8783, 1986.

100

E. Erwin, K. Obermayer, K. Schulten

no.

segregation

1
2
3
4
5
6
7
8
9
10
11
12
13
14

+
n

+
n

+
n

+
n

+
+
+
+
+
+

Properties of OD Maps
straani so- ODdisorder tropy bias bismus
n
+
+
n
n
n
n
n
n
+
n
n
n
n
n
+
+
n
n
n
n
n
+
+
+
n
n
n
n

+
+
+
+
+
+

+
+
+
+
+
+

+
+
+
+
+
+

+
+
+
+
+
n

Correlations Between OR and OD
sing.
spec.
global
local
orthog. orthog. vs.OD vs.OD
+:.l
+M
n
n
n
n
n
+2
n
n
+
n
n
n
n

_I

+1

+I ,O!

_I

n

n

n

n

-

+

+

+

n

n

?1

?1
+
+2
+1,2

n
n
n
n
n

n
n

n

n

?1
+
+2
+1,2

?1
+
+2
+1 ,2

n
n

n
n

Table 3: Left: Evaluation of ocular dominance (OD) map models. Properties of
the experimental maps include (left to right): Segregated bands of eye dominance;
global disorder; bandpass-like power spectra; global anisotropy; a bias to the representation of one eye; and OD-patterns in animals with strabismus. Right: Evalu
ation of correlations between OD and OR. Experimental maps show (left to right):
Local and global orthogonality between OR and OD slabs; singularities preferably
in monocular regions, and lower OR specificity in monocular regions. 1 Authors
treated OD and OR in independent models, but we consider a combined version.
2 Correlations are stronger than in the experimental data.
M

[13] K. D. Miller. J. Neurosci., 14:409- 441, 1994.
[14] K. D. Miller, J. B. Keller, and M. P. Stryker. Science, 245:605-615, 1989.
[15] E. Niebur and F. Worgotter. In F. H. Eeckman and J. M. Bower, Computation
and Neura.l Systems, pp. 409-413. Kluwer Academic Publishers, 1993.
[16] K. Obermayer and G. G. Blasdel. J. Neurosci., 13:4114-4129, 1993.
[17] K. Obermayer, G. G. Blasdel, and K. Schulten. Phys. Rev. A, 45:7568-7589,
1992.
[18] K Obermayer, L. Kiorpes, and G. G. Blasdel. In J. D. Cowan at al., Advances
in Neural Information Processing Systems 6. Morgan Kaufmann, 1994. 543550.
[19] K. Obermayer, H. Ritter, and K. Schulten.
87:8345- 8349, 1990.

Proc. Nat. Acad. Sci., USA,

[20] A. S. Rojer and E. L. Schwartz. Bioi. Cybern., 62:381 - 391, 1990.
[21] N. V. Swindale. Bioi. Cybern., 66:217-230, 1992.
[22] S. Tanaka. Bioi. Cybern., 65:91- 98, 1991.
[23] A. L. Yuille, J. A. Kolodny, and C. W. Lee. TR 91-3, Harvard Robotics
Laboratory, 1991.

"
1018,1994,Generalization in Reinforcement Learning: Safely Approximating the Value Function,,1018-generalization-in-reinforcement-learning-safely-approximating-the-value-function.pdf,Abstract Missing,"Generalization in Reinforcement Learning:
Safely Approximating the Value Function

Justin A. Boyan and Andrew W. Moore
Computer Science Department
Carnegie Mellon University
Pittsburgh, PA 15213
jab@cs.cmu.edu, awm@cs.cmu .edu

Abstract
A straightforward approach to the curse of dimensionality in reinforcement learning and dynamic programming is to replace the
lookup table with a generalizing function approximator such as a neural net. Although this has been successful in the domain of backgammon, there is no guarantee of convergence. In this paper, we show
that the combination of dynamic programming and function approximation is not robust, and in even very benign cases, may produce
an entirely wrong policy. We then introduce Grow-Support, a new
algorithm which is safe from divergence yet can still reap the benefits
of successful generalization .

1

INTRODUCTION

Reinforcement learning-the problem of getting an agent to learn to act from sparse,
delayed rewards-has been advanced by techniques based on dynamic programming
(DP). These algorithms compute a value function which gives, for each state, the minimum possible long-term cost commencing in that state. For the high-dimensional
and continuous state spaces characteristic of real-world control tasks, a discrete representation of the value function is intractable; some form of generalization is required.
A natural way to incorporate generalization into DP is to use a function approximator,
rather than a lookup table, to represent the value function. This approach, which
dates back to uses of Legendre polynomials in DP [Bellman et al., 19631, has recently
worked well on several dynamic control problems [Mahadevan and Connell, 1990, Lin,
1993] and succeeded spectacularly on the game of backgammon [Tesauro, 1992, Boyan,
1992]. On the other hand, many sensible implementations have been less successful
[Bradtke, 1993, Schraudolph et al., 1994]. Indeed, given the well-established success

370

Justin Boyan, Andrew W. Moore

on backgammon, the absence of similarly impressive results appearing for other games
is perhaps an indication that using function approximation in reinforcement learning
does not always work well.
In this paper, we demonstrate that the straightforward substitution of function approximators for lookup tables in DP is not robust and, even in very benign cases, may
diverge, resulting in an entirely wrong control policy. We then present Grow-Support,
a new algorithm designed to converge robustly. Grow-Support grows a collection of
states over which function approximation is stable. One-step backups based on Bellman error are not used; instead, values are assigned by performing ""rollouts"" -explicit
simulations with a greedy policy. We discuss potential computational advantages of
this method and demonstrate its success on some example problems for which the
conventional DP algorithm fails.

2

DISCRETE AND SMOOTH VALUE ITERATION

Many popular reinforcement learning algorithms, including Q-Iearning and TD(O),
are based on the dynamic programmin~ algorithm known as value iteration [Watkins,
1989, Sutton, 1988, Barto et al., 1989J, which for clarity we will call discrete value
iteration. Discrete value iteration takes as input a complete model of the world as a
Markov Decision Task, and computes the optimal value function J*:

J* (x)

= the minimum possible sum of future costs starting from x

To assure that J* is well-defined, we assume here that costs are nonnegative and that
some absorbing goal state-with all future costs O-is reachable from every state. For
simplicity we also assume that state transitions are deterministic. Note that J* and
the world model together specify a ""greedy"" policy which is optimal for the domain:
optimal action from state x

= argmin(CosT(x,
a) + J*(NEXT-STATE(X, a)))
aEA

We now consider extending discrete value iteration to the continuous case: we replace
the lookup table over all states with a function approximator trained over a sample of
states. The smooth value iteration algorithm is given in the appendix. Convergence
is no longer guaranteed; we instead recognize four possible classes of behavior:
good convergence The function approximator accurately represents the intermediate value functions at each iteration (that is, after m iterations, the value
function correctly represents the cost of the cheapest m-step path), and successfully converges to the optimal J* value function.
lucky convergence The function approximator does not accurately represent the
intermediate value functions at each iteration; nevertheless, the algorithm
manages to converge to a value function whose greedy policy is optimal.
bad convergence The algorithm converges, i.e. the target J-values for the N training points stop changing, but the resulting value function and policy are
poor.
divergence Worst of all: small fitter errors may become magnified from one iteration
to the next, resulting in a value function which never stops changing.
The hope is that the intermediate value functions will be smooth and we will achieve
""good convergence."" Unfortunately, our experiments have generated all four of these
behaviors-and the divergent behavior occurs frequently, even for quite simple problems.

Generalization in Reinforcement Learning: Safely Approximating the Value Function

2.1

37 J

DIVERGENCE IN SMOOTH VALUE ITERATION

We have run simulations in a variety of domains-including a continuous gridworld,
a car-on-the-hill problem with nonlinear dynamics, and tic-tac-toe versus a stochastic opponent-and using a variety of function approximators, including polynomial
regression, backpropagation, and local weighted regression. In our experiments, none
of these function approximators was immune from divergence.
The first set ofresults is from the 2-D continuous gridworld, described in Figure 1.
By quantizing the state space into a 100 x 100 grid, we can compute J* with discrete
value iteration, as shown in Figure 2. The optimal value function is exactly linear:
J*(x, y) = 20 - lOx - lOy.
Since J* is linear, one would hope smooth value iteration could converge to it with a
function approximator as simple as linear or quadratic regression. However, the intermediate value functions of Figure 2 are not smooth and cannot be fit accurately by
a low-order polynomial. Using linear regression on a sample of 256 randomly-chosen
states, smooth value iteration took over 500 iterations before ""luckily"" converging to
optimal. Quadratic regression, though it always produces a smaller fit error than linear regression, did not converge (Figure 3). The quadratic function, in trying to both
be flat in the middle of state space and bend down toward 0 at the goal corner, must
compensate by underestimating the values at the corner opposite the goal. These
underestimates then enlarge on each iteration, as the one-step DP lookaheads erroneously indicate that points can lower their expected cost-to-go by stepping farther
away from the goal. The resulting policy is anti-optimal.
fontinuous Gridworld

J*(x,y)

0.8
0.6

>.
0.4
0.2
0L-0~.~2-0~.~4-0~.~6~0~.~8~1

x

Figure 1: In the continuous gridworld domain, the state is a point (x, y) E [0,1]2. There are
four actions corresponding to short steps (length 0.05, cost 0.5) in each compass direction,
and the goal region is the upper right-hand corner. l*(x, y) is linear.
Iteration 12

Iteration 25

.8

1

Figure 2: Computation of 1* by discrete value iteration

Iteration 40

Justin Boyan, Andrew W. Moore

372

Iteration 17

Iteration 43

Iteration 127

1
.8

.8

.8

1

Figure 3: Divergence of smooth value iteration with quadratic regression (note z-axis).
J*(x , y)

Iteration 144

o.
o.
>.

o.

.8

o.
0.20 . 40.60 . 8
x

1
1

Figure 4: The 2-D continuous gridworld with puddles, its optimal value function, and a
diverging approximation of the value function by Local Weighted Regression (note z-axis).
car- o n-the-Hill

J* (pa s, vel)

0.5

pas

Figure 5: The car-on-the-hill domain. When the velocity is below a threshold, the car must
reverse up the left hill to gain enough speed to reach the goal, so r is discontinuous.
Iteration 11

Iterati on 101

Iteration 201

Figure 6: Divergeri'ce oYsmooth value iteration wit~'
for car-on-th~~hill~ The
neural net, a 2-layer MLP with 80 hidden units, was trained for 2000 epochs per iteration.

It may seem as though the divergence of smooth value iteration shown above can be
attributed to the global nature of polynomial regression. In fact, when the domain
is made slightly less trivial, the same types of instabilities appear with even a highly

Generalization in Reinforcement Learning: Safely Approximating the Value Function

373

Table 1: Summary of convergence results: Smooth value iteration
Domain
2-D grid world
2-D puddle world
Car-on-the-hill

Linear
lucky

Quadratic
diverge

-

-

-

-

LWR
good
diverge
good

Backprop
lucky
diverge
diverge

local memory-based function approximator such as local weighted regression (LWR)
[Cleveland and Delvin, 1988]. Figure 4 shows the continuous gridworld augmented
to include two oval ""puddles"" through which it is costly to step. Although LWR can
fit the corresponding J* function nearly perfectly, smooth value iteration with LWR
nonetheless reliably diverges. On another two-dimensional domain, the car-on-the-hill
(Figure 5), smooth value iteration with LWR did converge, but a neural net trained
by backpropagation did not (see Figure 6) . Table 1 summarizes our results .
In light of such experiments, we conclude that the straightforward combination of
DP and function approximation is not robust. A general-purpose learning method
will require either using a function approximator constrained to be robust during DP
[Yee, 1992], or an algorithm which explicitly prevents divergence even in the face of
imperfect function approximation, such as the Grow-Support algorithm we present
in Section 3.
2.2 RELATED WORK
Theoretically, it is not surprising that inserting a smoothing process into a recursive
DP procedure can lead to trouble. In [Thrun and Schwartz, 1993] one case is analyzed
with the assumption that errors due to function approximation bias are independently
distributed. Another area of theoretical analysis concerns inadequately approximated
J* functions. In [Singh and Yee, 1994] and [Williams, 1993] bounds are derived for the
maximum reduction in optimality that can be produced by a given error in function
approximation. If a basis function approximator is used, then the reduction can be
large [Sabes, 1993]. These results assume generalization from a dataset containing
true optimal values; the true reinforcement learning scenario is even harder because
each iteration of DP requires its own function approximation.

3

THE GROW-SUPPORT ALGORITHM

The Grow-Support algorithm is designed to construct the optimal value function with
a generalizing function approximator while being robust and stable. It recognizes that
function approximators cannot always be relied upon to fit the intermediate value
functions produced by DP. Instead, it assumes only that the function approximator
can represent the final J* function accurately. The specific principles of Grow-Support
are these:
1. We maintain a ""support"" set of states whose final J* values have been computed, starting with goal states, and growing this set out from the goal. The
fitter is trained only on these values, which we assume it is capable of fitting.
2. Instead of propagating values by one-step DP backups, we use simulations
with the current greedy policy, called ""rollouts"". They explicitly verify the
achievability of a state's cost-to-go estimate before adding that state to the

374

Justin Boyan, Andrew W. Moore

support. In a rollout, the J values are derived from costs of actual paths to the
goal, not from the values of the previous iteration's function approximation.
This prevents divergence .
3. We take maximum advantage of generalization. Each iteration, we add to
the support set any sample state which can, by executing a single action,
reach a state that passes the rollout test. In a discrete environment, this
would cause the support set to expand in one-step concentric ""shells"" back
from the goal. But in our continuous case, the function approximator may
be able to extrapolate correctly well beyond the support region-and when
this happens, we can add many points to the support set at once. This leads
to the very desirable behavior that the support set grows in big jumps in
regions where the value function is smooth.
Iteration 1,

I Support I =4

Iteration 2,

1Support 1=12

Iteration 3,

ISupportl=256

Figure 7: Grow-Support with quadratic regression on the gridworld. (Compare Figure 3.)
Iteration 1,

I Support I =3

Iteration 2,

ISupportl=213

Iteration 5,

ISupportl=253

Figure 8: Grow-Support with LWR on the two-puddle gridworld. (Compare Figure 4.)
Iteration 3,

I Support I =79

Iteration 8,

ISupportl=134

Iteration 14,

ISupportl=206

3

O.

2

O.

-2

o.

Figure 9: Grow-Support with backprop on car-on-the-hill. (Compare Figure 6.)

The algorithm, again restricted to the deterministic case for simplicity, is outlined in
the appendix. In Figures 7-9, we illustrate its convergence on the same combinations
of domain and function approximator which caused smooth value iteration to diverge.
In Figure 8, all but three points are added to the support within only five iterations,

Generalization in Reinforcement Learning: Safely Approximating the Value Function

375

and the resulting greedy policy is optimal. In Figure 9, after 14 iterations, the algorithm terminates. Although 50 states near the discontinuity were not added to the
support set, the resulting policy is optimal within the support set. Grow-support
converged to a near-optimal policy for all the problems and fitters in Table 1.
The Grow-Support algorithm is more robust than value iteration. Empirically, it was
also seen to be no more computationally expensive (and often much cheaper) despite
the overhead of performing rollouts. Reasons for this are (1) the rollout test is not
expensive; (2) once a state has been added to the support, its value is fixed and it
needs no more computation; and most importantly, (3) the aggressive exploitation
of generalization enables the algorithm to converge in very few iterations. However,
with a nondeterministic problem, where multiple rollouts are required to assess the
accuracy of a prediction, Grow-Support would become more expensive.
It is easy to prove that Grow-Support will always terminate after a finite number
of iterations. If the function approximator is inadequate for representing the J*
function, Grow-Support may terminate before adding all sample states to the support
set. When this happens, we then know exactly which of the sample states are having
trouble and which have been learned. This suggests potential schemes for adaptively
adding sample states to the support in problematic regions. Investigation of these
ideas is in progress.

In conclusion, we have demonstrated that dynamic programming methods may diverge when their tables are replaced by generalizing function approximators. Our
Grow-Support algorithm uses rollouts, rather than one-step backups, to assign training values and to keep inaccurate states out of the training set. We believe these
principles will contribute substantially to producing practical, robust, reinforcement
learning.
Acknowledgements
We thank Scott Fahlman, Geoff Gordon, Mary Lee, Michael Littman and Marc Ringuette for
their suggestions, and the NDSEG fellowship and NSF Grant IRI-9214873 for their support.

APPENDIX: ALGORITHMS
Smooth Value Iteration(X, G, A, NEXT-STATE, COST, FITJ):
Given: _ a finite collection of states X = {Xl, X2, .. . XN} sampled from the
continuous state space X C fR n , and goal region G C X
_ a finite set of allowable actions A
_ a deterministic transition function NEXT-STATE: X x A -+ X
_ the I-step cost function COST: X x A -+ fR
_ a smoothing function approximator FIT J
iter := 0
]<0) [i] := 0 Vi = 1 ... N
{X I t-+ J?(iter) [1] }
repeat
!rain ~ITJ(iter) to approximate the training set:
:
Iter .:= Iter + 1;
XN t-+ /iter)[N]
for ~ := 1 ... N do
.(iter) [.] ._ { 0
.
J
1.minaEA (COST(Xi,a) + FITJ(lter-I)(NEXT-STATE(xi,a)))
until j array stops changing

if Xi E G
otherwise

376

Justin Boyan, Andrew W. Moore

subroutine RoIloutCost(x, J):
Starting from state x , follow the greedy policy defined by value function J until
either reaching the goal, or exceeding a total path cost of J(x) + ?. Then return:
--t the actual total cost of the path, if goal is reached from x with cost ~ J(x) + e
--t 00, if goal is not reached in cost J(x) + ?.
Grow-Support(X,G,A, NEXT-STATE, COST, FITJ):
Given: ? exactly the same inputs as Smooth Value Iteration.
SUPPORT := {(Xi t-+ 0) I Xi E G}
repeat
Train FIT J to approximate the training set SUPPORT
for each Xi ~ SUPPORT do
c := minaEA [COsT(xi,a) + RolloutCost(NEXT-STATE(Xi, a), FITJ)]
if c < 00 then
add (Xi t-+ c) to the training set SUPPORT
until SUPPORT stops growing or includes all sample points.

References
[Barto et al., 1989] A . Barto, R. Sutton , and C . Watkins . Learning and sequential decision making. Technical Report COINS 89- 95, Univ. of Massachusetts, 1989 .
[Bellman et al., 1963] R . Bellman, R . Kalaba, and B . Kotkin. Polynomial approximation-a new computational technique in dynamic programming: Allocation processes . Mathematics of Computation, 17,
1963.
[Boyan , 1992] J. A . Boyan. Modular neural networks for learning context-dependent game strategies .
Master's thesis, Cambridge University, 1992.
[Bradtke, 1993] S. J. Bradtke. Reinforcement learning applied to linear quadratic regulation. In S. J .
Hanson, J . Cowan, and C . L . Giles, editors, NIPS-5. Morgan Kaufmann , 1993.
[Cleveland and Delvin, 1988] W . S. Cleveland and S. J. Delvin. Locally weighted regression : An approach
to regression analysis by local fitting. JASA , 83(403):596-610, September 1988.
[Lin, 1993] L.-J . Lin . Reinforcement Learning for Robots Using Neural Networks. PhD thesis, Carnegie
Mellon University, 1993.
[Mahadevan and Connell , 1990] S. Mahadevan and J. Connell . Automatic programming of behavior-based
robots using reinforcement learning. Technical report, IBM T. J . Watson Research Center, NY 10598,
1990 .
[Sabes, 1993] P. Sabes . Approximating Q-values with basis function represent ations. In Proceedings of
the Fourth C onnectionist Models Summer School, 1993.
[Schraudolph et al., 1994] N . Schraudolph, P . Dayan, and T. Sejnowski . Using TD(>.) to learn an evaluation function for the game of Go . In J. D. Cowan, G . Tesauro , and J . Alspector, editors, NIPS-6.
Morgan Kaufmann, 1994.
[Singh and Yee, 1994] S. P. Singh and R. Yee. An upper bound on the loss from approximate optimal-value
functions . Machine Learning, 1994. Technical Note (to appear) .
[Sutton, 1988] R . Sutton . Learning to predict by the methods of temporal differences. Machine Learning,
3,1988.
[Tesauro, 1992] G. Tesauro. Practical issues in temporal difference learning. Machine Learning, 8(3/4),
May 1992.
[Thrun and Schwartz, 1993] S. Thrun and A . Schwartz. Issues in using function approximation for reinforcement learning. In Proceedings of the Fourth Connectionist Models Summer School, 1993.
[Watkins, 1989] C . Watkins. Learning from Delayed Rewards. PhD thesis, Cambridge University, 1989 .
[Williams, 1993] R. Williams . Tight performance bounds on greedy policies based on imperfect value
functions . Technical Report NU-CCS-93-13, Northeastern University, 1993.
[Yee, 1992] R . Yee.
sachusetts, 1992.

Abstraction in control learning.

Technical Report COINS 92-16 , Univ. of Mas-

"
1019,1994,A Mixture Model System for Medical and Machine Diagnosis,,1019-a-mixture-model-system-for-medical-and-machine-diagnosis.pdf,Abstract Missing,"A Mixture Model System for Medical and
Machine Diagnosis

Terrence J. Sejnowski

Magnus Stensmo

Computational Neurobiology Laboratory
The Salk Institute for Biological Studies
10010 North Torrey Pines Road
La Jolla, CA 92037, U.S.A.
{magnus,terry}~salk.edu

Abstract
Diagnosis of human disease or machine fault is a missing data problem
since many variables are initially unknown. Additional information needs
to be obtained. The j oint probability distribution of the data can be used to
solve this problem. We model this with mixture models whose parameters
are estimated by the EM algorithm. This gives the benefit that missing
data in the database itself can also be handled correctly. The request for
new information to refine the diagnosis is performed using the maximum
utility principle. Since the system is based on learning it is domain
independent and less labor intensive than expert systems or probabilistic
networks. An example using a heart disease database is presented.

1 INTRODUCTION
Diagnosis is the process of identifying diseases in patients or disorders in machines by
considering history, symptoms and other signs through examination. Diagnosis is a common
and important problem that has proven hard to automate and formalize. A procedural
description is often hard to attain since experts do not know exactly how they solve a
problem.
In this paper we use the information about a specific problem that exists in a database

1078

Magnus Stensmo. Terrence J. Sejnowski

of cases. The disorders or diseases are determined by variables from observations and
the goal is to find the probability distribution over the disorders, conditioned on what has
been observed. The diagnosis is strong when one or a few of the possible outcomes are
differentiated from the others. More information is needed if it is inconclusive. Initially
there are only a few clues and the rest of the variables are unknown. Additional information
is obtained by asking questions and doing tests. Since tests may be dangerous, time
consuming and expensive, it is generally not possible or desirable to find the answer to
every question. Unnecessary tests should be avoided .
. There have been many attempts to automate diagnosis. Early work [Ledley & Lusted, 1959]
realized that the problem is not always tractable due to the large number of influences that
can exist between symptoms and diseases. Expert systems, e.g. the INTERNIST system
for internal medicine [Miller et al., 1982], have rule-bases which are very hard and time
consuming to build. Inconsistencies may arise when new rules are added to an existing
database. There is also a strong domain dependence so knowledge bases can rarely be
reused for new applications.
Bayesian or probabilistic networks [Pearl, 1988] are a way to model a joint probability
distribution by factoring using the chain rule in probability theory. Although the models
are very powerful when built, there are presently no general learning methods for their
construction. A considerable effort is needed. In the Pathfinder system for lymph node
pathology [Heckerman et al., 1992] about 14,000 conditional probabilities had to be assessed
by an expert pathologist. It is inevitable that errors will occur when such large numbers of
manual assessments are involved.
Approaches to diagnosis that are based on domain-independent machine learning alleviate
some of the problems with knowledge engineering. For decision trees [Quinlan, 1986], a
piece of information can only be used if the appropriate question comes up when traversing
the tree. This means that irrelevant questions can not be avoided. Feedforward multilayer
perceptrons for diagnosis [Baxt, 1990] can classify very well, but they need full information
about a case. None of these these methods have adequate ways to handle missing data during
learning or classification.
The exponentially growing number of probabilities involved can make exact diagnosis
intractable. Simple approximations such as independence between all variables and conditional independence given the disease (naive Bayes) introduce errors since there usually are
dependencies between the symptoms. Even though systems based on these assumptions
work surprisingly well, correct diagnosis is not guaranteed. This paper will avoid these
assumptions by using mixture models.

2 MIXTURE MODELS
Diagnosis can be formulated as a probability estimation problem with missing inputs. The
probabilities of the disorders are conditioned on what has currently been observed. If we
model the joint probability distribution it is easy to marginalize to get any conditional
probability. This is necessary in order to be able to handle missing data in a principled
way [Ahmad & Tresp, 1993]. Using mixture models [McLachlan & Basford, 1988], a
simple closed form solution to optimal regression with missing data can be formulated. The
EM algorithm, a method from parametric statistics for parameter estimation, is especially
interesting in this context since it can also be formulated to handle missing data in the

A Mixture Model System for Medical and Machine Diagnosis

1079

training examples [Dempster et al., 1977; Ghahramani & Jordan, 1994].

2.1 THE EM ALGORITHM
The data underlying the model is assumed to be a set of N D-dimensional vectors X =
{ Z I, . . . , Z N }. Each data point is assumed to have been generated independently from a
mixture density with M components
M

p(z)

M

= LP(z,Wj;Oj) = LP(Wj)p(zlwj;Oj),

(1)
j=l
j=l
where each mixture component is denoted by Wj. p(Wj), the a priori probability for
mixturewj, and 8 = (0 1 , ... , OM) are the model parameters.
To estimate the parameters for the different mixtures so that it is likely that the linear combination of them generated the set of data points, we use maximum likelihood estimation. A
good method is the iterative Expectation-Maximization, or EM, algorithm [Dempster et al.,
1977].
Two steps are repeated. First a likelihood is formulated and its expectation is computed in
the E-step. For the type of models that we will use, this step will calculate the probability
that a certain mixture component generated the data point in question. The second step
is the M-step where the parameters that maximize the expectation are found. This can be
found analytically for models that can be written in an exponential form, e.g. Gaussian
functions. Equations can be derived for both batch and on-line learning. Update equations
for Gaussian distributions with and without missing data will be given here, other distributions are possible, e.g. binomial or multinomial [Stensmo & Sejnowski, 1994]. Details and
derivations can be found in [Dempster et al., 1977; Nowlan, 1991; Ghahramani & Jordan,
1994; Stensmo & Sejnowski, 1994].
From (1) we form the log likelihood of the data
N

N

M

L(8IX) = L logp(zi; OJ) = L log LP(Wj )P(Zi IWj; OJ).
j=l
There is unfortunately no analytic solution to the logarithm of the sum in the right hand side
of the equation. However, if we were to know which of the mixtures generated which data
point we could compute it. The EM algorithm solves this by introducing a set of binary
indicator variables Z = {Zij}. Zij = 1 if and only if the data point Zi was generated by
mixture component j. The log likelihood can then be manipulated to a form that does not
contain the log of a sum.
The expectation of %i using the current parameter values 8 k is used since %i is not known
directly. This is the E-step of the EM algorithm. The expected value is then maximized in
theM-step. The two steps are iterated until convergence. The likelihood will never decrease
after an iteration [Dempster et al., 1977]. Convergence is fast compared to gradient descent.
One of the main motivations for the EM-algorithm was to be able to handle missing values
for variables in a data set in a principled way. In the complete data case we introduced
missing indicator variables that helped us solve the problem. With missing data we add
the missing components to the Z already missing [Dempster et aI., 1977; Ghahramani &
Jordan, 1994].

1080

Magnus Stensmo, Terrence J. Sejnowski

2.2 GAUSSIAN MIXTURES
We specialize here the EM algorithm to the case where the mixture components are radial
Gaussian distributions. For mixture component j with mean I-'j and covariance matrix 1:j
this is

The form of the covariance matrix is often constrained to be diagonal or to have the
same values on the diagonal, 1:j = o} I. This corresponds to axis-parallel oval-shaped
and radially symmetric Gaussians, respectively. Radial and diagonal basis functions can
function well in applications [Nowlan, 1991], since several Gaussians together can form
complex shapes in the space. With fewer parameters over-fitting is minimized. In the radial
case, with variance o}

In the E-step the expected value of the likelihood is computed. For the Gaussian case this
becomes the probability that Gaussian j generated the data point

Pj(Z)

=

!(Wj)Gj(z)
.
l:k=l P(Wk)Gk(Z)

The M-step finds the parameters that maximize the likelihood from the E-step. For complete
data the new estimates are

(2)
N

where Sj = I:Pj(Zi).
i=l

When input variables are missing the Gj(z) is only evaluated over the set of observed
dimensions O. Missing (unobserved) dimensions are denoted by U. The update equation
= itY and use (2). The variance
for p(Wj) is unchanged. To estimate itj we set
becomes

zf

A least squares regression was used to fill in missing data values during classification. For
missing variables and Gaussian mixtures this becomes the same approach used by [Ahmad &
Tresp, 1993]. The result of the regression when the outcome variables are missing is a
probability distribution over the disorders. This can be reduced to a classification for
comparison with other systems by picking the outcome with the maximum of the estimated
probabilities.

A Mixture Model System for Medical and Machine Diagnosis

3

1081

REQUESTING MORE INFORMATION

During the diagnosis process, the outcome probabilities are refined at each step based on
newly acquired knowledge. It it important to select the questions that lead to the minimal
number of necessary tests. There is generally a cost associated with each test and the goal
is to minimize the total cost. Early work on automated diagnosis [Ledley & Lusted, 1959]
acknowledged the problem of asking as few questions as possible and suggested the use of
decision analysis for the solution. An important idea from the field of decision theory is the
maximum expected utility principle [von Neuman & Morgenstern, 1947]: A decision maker
should always choose the alternative that maximizes some expected utility of the decision .
For diagnosis it is the cost of misclassification . Each pair of outcomes has a utility u(x, y)
when the correct diagnosis is x but y has been incorrectly determined. The expectation can
be computed when we know the probabilities of the outcomes.
The utility values have to be assessed manually in what can be a lengthy and complicated
process. For this reason a simplification of this function has been suggested by [Heckerman et al., 1992]: The utility u(x, y) is 1 when both x and y are benign or both are malign,
and 0 otherwise. This simplification has been found to work well in practice. Another
complication with maximum expected utility principle can also make it intractable. In the
ideal case we would evaluate every possible sequence of future choices to see which is
the best. Since the size of the search tree of possibilities grows exponentially this is often
not possible. A simplification is to 100k ahead only one or a few steps at a time. This
nearsighted or myopic approach has been tested in practice with good results [Gorry &
Barnett, 1967; Heckerman et al., 1992] .

4

THE DIAGNOSIS SYSTEM

The system we have developed has two phases. First there is a learning phase where a
probabilistic model is built. This model is then used for inference in the diagnosis phase.
In the learning phase, the joint probability distribution of the data is modeled using mixture
models. Parameters are determined from a database of cases by the EM algorithm. The
k-means algorithm is used for initialization. Input and output variables for each case are
combined into one vector per case to form the set of training patterns. The outcomes and
other nominal variables are coded as J of N . Continuous variables are interval coded.
In the diagnosis phase, myopic one-step look-ahead was used and utilities were simplified
as above. The following steps were performed:
1. Initial observations were entered.
2. Conditional expectation regression was used to fill in unknown variables.
3. The maximum expected utility principle was used to recommend the next observation to make. Stop if nothing would be gained by further observations.
4. The user was asked to determine the correct value for the recommended observation. Any other observations could be made, instead of or in addition to this.
5. Continue with step 2.

Magnus Stensmo, Terrence J. Sejnowski

1082

Table 1: The Cleveland Heart Disease database.
1
2
3
4
5
6
7
8
9
10

Observation
age
sex
cp
trestbps
chol
fbs
restecg
thalach
exang
oldpeak

11

slope

12
13

ca
thaI
Disorder
num

14

Description
Age in years
Sex of subject
Chest pain
Resting blood pressure
Serum cholesterol
Fasting blood sugar
Resting electrocardiogr.
Max heart rate achieved
Exercise induced angina
ST depr. induced by
exercise relative to rest
Slope of peak exercise
STsegment
# major vess. col. ftourosc.
Defect type
Description
Heart disease

Values
continuous
male/female
four types
continuous
continuous
It or gt 120 mg/dl
five values
continuous
yes/no
continuous
up/flat/down
0-3
normal/fixed/reversible
Values
Not present/4 types

5 EXAMPLE
The Cleveland heart disease data set from UC, Irvine has been used to test the system. It
contains 303 examples of four types of heart disease and its absence. There are thirteen
continuous- or nominally-valued variables (Table 1). The continuous variables were interval
coded with one unit per standard deviation away from the mean value. This was chosen since
they were approximately normally distributed. Nominal variables were coded with one unit
per value. In total the 14 variables were coded with 55 units. The EM steps were repeated
until convergence (60-150 iterations). A varying number of mixture components (20-120)
were tried.
Previously reported results have used only presence or absence of the heart disease. The
best of these has been a classification rate of 78.9% using a system that incrementally
built prototypes [Gennari et al., 1989]. We have obtained 78.6% correct classification
with 60 radial Gaussian mixtures as described above. Performance increased with the
number of mixture components. It was not sensitive to a varying number of mixture
components during training unless there were too few of them. Previous investigators have
pointed out that there is not enough information in the thirteen variables in this data set to
reach 100% [Gennari et al., 1989].
An annotated transcript of a diagnosis session is shown in Figure 1.

6 CONCLUSIONS AND FURTHER WORK
Several properties of this model remain to be investigated. It should be tested on several
more databases. Unfortunately databases are typically proprietary and difficult to obtain.
Future prospects for medical databases should be good since some hospitals are now using
computerized record systems instead of traditional paper-based. It should be fairly easy to

A Mixture Model System for Medical and Machine Diagnosis

1083

The leftmost number of the five numbers in a line is the estimated probability for no heart
disease, followed by the probabilities for the four types of heart disease. The entropy, defined
as Pi log Pi' of the diagnoses are given at the same time as a measure of how decisive
the current conclusion is. A completely detennined diagnosis has entropy O. Initially all of
the variables are unknown and starting diagnoses are the unconditional prior probabilities.

l:.

Disorders (entropy = 1.85):
0.541254 0.181518 0.118812
What is cp ? 3

0.115512

0.042904

The first question is chest pain, and the answer changes the estimated probabilities. This
variable is continuous. The answer is to be interpreted how far from the mean the observation
is in standard deviations. As the decision becomes more conclusive, the entropy decreases.

Disorders (entropy = 0.69):
0.888209 0.060963 0.017322
What is age ? 0

0.021657

0.011848

Disorders (entropy = 0.57):
0.91307619 0.00081289 0.02495360
What is oldpeak ? -2

0.03832095

0.02283637

Disorders (entropy = 0.38):
0.94438718 0.00089016 0.02539957
What is chol? -1

0.02691099

0.00241210

0.00507073

0 . 00294036

Disorders (entropy = 0.11):
0.98848758 0.00028553 0.00321580

We have now detennined that the probability of no heart disease in this case is 98.8%. The
remaining 0.2% is spread out over the other possibilities.

Figure 1: Diagnosis example.

generate data for machine diagnosis.
An alternative way to choose a new question is to evaluate the variance change in the output
variables when a variable is changed from missing to observed. The idea is that a variable
known with certainty has zero variance. The variable with the largest resulting conditional
variance could be selected as the query, similar to [Cohn et aI., 1995].
One important aspect of automated diagnosis is the accompanying explanation for the
conclusion, a factor that is important for user acceptance. Since the basis functions have
local support and since we have estimates for the probability of each basis function having
generated the observed data, explanations for the conclusions could be generated.
Instead of using the simplified utilities with values 0 and 1 for the expected utility calculations they could be learned by reinforcement learning. A trained expert would evaluate the
quality of the diagnosis performed by the system, followed by adjustment of the utilities.
The 0 and 1 values can be used as starting values.

1084

Magnus Stensmo. Terrence J. Sejnowski

Acknowledgements
The heart disease database is from the University of California, Irvine Repository of
Machine Learning Databases and originates from R. Detrano, Cleveland Clinic Foundation.
Peter Dayan provided helpful comments on an earlier version of this paper.

References
Ahmad, S. & Tresp, V. (1993). Some solutions to the missing feature problem in vision.
In Advances in Neural Information Processing Systems, vol. 5, pp 393-400. Morgan
Kaufmann, San Mateo, CA.
Baxt, W. (1990). Use of an artificial neural network for data analysis in clinical decisionmaking: The diagnosis of acute coronary occlusion. Neural Computation, 2(4),480489.
Cohn, D. A., Ghahramani, Z. & Jordan, M.1. (1995). Active learning with statistical models.
In Advances in Neural Information Processing Systems, vol. 7. Morgan Kaufmann, San
Mateo, CA.
Dempster, A., Laird, N. & Rubin, D. (1977). Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical Society, Series, B., 39, 1-38.
Gennari, 1, Langley, P. & Fisher, D. (1989). Models of incremental concept formation.
Artificial Intelligence, 40, 11-62.
Ghahramani, Z. & Jordan, M. (1994). Supervised learning from incomplete data via an EM
approach. In Advances in Neural Information Processing Systems, vol. 6, pp 120-127.
Morgan Kaufmann, San Mateo, CA.
Gorry, G. A. & Barnett, G. O. (1967). Experience with a model of sequential diagnosis.
Computers and Biomedical Research, 1, 490-507.
Heckerman, D., Horvitz, E. & Nathwani, B. (1992). Toward normative expert systems:
Part I. The Pathfinder project. Methods of Information in Medicine, 31, 90-105.
Ledley, R. S. & Lusted, L. B. (1959). Reasoning foundations of medical diagnosis. Science,
130(3366),9-21.
McLachlan, G. J. & Basford, K. E. (1988). Mixture Models: Inference and Applications to
Clustering. Marcel Dekker, Inc., New York, NY.
Miller, R. A., Pople, H. E. & Myers, 1 D. (1982). Internist-I: An experimental computerbased diagnostic consultant for general internal medicine. New England Journal of
Medicine, 307, 468-476.
Nowlan, S. J. (1991). Soft Competitive Adaptation: Neural Network Learning Algorithms
based on Fitting Statistical Mixtures. PhD thesis, School of Computer Science, Carnegie
Mellon University, Pittsburgh, PA.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference. Morgan Kaufmann, San Mateo, CA.
Quinlan, 1 R. (1986). Induction of decision trees. Machine Learning, 1,81-106.
Stensmo, M . & Sejnowski, T. J. (1994). A mixture model diagnosis system. Tech. Rep.
INC-9401, Institute for Neural Computation, University of California, San Diego.
von Neuman, J. & Morgenstern, O. (1947). Theory of Games and Economic Behavior.
Princeton University Press, Princeton, NJ.

"
102,1988,An Application of the Principle of Maximum Information Preservation to Linear Systems,,102-an-application-of-the-principle-of-maximum-information-preservation-to-linear-systems.pdf,Abstract Missing,"186

AN APPLICATION OF THE PRINCIPLE OF
MAXIMUM INFORMATION PRESERVATION
TO LINEAR SYSTEMS
Ralph Linsker
IBM T. J. Watson Research Center, Yorktown Heights, NY 10598

ABSTRACT
This paper addresses the problem of determining the weights for a
set of linear filters (model ""cells"") so as to maximize the
ensemble-averaged information that the cells' output values jointly
convey about their input values, given the statistical properties of
the ensemble of input vectors. The quantity that is maximized is the
Shannon information rate, or equivalently the average mutual
information between input and output. Several models for the role
of processing noise are analyzed, and the biological motivation for
considering them is described. For simple models in which nearby
input signal values (in space or time) are correlated, the cells
resulting from this optimization process include center-surround
cells and cells sensitive to temporal variations in input signal.

INTRODUCTION
I have previously proposed [Linsker, 1987, 1988] a principle of ""maximum

information preservation,"" also called the ""infomax"" principle, that may account for
certain aspects of the organization of a layered perceptual network. The principle
applies to a layer L of cells (which may be the input layer or an intermediate layer
of the network) that provides input to a next layer M. The mapping of the input
signal vector L onto an output signal vector M, f:L ~ M, is characterized by a
conditional probability density function (""pdf"") p(MI L). The set S of allowed
mappings I is specified. The input pdf PL(L) is also given. (In the cases considered
here, there is no feedback from M to L.) The infomax principle states that a
mapping I should be chosen for which the Shannon information rate [Shannon,
1949]
R(j) ==

f dL PL(L) f dM p(MI L) 10g[P(MI L)/PM(M)]

(1)

is a maximum (over allIin the set S). Here PM(M) == fdLPL(L)P(MIL) is the pdf
of the output signal vector M. R is identical to the average mutual information
between Land M.

187

Maximum Infonnation Preservation to Linear Systems

To understand better how the info max principle may be applied to biological systems
and complex synthetic networks, it is useful to solve the infomax optimization
problem explicitly for simpler systems whose properties are nonetheless biologically
motivated. This paper therefore deals with the practical computation of infomax
solutions for cases in which the mappings! are constrained to be linear.

INFOMAX SOLUTIONS FOR A SET OF LINEAR FILTERS
We consider the case of linear model ""neurons"" with multivariate Gaussian input
and additive Gaussian noise. There are N input (L) cells and N' output (M) cells.
The input column vector L = (Lt,~, ... ,LNF is randomly selected from an
N-dimensional Gaussian distribution having mean zero. That is,
(2)

where QL is the covariance matrix of the input activities, Q6
(Superscript T denotes the matrix transpose.)

= JdL PL(L)LjL

j

?

To specify the set S of allowed mappings !:L .... M, we define a processing model
that includes a description of (i) how noise enters during processing, (ii) the
independent variables over which we are to maximize R, and (iii) any constraints
on their values. Figure 1 shows several such models. We shall analyze the simplest,
then explain the motivation for the more complex models and analyze them in turn.
Model A -- Additive noise of constant variance

In Model A of Fig. 1 the output signal value of the nth M cell is:
(3)

The noise components ""11 are independently and identically distributed (fli.i.d. "")
random variables drawn from a Gaussian distribution having a mean of zero and
variance B.
Each mapping !:L .... M is characterized by the values of the {Cnj } and the noise
parameter B. The elements of the covariance matrix of the output activities are
(using Eqn. 3)
(4)

where

~nm

= 1 if n =

m and 0 otherwise.

Evaluating Eqn. 1 for this processing model gives the information rate:
R(j) = (1/2) In Det W(j)

(5)

where ~m = Q:!'/ B. (R is the difference of two entropy terms. See [Shannon,
1949], p.57, for the entropy of a Gaussian distribution.)

188

Linsker

If the components Cni of the C matrix are allowed to be arbitrarily large, then the

information rate can be made arbitrarily large, and the effects of noise become
arbitrarily small. One way to limit C is to impose a ""resource constraint"" on each
M cell. An example of such a constraint is ~jqj = 1 for all n. One can then attempt
directly, using numerical methods, to maximize Eqn. 5 over all allowed C for given
B. However, when some additional conditions (below) are satisfied, further
analytical progress can be made.
Suppose the NL-cells are uniformly spaced along the line interval [0,1] with periodic
boundary conditions, so that cell N is next to cell 1. [The analysis can be extended
to a two- (or higher-) dimensional array in a straightforward manner.] Suppose also
that (for given N) the covariance Q6 of the input values at cells i and j is a function
QL(Sj) only of the displacement s'J from i to j. (We deal with the periodicity by
defining Sab = b - a - Ya~ and choosing the integer Yab such that
-N/2 S Sab < N/2.) Then QL is a Toeplitz matrix, and its eigenvalues {Ak} are the
components of the discrete Fourier transform (""F.T."") of QL(S):
Ak = ~sQL(s) exp( -2~ks/N), (-N/2) S k

< N/2.

(6)

We now impose two more conditions: (1) N' = N. This simplifies the resulting
expressions, but is otherwise inessential, as we shall discuss. (2) We constrain each
M cell to have the same arrangement of C-values relative to the M cell's position.
That is, Cnj is to be a function C(Sni) only of the displacement Sni from n to i. This
constraint substantially reduces the computational demands. We would not expect

(S,C)

L?I

L?I

Figure 1.

(D)

Four processing models (A)-(D): Each diagram shows a single
M cell (indexed by n) having output activity Mn. Inputs {LJ may
be common to many M cells. All noise contributions (dotted
lines) are uncorrelated with one another and with {LJ. GC =
gain control (see text).

Maximum Information Preservation to Linear Systems

it to hold in general in a biologically realistic model -- since different M cells should
be allowed to develop different arrangements of weights -- although even then it
could be used as an Ansatz to provide a lower bound on R. The section,
""Temporally-correlated input patterns,"" deals with a situation in which it is
biologically plausible to impose this constraint.
Under these conditions, (Q:!') is also a Toeplitz matrix. Its eigenvalues are the
components of the F.T. of QM(snm). For N' = N these eigenvalues are (B + A~k) ,
where Zk = ICkl2 and Ck == ~sC(s) exp( -2'TT~ks/N) is the F.T. of C(s). [This
expression for the eigenvalues is obtained by rewriting Eqn. 4 as:
QM(snm) = B8n_m.o + ~j.jC(snJQL(Sj)C(sm) ,and taking the F.T. of both sides.]
Therefore
R = (1/2)~k In[l

+ AJcZk/ B].

(7)

We want to maximize R subject to ~sC(S)2 = 1, which is equivalent to ~Zk = N .
Using the Lagrange multiplier method, we maximize A == R + 11-(~Zk - N) over all
nonnegative {Zk}' Solving for (JA/ (JZk = 0 and requiring Zk ~ 0 for all k gives the
solution:
Zk

= max[( -1/211-)

(8)

- (B/Ak)' 0],

where (given B) 11- is chosen such that

~Zk =

N.

Note that while the optimal {Zk} are uniquely determined, the phases of the {ck} are
completely arbitrary [except that since the {C(s)} are real, we must have Ck * = c_ k
for all k]. The {C(s)} values are therefore not uniquely determined. Fig. 2a shows
two of the solutions for .an example in which QL(S) = exp[ - (s/ So)2] with So = 6,
N=N'=64, and B.:..:.l. Both solutions have ZO.?1..... ?6=5.417, 5.409, 5.378,
5.306, 5.134,4.689,3.376, and all other Zk == O. Setting all Ck phases to zero yields
the solid curve; a particular random choice of phases yields the dotted dHve. We
shall later see that imposing locality conditions on the {C(s)} (e.g., penalizing
nonzero C(s) for large Is I) can remove the phase ambiguity.
Our solution (Eqn. 8) can be described in terms of a so-called ""water-filling""
analogy: If one plots B /Ak versus k, then Zk is the depth of ""water"" at k when one
""pours"" into the ""vessel"" defined by the B / Ak curve a total quantity of ""water"" that
corresponds to ~Zk = N and brings the ""water level"" to ( -1/211-).
Let us contrast this problem with two other problems to which the ""water-filling""
analogy has been applied in the information-theory literature. In our notation, they
are:
1.

Given a transfer function {C(s)} and the noise variance B, how should a given
total input signal power ~Ak be apportioned among the various wavenumbers
k so as to maximize the information rate R [Gallager, 1968]? Our problem is
complementary to this: we fix the input signal properties and seek an optimal
transfer function subject to constraints.

189

190

Linsker

2.

Rate-distortion (R-D) calculation [Berger, 1971]: Given a distortion measure
(that defines a ""distance"" between the actual input signal and an estimate of it
that can be reconstructed from the channel's output), and the input power
spectrum p. k}, what choice of {Zk} minimizes the average distortion for given
information rate (or minimizes the required rate for given distortion)? In the
R-D problem there is a process of reconstruction, and a given measure for
assessing the ""goodness"" of reconstruction. In contrast, in our network there
is no reconstruction of the input signal, and no criterion of the ""goodness"" of
such a hypothetical reconstruction is provided.

Note also that infomax optimization is not the same as computing which channel
(that is, which mapping !:L .... M) selected from an allowed set has the maximum
information-theoretic capacity. In that problem, one is free to encode the inputs
before transmission so as to make optimal use of (Le., ""achieve the capacity of"") the
channel. In our case, there is no such pre-encoding; the input ensemble is prescribed
(by the environment or by the output of an earlier processing stage) and we need to
maximize the channel rate for that ensemble.
The simplifying condition that N = N' (above) is unnecessarily restrictive. Eqn. 7
can be easily generalized to the case in which N is a mUltiple of N' and the N' M cells
are uniformly spaced on the unit interval. Moreover, in the limit that 1/N' is much
smaller than the correlation length scale of QL, it can be shown that R is unchanged
when we simultaneously increase N' and B by the same factor. (For example, two
adjacent M cells each having noise variance 2B jointly convey the same information

c

c

c
(0)

(b)
l

.'..- s
-10
\

,:
\,/

Figure 2.

Example infomax solutions C(s) for locally-correlated
inputs: (a) Model A; region of nonnegligible C(s) extends over
all s; phase ambiguity in Ck yields non unique C(s) solutions, two
of which are shown. See text for details. (b) Models C (solid
curve) and D (dotted curve) with Gaussian g(S)-l favoring short
connections; shows center-surround receptive fields, more
pronounced in Model D. (c) ""Temporal receptive field"" using
Model D for temporally correlated scalar input to a single M cell;
C(s) is the weight applied to the input signal that occurred s time
steps ago. Spacing between ordinate marks is 0.1; ~ C(S)2 = 1 in
each case.

Maximum Information Preservation to Linear Systems

about L as one M cell having noise variance B.) For biological applications we are
mainly interested in cases in which there are many L cells [so that C(s) can be
treated as a function of a continuous variable] and many M cells (so that the effect
of the noise process is described by the single parameter B/ N).
The analysis so far shows two limitations of Model A. First, the constraint
~iqi = 1 is quite arbitrary. (It certainly does not appear to be a biologically natural
constraint to impose!) Second, for biological applications we are interested in
predicting the favored values of {C(s)}, but the phase ambiguity prevents this. In
the next section we show that a modified noise model leads naturally, without
arbitrary constraints on ~iqi' to the same results derived above. We then turn to a
model that favors local connections over long-range ones, and that resolves the
phase ambiguity issue.
Model B -- Independent noise on each input line

In Model B of Fig. 1 each input Li to the nth M cell is corrupted by Li.d. Gaussian
noise V l1i of mean zero and variance B. The output is
(9)

Since each V ni is independent of all other noise terms (and of the inputs {Li }), we find
(10)
We may rewrite the last term as
then R = (1/2) In DetWwhere

B~l1m (~iqy!2 (~jc;)l/2.

The information rate is
(11)

Define c' ni == Cl1i(~kqk)-1/2 ; then J?,.m = ~lIm + (~,.jc'lIiQbC' mj)/ B. Note that this is
identical (except for the replacement C ~ C') to the expression following Eqn. (5),
in which QM was given by Eqn. (4). By definition, the {C' nil satisfy ~iC';i = 1 for
all n. Therefore, the problem of maximizing R for this model (with no constraints
on ~jq;) is identical to the problem we solved in the previous section.
Model C -- Favoring of local connections

Since the arborizations of biological cells tend to be spatially localized in many cases,
we are led to consider constraints or cost terms that favor localization. There are
various ways to implement this. Here we present a way of modifying the noise
process so that the infomax principle itself favors localized solutions, without
requiring additional terms unrelated to information transmission.
Model C of Fig. 1 is the same as Model B, except that now the longer connections
are ""noisier"" than the shorter ones. That is, the variance of VIIi is <V;i> = B~(sn;)
where g(s) increases with 1s I. [Equivalently, one could attenuate the signal on the
(i ~ n) line by g(sll;) 1/2 and have the same noise variance Bo on all lines.]

191

192

Linsker

This change causes the last term of Eqn. 10 to be replaced by
Under the conditions discussed earlier (Toeplitz QL and QM, and N

Bo8I1m~g(SIl)qi .

= N), we derive
(12)

Recall that the {ck } are related to {C(s)} by a Fourier transform (see just before Eqn.
7). To cotppute which choice of IC(s)} maximizes R for a given problem, we used
a gradient ascent algorithm several times, each time using a different random set of
initial I C(s)} values. For the problems whose solutions are exhibited in Figs. 2b and
2c, multiple starting points usually yielded the same solution to within the error
tolerance specified for the algorithm [apart from an arbitrary factor by which all of
the C(s)'s can be multiplied without affecting R], and that solution had the largest
R of any obtained for the given problem. That is, a limitation sometimes associated
with gradient ascent algorithms -- namely, that they may yield multiple ""solutions""
that are local, but far from global, maxima -- did not appear to be a difficulty in these
cases.
Fig. 2b (solid curve) shows the infomax solution for an example having
QL(S) = exp[ - (S/sO)2] and g(s) = exp[(s/s.)2] with So = 4, s. = 6, N = N = 32,
and Bo = 0.1. There is a central excitatory peak flanked by shallow inhibitory
sidelobes (and weaker additional oscillations). (As noted, the negative of this
solution, having a central inhibitory region and excitatory sidelobes, gives the same
R.) As Bo is increased (a range from 0.001 to 20 was studied), the peak broadens,
the sidelobes become shallower (relative to the peak), and the receptive fields of
nearby M cells increasingly overlap. This behavior is an example of the
""redundancy-diversity"" tradeoff discussed in [Linsker, 1988].
Model D -- Bounded output variance

Our previous models all produce output values Mn whose variance is not explicitly
constrained. More biologically realistic cells have limited output variance. For
example, a cell's firing rate must lie between zero and some maximum value. Thus,
the output of a model nonlinear cell is often taken to be a sigmoid function of
(~iCII;L)?

Within the context of linear cell models, we can capture the effect of a bounded
output variance by using Model D of Fig. 1. We pass the intermediate output
~iClIi(Li + VIIi) through a gain control QC that normalizes the output variance to
unity, then we add a final (Li.d. Gaussian) noise term V'II of variance R.. That is,
(13)

Without the last term, this model wo~ld be identical to Model C, since mUltiplying
both the signal and the VIIi noise by the same factor GC would not affect R. The last
term in effect fixes the number of output values that can be discriminated (Le., not
confounded with each other by the noise process V'II) to be of order Rl1!2.
The information rate for this model is derived to be (cf. Eqn. 12):

Maximum Information Preservation to Linear Systems

(14)

where V( C) is the variance of the intermediate output before it is passed through
GC:
(15)

Fig. 2b (dotted curve) shows the infomax solution (numerically obtained as above)
for the same QL(S) and g(s) functions and parameter values as were used to generate
the solid curve (for Model C), but with the new parameter Bl = 0.4. The effect of
the new Bl noise process in this case is to deepen the inhibitory sidelobes (relative
to the central peak). The more pronounced center-surround character of the
resulting M cell dampens the response of the cell to differences (between different
input patterns) in the spatially uniform component of the input pattern. This
response property allows the L .... M mapping to be info max-optimal when the
dynamic range of the cells' output response is constrained.? (A competing effect can
complicate the analysis: If Bl is increased much further, for example to 50 in the
case discussed, the sidelobes move to larger s and become shallower. This behavior
resembles that discussed at the end of the previous section for the case of increasing
Bo; in the present case it is the overall noise level that is being increased when Bl
increases and Bo is kept constant.)
TemporaUy-correlated input patterns
Let us see how infomax can be used to extract regularities in input time series, as
contrasted with the spatially-correlated input patterns discussed above. We consider
a single M cell that, at each discrete time denoted by n, can process inputs {LJ from
earlier times i ~ n (via delay !ines, for example). We use the same Model D as
before. There are two differences: First, we want g(s) = 00 for all s > 0 (input lines
from future times are ""infinitely noisy""). [A technical point: Our use of periodic
boundary conditions, while computationally convenient, means that the input value
that will occur s time steps from now is the same value that occurred (N - s) steps
ago. We deal with this by choosing g(s) to equal 1 at s = 0, to increase as
s .... -N/2 (going into the past), and to increase further as s decreases from +N/2
to 1, corresponding to increasingly remote past times. The periodicity causes no
unphysical effects, provided that we make g(s) increase rapidly enough (or make N
large enough) so that C(s) is negligible for time intervals comparable to N.] Second,
the fact that C,,; is a function only of s'"" is now a consequence of the constancy of
connection weights C(s) of a single M cell with time, rather than merely a convenient
Ansatz to facilitate the infomax computation for a set of many M cells (as it was in
previous sections).
The

infomax solution is shown in Fig. 2c for an example having
QL(S) = exp[ - (S/So)2]; g(s) = exp[ -t(s}/s.J with t(s} = s for s ~ 0 and
t(s} = s - N for s ~ 1; So = 4, Sl = 6, N = 32, Bo = 0.1, and Bl = 0.4. The result is
that the ""temporal receptive field"" of the M cell is excitatory for recent times, and

193

194

Linsker

inhibitory for somewhat more remote times (with additional weaker oscillations).
The cell's output can be viewed approximately as a linear combination of a smoothed
input and a smoothed first time derivative of the input, just as the output of the
center-surround cell of Fig. 2b can be viewed as a linear combination of a smoothed
input and a smoothed second spatial derivative of the input. As in Fig. 2b, setting
BI = 0 (not shown) lessens the relative inhibitory contribution.

SUMMARY
To gain insight into the operation of the principle of maximum information
preservation, we have applied the principle to the problem of the optimal design of
an array of linear filters under various conditions. The filter models that have been
used are motivated by certain features that appear to be characteristic of biological
networks. These features include the favoring of short connections and the
constrained range of output signal values. When nearby input signals (in space or
time) are correlated, the infomax-optimal solutions for the cases studied include (1)
center-surround cells and (2) cells sensitive to temporal variations in input. The
results of the mathematical analysis presented here apply also to arbitrary input
covariance functions of the form QL( I i - j I). We have also presented more general
expressions for the information rate, which can be used even when QL is not of this
form. The cases discussed illustrate the operation of the infomax principle in some
relatively simple but instructive situations. The analysis and results suggest how the
principle may be applied to more biologically realistic networks and input ensembles.
References
T. Berger, Rate Distortion Theory (Prentice-Hall, Englewood Cliffs, N.J., 1971),
chap. 4.
R. G. Gallager, Information Theory and Reliable Communication (John Wiley and
Sons, N.Y., 1968), p. 388.
R. Linsker, in: Neural Information Processing Systems (Denver, Nov. 1987), ed.
D. Z. Anderson (Amer. Inst. of Physics, N.Y.), pp. 485-494.
R. Linsker, Computer 21 (3) 105-117 (March 1988).
C. E. Shannon and W. Weaver, The Mathematical Theory of Communication (Univ.
of Illinois Press, Urbana, 1949).

"
1020,1994,A Computational Model of Prefrontal Cortex Function,,1020-a-computational-model-of-prefrontal-cortex-function.pdf,Abstract Missing,"A Computational Model of Prefrontal
Cortex Function
Todd S. Braver
Dept. of Psychology
Carnegie Mellon Univ.
Pittsburgh, PA 15213

Jonathan D. Cohen
Dept. of Psychology
Carnegie Mellon Univ .
Pittsburgh , PA 15213

David Servan-Schreiber
Dept. of Psychiatry
Univ . of Pittsburgh
Pittsburgh , PA 15232

Abstract
Accumulating data from neurophysiology and neuropsychology
have suggested two information processing roles for prefrontal cortex (PFC): 1) short-term active memory; and 2) inhibition. We
present a new behavioral task and a computational model which
were developed in parallel. The task was developed to probe both
of these prefrontal functions simultaneously, and produces a rich
set of behavioral data that act as constraints on the model. The
model is implemented in continuous-time , thus providing a natural
framework in which to study the temporal dynamics of processing
in the task. We show how the model can be used to examine the behavioral consequences of neuromodulation in PFC . Specifically, we
use the model to make novel and testable predictions regarding the
behavioral performance of schizophrenics, who are hypothesized to
suffer from reduced dopaminergic tone in this brain area.

1

Introduction

Prefrontal cortex (PFC) is an area of the human brain which is significantly expanded relative to other animals. There is general consensus that the PFC is centrally involved in higher cognitive activities such as planning , problem solving and
language. Recently, the PFC has been associated with two specific information processing mechanisms : short-term active memory and inhibition . Active memory is
the capacity of the nervous system to maintain information in the form of sustained
activation states (e.g. , cell firing) for short periods of time. This can be distinguished from forms of memory that are longer in duration and are instantiated as

142

Todd S. Braver, Jonathan D. Cohen, David Servan-Schreiber

modified values of physiological parameters (e.g., synaptic strength). Over the last
two decades, there have been a large number of neurophysiological studies focusing
on the cellular basis of active memory in prefrontal cortex. These studies have revealed neurons in PFC that fire selectively to specific stimuli and response patterns,
and that remain active during a delay between these. Investigators such as Fuster
(1989) and Goldman-Rakic (1987) have argued from this data that PFC maintains
temporary information needed to guide behavioral responses through sustained patterns of neural activity. This hypothesis is consistent with behavioral findings from
both animal and human lesion studies, which suggest that PFC is required for tasks
involving delayed responses to prior stimuli (Fuster, 1989; Stuss & Benson, 1986).
In addition to its role in active memory, many investigators have focused on the
inhibitory functions of PFC. It has been argued that PFC representations are required to overcome reflexive or previously reinforced response tendencies in order
to mediate a contextually appropriate - but otherwise weaker - response (Cohen &
Servan-Schreiber, 1992). Clinically, it has been observed that lesions to PFC are often associated with a syndrome of behavioral disinhibition, in which patients act in
impulsive and often socially inappropriate ways (Stuss & Benson, 1986). This syndrome has often been cited as evidence that PFC plays an important role inhibiting
behaviors which are compelling but socially inappropriate.
While the involvement of PFC in both active memory and inhibition is generally
agreed upon, computational models can play an important role in providing mechanisms by which to explain how these two information processing functions arise.
There are several computational models now in the literature which have focused
on either the active memory (Zipser, 1991), or inhibitory (Levine & Pruiett, 1989)
functions of PFC, or both functions together (Dehaene & Changeux, 1989; Cohen & Servan-Schreiber, 1992). These models have been instrumental in explaining
the role of PFC in a variety of behavioral tasks (e.g., the Wisconsin Card Sort and
Stroop). However, these earlier models are limited by their inability to fully capture the dynamical processes underlying active memory and inhibition. Specifically,
none of the simulations have been tightly constrained by the temporal parameters
found in the behavioral tasks (e.g., durations of stimuli, delay periods, and response
latencies). This limitation is not found solely in the models, but is also a feature of
the behavioral tasks themselves. The tasks simulated were not structured in ways
that could facilitate a dynamical analysis of processing.
In this paper we address the limitations of the previous work by describing both a
new behavioral task and a computational model of PFC. These have been developed
in parallel and, together, provide a useful framework for exploring the temporal
dynamics of active memory and inhibition and their consequences for behavior. We
then go on to describe how this framework can be used to examine neuromodulatory
effects in PFC, which are believed to playa critical role in both normal functioning
and in psychiatric disorders, such as schizophrenia.

2

Behavioral Assessment of Human PFC Function

We have developed a task paradigm which incorporates two components central to
the function of prefrontal cortex - short-term active memory and inhibition - and
that can be used to study the dynamics of processing. The task is a variant of the
continuous performance test (CPT), which is commonly used to study attention in

A Computational Model of Prefrontal Cortex Function

143

behavioral and clinical research. In a standard version of the task (the CPT-AX),
letters are presented one at a time in the middle of a computer screen. Subjects are
instructed to press the target button to the letter X (probe stimulus) but only when
it is preceded by an A (the cue stimulus). In previous versions of the CPT, subjects
only responded on target trials. In the present version of the task, a two response
forced-choice procedure is employed; on non-A-X trials subjects are asked to press
the non-target button. This procedure allows for response latencies to be evaluated
on every trial , thus providing more information about the temporal dimensions of
processing in the task .
Two additional modifications were made to the standard paradigm in order to
maximally engage PFC activity. The memory function of PFC is tapped by manipulating the delay between stimuli. In the CPT-AX , the prior stimulus (cue or
non-cue) provides the context necessary to decide how to respond to the probe letter . However, with a short delay (750 msec .), there is little demand on memory
for the prior stimulus. This is supported by evidence that PFC lesions have been
shown to have no effect on performance when there is only a short delay (Stuss &
Benson, 1986). With a longer delay (5000 msec.), however, it becomes necessary to
maintain a representation of the prior stimulus in order for it to be used as context
for responding to the current one. The ability of the PFC to sustain contextual
representations over the delay period can be determined behaviorally by comparing
performance on short delay trials (50%) against those with long delays (50%).
The inhibitory function of PFC is probed by introducing a prepotent response
tendency that must be overcome to respond correctly. This tendency is introduced
into the task by increasing the frequency of target trials (A followed by X). In the
remaining trials, there are three types of distractors: 1) a cue followed by a nontarget probe letter (e.g. , A-Y); 2) a non-cue followed by the target probe letter (e.g .,
B-X); and a non-cue followed by a non-target probe letter (e.g., B-Y). Target trials
occur 70% of the time, while each type of distract or trial occurs only 10% of the
time. The frequ ency of targets promotes the development of a strong tendency to
respond to the target probe letter whenever it occurs , regardless of the identity of
the cue (since a response to the X itself is correct 7 out of 8 times).
The ability to inhibit this response tendency can be examined by comparing accuracy on trials when the target occurs in the absence of the cue (B-X trials) , with
those made when neither the cue nor target occurs (i.e., B-Y trials , which provide a
measure of non-specific response bias and random responding). Trials in which the
cue but not the target probe appears (A-Y trials) are also particularly interesting
with respect to PFC function. These trials measure the cumulative influence of
active representations of context in guiding responses. In a normally functioning
system, context representations should stabilize and increase in strength as time
progresses. Thus , it is expected that A- Y accuracy will tend to decrease for long
delay trials relative to short ones .
As mentioned above, the primary benefit of this paradigm is that it provides a
framework in which to simultaneously probe the inhibitory and memory functions
associated with PFC. This is supported by preliminary neuroimaging data from
our laboratory (using PET) which suggests that PFC is, in fact, activated during
performance of the task. Although it is simple in structure, the task also generates
a rich set of behavioral data. There are four stimulus conditions crossed with two
delay conditions for which both accuracy and reaction time performance can be

144

Todd S. Braver, Jonathan D. Cohen, David Servan-Schreiber

100
90
80
70

Accurac, (Short Delay)

Accurac, (Long Delay)

V

V

60

1-

MODEL (Ace)
.DATA (Ace)

RT(ShortDelay)

J
RT (Long Delay)

750
650

!

I

550

""

.Ii

1 450

..:

350
250

~
-~~

AX

,

,

,

AY
BX
BY
Trial Condition

09
'''J!.'

AX

-~~-~

AY
BX
BY
Trial Condition

MODEL (Correct)
--- MODEL (Incorrect)
.. DATA (Correct)
'V DATA (Incorrect)

Figure 1:

Subjecl beha.viora.1 da.la. with model performa.nce s uperimpos ed . Top Panels: Acc ura.cy a.c ross

both dela.ys in a.1I four condilion s. Bottom Panels: Rea.ction times for both correc t a.nd incorrec t res pon se s in
a.1I condition s . Ba.r s repre sent s ta.nda.rd error of mea.s ure ment for the empirica.l da.ta..

measured. Figure 1 shows data gathered from 36 college-age subjects performing
this task.
In brief, we found that: 1) Accuracy was relatively unchanged in the long delays
compared to the short, demonstrating that active memory was adequately supporting performance; 2) A-Y accuracy, however, did slightly decrease at long delays,
reflecting the normal build-up of context representations over time; 3) Accuracy
on B-X trials was relatively high, supporting the assumption that subjects could
effectively use context representations to inhibit prepotent responses ; 4) A distinct
pattern emerged in the latencies of correct and incorrect responses , providing information on the temporal dynamics of processing (i .e. , responses to A-Y trials are
slow on correct trials and fast on incorrect ones; the pattern is reversed for B-X trials) . Taken together, the data provides specific, detailed information about normal
PFC functioning, which act as constraints on the development and evaluation of a
computational model.

3

A Computational Model of the CPT-AX

We have developed a recurrent network model which produces detailed information
regarding the temporal course of processing in the CPT-AX task. The network is
composed of three modules: an input module, a memory module, and an output
module. The memory module implements the memory and inhibitory functions
believed to be carried out by PFC. Figure 2 shows a diagram of the model.
Each unit in the input module represents a different stimulus condition: A, B, X &

A Computational Model of Prefrontal Cortex Function

145

OUTPUT LAYER

~~L0~
INPUT LAYER

Figure 2:

A diagram of the CPT?AX model.

Y. Units in the input module make excitatory connections on the response module,
both directly and indirectly through the memory module. Lateral inhibition within
each layer produces competition for representations . Activity from the cue stimulus
flows to the memory module, which is responsible for maintaining a trace of the
relevant context in each trial. Units in the memory module have self-excitatory
connections, which allow for the activity generated by the cue to be sustained in
the absence of input. The recurrent connectivity utilized by each unit in this module
is assumed to be a simpler, but formally equivalent analogue of a fully connected
recurrent cell assembly. Further, Zipser (1991) has used this type of connectivity to
produce temporal activity patterns which are highly similar to the firing patterns
of neurons in memory-associated areas of cortex, such as PFC. Activity from the
input and memory modules is integrated in the output module. The output of this
module determines whether a target (T) or non-target (N) response is made.
To simulate the CPT-AX task we have purposefully kept the network architecture
and size as simple as possible in order to maximize the model's interpretability. We
have therefore not attempted to simulate neural information processing in a neuronby-neuron manner. Rather, the populations of a few units are seen as capturing the
information processing characteristics of much larger populations of real neurons.
In this way, it is possible to capture the stochastic, distributed, and dynamical
properties of real neural networks with small and analytically tractable simulations.
The simulation is run in a temporally continuous framework in which processing is
governed by the following difference equation:
(1 )
where

1

(2)

is the state of unit j, Ij is the total input to j , dt is the time-step of integration, 'Y
is the gain and f3 is the bias. The continuous framework is preferable to a discrete
event-based one in that it allows for a plausible way to scale events appropriately
to the exact temporal specifications of the task (i.e., the duration of stimuli and
the delay between cue and probe). In addition, the continuous character of the
simulation naturally provides a framework for inferring the reaction times in the
various conditions.

146

4

Todd S. Braver, Jonathan D. Cohen, David Servan-Schreiber

Simulations of Behavioral Performance

We used a continuous recurrent generalization of backpropagation (Pearlmutter ,
1989) to train the network to perform the CPT-AX. All of the connection weights
were developed entirely by the training procedure , with the constraint that that all
self and between layer weights were forced to be positive and all within layer weights
were forced to be negative. Training consisted of repeated presentation of each of
the 8 conditions in the task (A-X,A-Y,B-X ,B-Y, at both long and short delays), with
the presentation frequency of each condition matching that of the behavioral task .
Weights were updated after the presentation of each trial, biases ({3) were fixed at
-2.5, and dt was set at 0.1. The network was trained deterministically ; completion
of training occurred when network accuracy reached 100% for each condition.
Following training, weights were fixed. Errors and reaction time distributions were
then simulated by adding zero-mean Gaussian noise to the net input of each unit
at every time step during trial presentation. A trial consisted of the presentation
of the cue stimulus, a delay period and then the probe stimulus. As mentioned
above, the duration of these events was appropriately scaled to match the temporal
parameters of the task (e.g ., 300 msec. duration for cue and probe presentation,
750 msec . for short delays, 5000 msec. for long delays). A time constant (1"") of 50
msec. was used for simulation in the network. This scaling factor provided sufficient
temporal resolution to capture the relationship between the two task delays while
still permitting a tractable way of simulating the events .
Responses were determined by noting which output unit reached a threshold value
first following presentation of the probe stimulus. Response latency was determined
by calculating the number of time steps taken by the model to reach threshold
multiplied by the time constant 1"". To facilitate comparisons with the experimental
reaction times, a constant k was added to all values produced . This parameter might
correspond to the time required to execute a motor response. The value of k was
determined by a least mean squares fit to the data. 1000 trials of each condition
were run in order to obtain a reliable estimate of performance under stochastic
conditions. The standard deviation of the noise distribution (0') and the threshold
(T) of the response units were adjusted to produce the best fit to the subject data.
Figure 1 compares the results of the simulation against the behavioral data.
As can be seen in the figure, the model provides a good fit to the behavioral data
in both the pattern of accuracy and reaction times . The model not only matches
the qualitative pattern of errors and reaction times but produces very similar quantitative results as well. The match between model and experimental results is particularly striking when it is considered that there are a total of 24 data points that
this model is fitting, with only 4 free parameters (O',T,1"" ,k). The model's ability to
successfully account for the pattern of behavioral performance provides convincing
evidence that it captures the essential principles of processing in the task. We can
then feel confident in not only examining normal processing, but also in extending
the model to explore the effects of specific disturbances to processing in PFC .

5

Behavioral Effects of Neuromodulation in PFC

In a previous meeting of this conference a simulation of a simpler version of the CPT
was discussed (Servan-Schreiber, Printz, & Cohen, 1990). In this simulation the

147

A Computational Model of Prefrontal Cortex Function

Accuracy (Short Delay)

Accuracy (Long Delay)

100

....CJ

......
~

,,
I

90

I

I

Q

I

U

....
==
~

...

,
I

,

CJ
~

=-

I
,

80

I

~

70
60

AX

AY

BX

BY

AX

AY

BX

BY

MODEL (Normal Gain)
-- - MODEL (Reduced Gain)
.DATA (Controls)
Comparision of of model performance with normal and redu ced gain . The graph illustrates ~he effec~
of reducing gain in the memory layer on task performance. In the baseline network ""1=1 , in ~he reduced-gain
network ""1=0.8.

Figure 3:

effects of system-wide changes in catecholaminergic tone were captured by changing
the gain (-r) parameter of network units. Changes in gain are thought correspond to
the action of modulatory neurotransmitters in modifying the responsivity of neurons
to input signals (Servan-Schreiber et aI. , 1990; Cohen & Servan-Schreiber, 1992).
The current simulation of the CPT offers the opportunity to explore the effects
of neuromodulation on the information processing functions specific to PFC. The
transmitter dopamine is known to modulate activity in PFC , and manipulations
to prefrontal dopamine have been shown to have effects on both memory-related
neuronal activity and behavioral performance (Sawaguchi & Goldman-Rakic, 1991).
Furthermore, it has been hypothesized that reductions of the neuromodulatory effects of dopamine in PFC are responsible for some of the information processing
deficits seen in schizophrenia. To simulate the behavior of schizophrenic subjects,
we therefore reduce the gain ('Y) of units in the memory module of the network.
With reduced gain in the memory module, there are striking changes in the model's
performance of the task. As can be seen in Figure 3, in the short delay conditions
the performance of the reduced-gain model is relatively similar to that of control
subjects (and the intact model). However, at long delays , the reduced-gain model
produces a qualitatively different pattern of performance. In this condition, the
model has a high B-X error rate but a low A-Y error rate, a pattern which is opposite
to that seen in the control subjects. This double dissociation in performance is a
robust effect of the reduced-gain simulation (i.e. , it seems relatively uninfluenced
by other parameter adjustments) .
Thus , the model makes clear-cut predictions which are both novel and highly
testable. Specifically, the model predicts that: 1) Differences in performance be-

148

Todd S. Braver, lonatMn D. Cohen, David Servan-Schreiber

tween control and schizophrenic subjects will be most apparent at long delays ; 2)
Schizophrenics will perform significantly worse than control subjects on B-X trials
at long delays; 3) Schizophrenics will perform significantly better than control subjects on A-Y trials at long delays. This last prediction is especially interesting given
the fact that tasks in which schizophrenics show superior performance relative to
controls are relatively rare in experimental research.
Furthermore, the model not only makes predictions regarding schizophrenic behavioral performance, but also offers explanations as to their mechanisms. Analyses of
the trajectories of activation states in the memory module reveals that both of the
dissociations in performance are due to failures in maintaining representations of
the context set up by the cue stimulus. Reducing gain in the memory module blurs
the distinction between signal and noise , and causes the context representations to
decay over time. As a result, in the long delay trials , there is a higher probability
that the model will show both failures of inhibition (more B-X errors) and memory
(less A- Y errors) .

6

Conclusions

The results of this paper show how a computational analysis of the temporal dynamics of PFC information processing can aid in understanding both normal and disturbed behavior. We have developed a behavioral task which simultaneously probes
both the inhibitory and active memory functions of PFC. We have used this task in
combination with a computational model to explore the effects of neuromodulatory
dysfunction, making specific predictions regarding schizophrenic performance in the
CPT-AX. Confirmation of these predictions now await further testing.

References
Cohen, J. & Servan-Schreiber, D. (1992). Context , cortex, and dopamine: A connectionist
approach to behavior and biology in schizophrenia. Psychological Review, 99 , 45- 77.
Dehaene, S. & Changeux, J. (1989). A simple model of prefrontal cortex function
delayed-response tasks. Journal of Cognitive Neuroscience, 1 (3), 244- 261.

III

Fuster, J . (1989). The prefrontal cortex. New York: Raven Press.
Goldman-Rakic, P. (1987). Circuitry of primate prefrontal cortex and regulation of behavior by representational memory. In F. Plum (Ed .) , Handbook of physiology-the nervous
system, v. Bethesda, MD: American Physiological Society, 373-417.
Levine, D. & Pruiett, P. (1989). Modeling some effects of frontal lobe damage: novelty
and perseveration. Neural Networks, 2 , 103-116.
Pearlmutter, B. (1989). Learning state space trajectories in recurrent neural networks.
Neural Computation, 1 , 263-269.
Sawaguchi, T. & Goldman-Rakic, P. (1991). D1 dopamine receptors in prefrontal cortex:
Involvement in working memory. Science , 251 , 947-950.
Servan-Schreiber, D., Printz, H., & Cohen, J. (1990). The effect of catecholamines on
performance: From unit to system behavior. In D. Touretzky (Ed.), Neural information
processing systems 2. San Mateo, GA: Morgan Kaufman , 100-108.
Stuss, D. & Benson , D. (1986) . The frontal lobes. New York: Raven Press.
Zipser, D. (1991). Recurrent network model of the neural mechanism of short-term active
memory. Neural Computation, 3,179- 19.3.

"
1021,1995,The Gamma MLP for Speech Phoneme Recognition,,1021-the-gamma-mlp-for-speech-phoneme-recognition.pdf,Abstract Missing,"The Gamma MLP for Speech Phoneme
Recognition

Steve

Lawrence~

Ah Chung Tsoi, Andrew D. Back
{lawrence,act,back}Oelec.uq.edu.au

Department of Electrical and Computer Engineering
University of Queensland
St. Lucia Qld 4072 Australia

Abstract
We define a Gamma multi-layer perceptron (MLP) as an MLP
with the usual synaptic weights replaced by gamma filters (as proposed by de Vries and Principe (de Vries and Principe, 1992)) and
associated gain terms throughout all layers. We derive gradient
descent update equations and apply the model to the recognition
of speech phonemes. We find that both the inclusion of gamma
filters in all layers, and the inclusion of synaptic gains, improves
the performance of the Gamma MLP. We compare the Gamma
MLP with TDNN, Back-Tsoi FIR MLP, and Back-Tsoi I1R MLP
architectures, and a local approximation scheme. We find that the
Gamma MLP results in an substantial reduction in error rates.

1
1.1

INTRODUCTION
THE GAMMA FILTER

Infinite Impulse Response (I1R) filters have a significant advantage over Finite Impulse Response (FIR) filters in signal processing: the length of the impulse response
is uncoupled from the number of filter parameters. The length of the impulse response is related to the memory depth of a system, and hence I1R filters allow a
greater memory depth than FIR filters of the same order. However, I1R filters are
*http://www.neci.nj.nec.com/homepages/lawrence

786

S. LAWRENCE, A. C. TSOI, A. D. BACK

not widely used in practical adaptive signal processing. This may be attributed
to the fact that a) there could be instability during training and b) the gradient
descent training procedures are not guaranteed to locate the global optimum in the
possibly non-convex error surface (Shynk, 1989).
De Vries and Principe proposed using gamma filters (de Vries and Principe, 1992),
a special case of IIR filters, at the input to an otherwise standard MLP. The gamma
filter is designed to retain the uncoupling of memory depth to the number of parameters provided by IIR filters, but to have simple stability conditions.
The output of a neuron in a multi-layer perceptron is computed using 1
I
f L--i=O WkiYi
I
1-1)
De Vries and Principe consider adding short
Yk =
.

("""",Nr-l

term memory with delays: YkI --

f

("""",Nr-l
L--i=O """",K
L--j=O 9kij (t I

h
J.) Yi1-1 (t - J.)) were

(r!i)!

9~ij =
tj-le-/-'~it
j = 1, ... , K . The depth of the memory is controlled
by J.t, and K is the order of the filter. For the discrete time case, we obtain the
recurrence relation: zo(t) = x(t) and Zj(t) = (1 - J.t)Zj(t - 1) + J.tZj-l (t - 1) for
j = 1, ... , K. In this form, the gamma filter can be interpreted as a cascaded series
of filter modules, where each module is a first order IIR filter with the transfer function q-(I-/-,) , where qZj(t) ~ Zj(t + 1). We have a filter with K poles, all located
at 1 - J.t. Thus, the gamma filter may be considered as a low pass filter for J.t < 1.
The value of J.t can be fixed, or it can be adapted during training.

2

NETWORK MODELS

Figure 1: A gamma filter synapse with an associated gain term 'c'.
We have defined a gamma MLP as a multi-layer perceptron where every synapse
contains a gamma filter and a gain term, as shown in figure 1. The motivation
behind the inclusion of the gain term is discussed later. A separate J.t parameter
is used for each filter. Update equations are derived in a manner analogous to the
standard MLP and can be found in Appendix A. The model is defined as follows.

lwhere yi is the output of neuron k in layer I, Nl is the number of neurons in layer I,
is the weight connecting neuron k in layer I to neuron i in layer I - 1, yb = 1 (bias),
and / is commonly a sigmoid function.

Wii

The Gamma MLP for Speech Phoneme Recognition

787

Definition 1 A Gamma MLP with L layers excluding the input layer (0,1, ... , L),
gamma filters of order K, and No, N 1 , ... , NL neurons per layer, is defined as

f (x~ (t))
N'-l

K

i=O

j=O

L C~i(t) L wL j (t)Zkij (t)

Ziij (t)
Ziij (t)

(1- ILL(t))zkij(t -1) + ILL(t)zki(j_I)(t -1)
y!-l (t)

(1)

1

~j ~

K

j=O
eO / 2 _e- o / 2

where y(t) = neuron output, c'ki = synaptic gain, f(a) = eO/2+e 0/2, k
1,2, ... ,N, (neuronindex), I = 0,1, ... ,L(layer), and Ziijli=O = 1, W~ij li=O,#O
0, C~ij li=O = 1(bias).

o
For comparison purposes, we have used the TDNN (Time Delay Neural Network)
architecture2 , the Back-Tsoi FIR3 and I1R MLP architectures (Back and Tsoi,
1991a) where every synapse contains an FIR or I1R filter and a gain term, and the
local approximation algorithm used by Casdagli (k-NN LA) (Casdagli, 1991)4. The
Gamma MLP is a special case of the I1R MLP.

3
3.1

TASK
MOTIVATION

Accurate speech recognition requires models which can account for a high degree
of variability in the data. Large amounts of data may be available but it may be
impractical to use all of the information in standard neural network models.
Hypothesis: As the complexity of a problem increases (higher dimensionality, greater

variety of training data), the error surface of a neural network becomes more complex. It may contain a number of local minima5 many of which may be much worse
than the global minimum. The training (parameter estimation) algorithms become
""stuck"" in local minima which may be increasingly poor compared to the global
optimum. The problem suffers from the so called ""curse of dimenSionality"" and the
2We use TDNN to refer to an MLP with a time window of inputs, not the replicated
architecture introduced by Lang (Lang et al., 1990) .
3We distinguish the Back-Tsoi FIR network from the Wan FIR network in that the
Wan architecture has no synaptic gains, and the update algorithms are different. The
Back-Tsoi update algorithm has provided better convergence in previous experiments.
4Casdagli created an affine model of the following form for each test pattern: yi =
aD + L~=l ai~, where k is the number of neighbors, j = 1, ... , k, and n is the input
dimension. The resulting model is used to find y for the test pattern.
5We note that it can be difficult to distinguish a true local minimum from a long plateau
in the standard backpropagation algorithm.

788

S. LAWRENCE, A. C. TSOI, A. D. BACK

difficulty in optimizing a function with limited control over the nature of the error
surface.
We can identify two main reasons why the application of the Gamma MLP may
be superior to the standard TDNN for speech recognition: a) the gamma filtering
operation allows consideration of the input data using different time resolutions and
can account for more past history of the signal which can only be accounted for in
an FIR or TDNN system by increasing the dimensionality of the model, and b)
the low pass filtering nature of the gamma filter may create a smoother function
approximation task, and therefore a smoother error surface for gradient descent 6 .

3.2

TASK DETAILS
Model Input Window

[~

Networl( Output 1
Target Function

Classification 0
Networl( Output 2

II

;

~}
!

Frames of RASTA data

...::.. ...:::'.!'}

j :

~.} ,.;:!.. """"::'I'}
. ; i

~

l I~

~

Sequence End

~

Figure 2: PLP input data format and the corresponding network target functions for the
phoneme ""aa"" .
Our data consists of phonemes extracted from the TIMIT database and organized
as a number of sequences as shown in figure 2 (example for the phoneme ""aa"").
One model is trained for each phoneme. Note that the phonemes are classified in
context, with a number of different contexts, and that the surrounding phonemes
are labelled only as not belonging to the target phoneme class. Raw speech data
was pre-processed into a sequence of frames using the RASTA-PLP v2.0 software7 .
We used the default options for PLP analysis. The analysis window (frame) was
20 ms. Each succeeding frame overlaps with the preceding frame by 10 ms. 9
PLP coefficients together with the signal power are extracted and used as features
describing each frame of data. Phonemes used in the current tests were the vowel
""aa"" and the fricative ""s"" . The phonemes were extracted from speakers coming
from the same demographic region in the TIMIT database. Multiple speakers were
used and the speakers used in the test set were not contained in the training set.
The training set contained 4000 frames, where each phoneme is roughly 10 frames.
The test set contained 2000 frames, and an additional validation set containing 2000
frames was used to control generalization.
6If we consider a very simple network and derive the relationship of the smoothness of
the required function approximation to the smoothness of the error surface this statement
appears to be valid. However, it is difficult to show a direct relationship for general
networks.
7 Obtained from ftp:/ /ftp.icsi.berkeley.edu/pub/speech/rasta2.0.tar.Z.

The Gamma MLP for Speech Phoneme Recognition

4

789

RESULTS

Two outputs were used in the neural networks as shown by the target functions in
figure 2, corresponding to the phoneme being present or not. A confidence criterion
was used: Ymax x (Ymax - Ymin) (for soft max outputs). The initial learning rate was
0.1, 10 hidden nodes were used, FIR and Gamma orders were 5 (6 taps), the TDNN
and k-NN models had an input window of 6 steps in time, the tanh activation function was used, target outputs were scaled between -0.8 and 0.8, stochastic update
was used, and initial weights were chosen from a set of candidates based on training
set performance. The learning rate was varied over time according to the schedule:

= 'TIo/ (N/2 + max (1,(cI- "",~!(o.Cj(n
?)) where'TI = learning rate, 'TIo = initial
(I
learning rate, N = total epochs, n = current epoch, Cl = 50, C2 = 0.65. This is
'TI

C2 N
C2)N

similar to the schedule proposed in (Darken and Moody, 1991) with an additional
term to decrease the learning rate towards zero over the final epochs 8 .
I

Train Error %
FIR MLP
Gamma MLP
TDNN
k-NN LA

I

2-NN

I

Test Error %
FIR MLP
Gamma MLP
TDNN
k-NN LA

I

2-NN

I

Test False +ve
FIR MLP
Gamma MLP
TDNN
k-NN LA

I

Test False -ve
FIR MLP
Gamma MLP
TDNN
k-NN LA

I

5-NN

1st layer
17.6
0.43
0 .39
7.78

I

All layers
14.5
1.5
5.73
0 .88

I

Gains , 1st layer
27.2
0 .59
6 .07
0 .12

I

Gains , all layers
40 .9
19.8
5.63
1.68
14.4
0.86

I

5-NN l i s t layer
22.2
0.97
0.16
14 .7

I

All layers
20.4
0 .61
13.5
0 . 33

I

Gams , 1st layer
29
0.14
12.8
1.0

I

Gams , all layers
41
21
12.7
0.50
24.5
0 .68

I

All layers
2.0
11.4
7 .01
0.47

I

All layers
44.1
5 .6
2.2
30.4

0

0

31

I

2-NN

2-NN

53

I

28 .4

I

22.6

I

I

5-NN l i s t layer
13.5
0 .67
7 .94
0.45

I

Gams , 1st layer
4.5
0 .77
6.83
0 .34

I

Gams , all layers
31.3
49.0
8.05
1.8
13
0 .27

I

17.4

I

5-NN l i s t layer
44.9
2 .6
32 .2
1.2

I

Gams , 1st layer
92.9
2.4
2 .8
28.4

I

Gams , all
66.4
24.7
54.6

layers
53
4.4
1.8

I

56.8

Table 1: Results comparing the architectures and the use of filters in all layers and
synaptic gains for the FIR and Gamma MLP models. The NMSE is followed by the
standard deviation. The TDNN results are listed under an arbitrary column heading
(gains and 1st layer/alilayers does not apply).
The results of the simulations are shown in table 19 . Each result represents an
average over four simulations with different random seeds - the standard deviation
of the four individual results is also shown. The FIR and Gamma MLP networks
have been tested both with and without synaptic gains, and with and without
filters in the output layer synapses. These results are for the models trained on
the ""s"" phoneme, results for the ""aa"" phoneme exhibit the same trend. ""Test false
negative"" is probably the most important result here, and is shown graphically
in figure 3. This is the percentage of times a true classification (ie. the current
8Without this term we have encountered considerable parameter fluctuation over the
last epoch.
9NMSE

= 2:~=1 (d(k) -

y(k))2

I

(2:~=1 (d(k) - (2:~=1 d(k)) INr) IN.

790

S. LAWRENCE, A. C. TSOI, A. D. BACK
60

--""

55
Q)

~

~

45

Q)

40

'""

LL

35

i

30

I-

k-NN LA _._._ ..

f------ f

Z

.!!2

~ Ga:~~TDNN
~t~ -_=-=-~-'
.. _.-

50

I - - -__ I

1

-r-?---- ----+ --- -

25
20
2-NN

5-NN

NG 1 L NG AL

G lL

GAL

Figure 3: Percentage of false negative classifications on the test set. NG=No gains,
G=Gains, lL=filters in the first layer only, AL=filters in all layers. The error bars show
plus and minus one standard deviation. The synaptic gains case for the FIR MLP is not
shown as the poor performance compresses the remainder of the graph. Top to bottom,
the lines correspond to: k-NN LA (left), TDNN, FIR MLP, and Gamma MLP.
phoneme is present) is incorrectly reported as false. From the table we can see
that the Gamma MLP performs Significantly better than the FIR MLP or standard
TDNN models for this problem. Synaptic gains and gamma filters in all layers
improve the performance of the Gamma MLP, while the inclusion of synaptic gains
presented difficulty for the FIR MLP. Results for the IIR MLP are not shown - we
have been unable to obtain significant convergence lO . We investigated values of k
not listed in the table for the k-NN LA model, but it performed poorly in all cases.

5

CONCLUSIONS

We have defined a Gamma MLP as an MLP with gamma filters and gain terms in
every synapse. We have shown that the model performs significantly better on our
speech phoneme recognition problem when compared to TDNN, Back-Tsoi FIR and
IIR MLP architectures, and Casdagli's local approximation model. The percentage
of times a phoneme is present but not recognized for the Gamma MLP was 44%
lower than the closest competitor, the Back-Tsoi FIR MLP model.
The inclusion of gamma filters in all layers and the inclusion of synaptic gains improved the performance of the Gamma MLP. The improvement due to the inclusion
of synaptic gains may be considered non-intuitive to many - we are adding degrees
of freedom, but no additional representational power. The error surface will be different in each case, and the results indicate that the surface for the synaptic gains
case is more amenable to gradient descent. One view of the situation is seen by
Back & Tsoi with their FIR and IIR MLP networks (Back and Tsoi, 1991b): From
a signal processing perspective the response of each synapse is determined by polezero positions. With no synaptic gains, the weights determine both the static gain
and the pole-zero positions of the synapses. In an experimental analysis performed
by Back & Tsoi it was observed that some synapses devoted themselves to modellOTheoretically, the IIR MLP model is the most powerful model used here. Though it
is prone to stability problems, the stability of the model can and was controlled in the
simulations performed here (basically, by reflecting poles that move outside the unit circle
back inside). The most obvious hypothesis for the difficulty in training the model is related
to the error surface and the nature of gradient descent. We expect the error surface to be
considerably more complex for the IIR MLP model, and for gradient descent update to
experience increased difficulty optimizing the function.

The Gamma MLP for Speech Phoneme Recognition

791

ing the dynamics of the system in question, while others ""sacrificed"" themselves to
provide the necessary static gains l l to construct the required nonlinearity.

APPENDIX A: GAMMA MLP UPDATE EQUATIONS
~W~i;(t)

=

8J(t)
-'1 8

I

I

()

w"",; t

I

I

= '1 6"" (t)c"", (t)Z""i; (t)

(2)

~C~i(t)
~J'~i (t)

(3)

=

(4)

o

=

j=O

(1 - J'~i(t))a~,;(t -1) + J'~i(t)a~iC;_I)(t - 1)
+z~,(;_I)(t -1) - Z~i;(t - 1)

(5)
1 $j $ K

I=L

(6)

1 $j $ K

1

(1 - J';,,(t)).B;,,;(t -1)

j=O

+ J';,,(t).B~""(;_l) (t -

1)

1 $j $K

(7)

Acknowledgments
This work has been partially supported by the Australian Research Council (ACT and
ADB) and the Australian Telecommunications and Electronics Research Board (SL).

References
Back, A. and Tsoi, A. (1991a). FIR and IIR synapses, a new neural network architecture
for time series modelling. Neural Computation, 3(3):337-350.
Back, A. D. and Tsoi, A. C. (1991b). Analysis of hidden layer weights in a dynamic locally
recurrent network. In Simula, 0., editor, Proceedings International Conference on
Artificial Neural Networks, ICANN-91, volume 1, pages 967-976, Espoo, Finland.
Casdagli, M. (1991). Chaos and deterministic versus stochastic non-linear modelling. J.R.
Statistical Society B, 54(2):302-328.
Darken, C. and Moody, J. (1991). Note on learning rate schedules for stochastic optimization. In Neural Information Processing Systems 3, pages 832-838. Morgan Kaufmann.
de Vries, B. and Principe, J. (1992). The gamma model- a new neural network for temporal
processing. Neural Networks, 5(4):565-576.
Lang, K. J., Waibel, A. H., and Hinton, G. E. (1990). A time-delay neural network
architecture for isolated word recognition. Neural Networks, 3:23-43.
Shynk, J . (1989). Adaptive IIR filtering. IEEE ASSP Magazine, pages 4-21.
llThe neurons were observed to have gone into saturation, providing a constant output.

PART VII
VISION

"
1022,1995,A Multiscale Attentional Framework for Relaxation Neural Networks,,1022-a-multiscale-attentional-framework-for-relaxation-neural-networks.pdf,Abstract Missing,"A Multiscale Attentional Framework for
Relaxation Neural Networks
Dimitris I. Tsioutsias
Dept. of Electrical Engineering
Yale University
New Haven, CT 06520-8285

Eric Mjolsness
Dept. of Computer Science & Engineering
University of California, San Diego
La Jolla, CA 92093-0114

tsioutsias~cs.yale.edu

emj~cs.ucsd.edu

Abstract
We investigate the optimization of neural networks governed by
general objective functions. Practical formulations of such objectives are notoriously difficult to solve; a common problem is the
poor local extrema that result by any of the applied methods. In
this paper, a novel framework is introduced for the solution oflargescale optimization problems. It assumes little about the objective
function and can be applied to general nonlinear, non-convex functions; objectives in thousand of variables are thus efficiently minimized by a combination of techniques - deterministic annealing ,
multiscale optimization, attention mechanisms and trust region optimization methods.

1

INTRODUCTION

Many practical problems in computer vision, pattern recognition , robotics and other
areas can be described in terms of constrained optimization . In the past decade,
researchers have proposed means of solving such problems with the use of neural
networks [Hopfield & Tank, 1985; Koch et ai., 1986], which are thus derived as
relaxation dynamics for the objective functions codifying the optimization task.
One disturbing aspect of the approach soon became obvious , namely the apparent inability of the methods to scale up to practical problems , the principal reason
being the rapid increase in the number of local minima present in the objectives as
the dimension of the problem increases. Moreover most objectives, E( v), are highly
nonlinear, non-convex functions of v , and simple techniques (e.g. steepest descent)

D. I. TSIOUTSIAS, E. MJOLSNESS

634

will , in general , locate the first minimum from the starting point.
In this work, we propose a framework for solving large-scale instances of such optimization problems. We discuss several techniques which assist in avoiding spurious
minima and whose combined result is an objective function solution that is computationallyefficient, while at the same time being globally convergent. In section 2.1
we discuss the use of deterministic annealing as a means of avoiding getting trapped
into local minima. Section 2.2 describes multiscale representations of the original
objective in reduced spatial domains. In section 2.3 we present a scheme for reducing the computational requirements of the optimization method used, by means of
a focus of attention mechanism. Then, in section 2.4 we introduce a trust region
method for the relaxation phase of the framework, which uses second order information (i.e. curvature) of the objective function. In section 3 we present experimental
results on the application of our framework to a 2-D region segmentation objective
with discontinuities. Finally, section 4 summarizes our presentation.

2

THEORETWALFRAMEWORK

Our optimization framework takes the form of a list of nested loops indicating the
order of conceptual (and computational) phases that occur: from the outer to the
inner loop we make use of deterministic annealing, a multiscale representation , an
attentional mechanism and a trust region optimization method.

2.1

ANNEALING NETS

The usefulness of statistical mechanics for designing optimization procedures has
recently been established; prime examples are simulated annealing and its various
mean field theory approximations [Hopfield & Tank, 1985; Durbin & Willshaw,
1987]. The success of such methods is primarily due to entropic terms included in
the objective (i .e. syntactic terms), but the price to pay is their highly nonlinear
form. Interestingly, those terms can effectively be convexified by the use of a ""temperature"" parameter, T , allowing for a reduction in the number of minima and the
ability to track the solution through ""temperature"".

2.2

MULTISCALE REPRESENTATION

To solve large-scale problems in thousands of variables , we need to speed up the
convergence of the method while still retaining valid state-space trajectories. To
accomplish this we introduce smaller, approximate versions of the problem at coarser
spatial scales [Mjolsness et al. , 1991] ; the nonlinearity of the original objective is
maintained at all scales, as opposed to other approaches where the objectives and
their derivatives are either approximated by the use of finite difference methods ,
or solved for by multigrid techniques where a quadratic objective is still assumed .
Consequently, the multiscale representation exploits the effective smoothness in the
objectives: by alternating relaxation phases between coarser and finer scales, we
use the former to identify extrema and the latter to localise them.

2.3

FOCUS OF ATTENTION

To further reduce the computational requirements of larg~scale optimization (and
indirectly control its temporal behavior), we use a focus of attention (FoA) mechanism [Mjolsness & Miranker , 1993], reminiscent of the spotlight hypothesis argued

A Multiscale Attentional Framework for Relaxation Neural Networks

635

to exist in early vision systems [Koch & Ullman, 1985; Olshausen et al., 1993]. The
effect of a FoA is to support efficient, responsive analysis: it allows resources to be
focused on selected areas of a computation and can rapidly redirect them as the
task requirements evolve.
Specifically, the FoA becomes a characteristic function, 7l'(X) , determining which
of the N neurons are active and which are clamped during relaxation, by use of a
discrete-valued vector, X, and by the rule: 7l'i(X) = 1 if neuron Vi is in the FoA, and
zero otherwise. Moreover, a limited number, n, of neurons Vi are active at any given
instant: I:i 7l'i(X) = n, with n? Nand n chosen as an optimal FoA size. To tie the
attentional mechanism to the multiscale representation, we introduce a partition
of the neurons Vi into blocks indexed by a (corresponding to coarse-scale blockneurons), via a sparse rectangular matrix Bia E {O, I} such that I:a Bia = 1, Vi,
with i = 1, ... ,N, a = 1,oo.,K and K?N. Then 7l'i(X) = I:aBiaXa, and we use
each component of X for switching a different block of the partition; thus, a neuron
Vi is in the FoA iff its coarse scale block a is in the FoA, as indicated by Xa. As
a result, our FoA need not necessarily have a single region of activity: it may well
have a distributed activity pattern as determined by the partitions Bia. 1

Clocked objective function notation [Mjolsness & Miranker, 1993] makes the task
more apparent: during the active-x phase the FoA is computed for the next activev phase, determining the subset of neurons Vi on which optimization is to be carried
out. We introduce the quantity E ;dv] == g~ ~ (Ti is a time axis for Vi) [Mjolsness
& Miranker, 1993] as an estimate of the predicted dE arising from each Vi if it joins
the FoA. For HopfieldjGrossberg dynamics this measure becomes:
E ;d v ] =

_g~(gi1(Vi)) (~~)

2

(1)

== -gH U i)(E,i)2

wi th E,i ~f 'V'i E, and gi the transfer function for neuron Vi (e.g. a sigmoid function). Eq. (1) is used here analogously to saliency measures introduced into neurophysiological work [Koch & Ullman, 1985]; we propose it as a global measure
of conspicuousness. As a result, attention becomes a k-winner-take-all (kWTA)
network:

a

a

where I refers to the scale for which the FoA is being determined (I = 1, ... , L), EEl
conforms with the clocked objective notation, and the last summand corresponds
to the subspace on which optimization is to be performed, as determined by the
current FoA.2 Periodically, an analogous FoA through spatial scales is run, allowing
re-direction of system resources to the scale which seems to be having the largest
combined benefit and cost effect on the optimization [Tsioutsias & Mjolsness, 1995].
The combined effect of multiscale optimization and FoA is depicted schematically in
Fig. 1: reduced-dimension functionals are created and a FoA beam ""shines"" through
scales picking the neurons to work on.
Preferably, Bia will be chosen to minimize the number of inter-block connections.
Before computing a new FoA we update the neighbors of all neurons that were included
in the last focus; this has a similar effect to an implicit spreading of activation.
1

2

D. I. TSIOUTSIAS, E. MJOLSNESS

636

Layer 3

Layer 1

Figure 1: Multiscale Attentional Neural Nets: FoA on a layer (e.g. L=l) competes
with another FoA (e.g . L=2) to determine both preferable scale and subspace.
2.4

OPTIMIZATION PHASE

To overcome the problems generally associated with the steepest descent m ethod,
other techniques have been devised . Newton 's method , although successful in small
to medium-sized problems, does not scale well in large non-convex instances and is
computationally intensive. Quasi-Newton methods are efficient to compute , have
quadratic termination but are not globally convergent for general nonlinear, nonconvex functions. A method that guarantees global convergence is the trust region
method [Conn et al., 1993] . The idea is summarized as follows : Newton's method
suffers from non-positive definite Hessians; in such a case, the underlying function
m(k)(6) obtained from the 2nd order Taylor expansion of E(Vk + 6) does not have
a minimum and the method is not defined, or equivalently, the region around the
current point Vk in which the Taylor series is adequate does not include a minimizing
point of m(k)(6). To resolve this, we can define a neighborhood Ok of Vk such that
m(k)(6) agrees with E(Vk + 6) in some sense; then, we pick Vk+l
Vk + 6 k , where
6 k minimizes m(k)(6) , V(Vk + 6) E Ok . Thus , we seek a solution to the resulting
subproblem:

=

(3)
where 1I ?lIp is any kind of norm (for instance, the L2 norm leads to the LevenbergMarquardt methods) , and ~k is the radius of Ok, adaptively modified based on an

=

+

accuracy ratio Tk = (~E(k)/~m(k)
(E(k ) - E(Vk
6k?/(m(k)(O) - m(k)(6 k ?;
~E(k) is the ""actual reduction"" in E(k) when step 6 k is taken, and ~m(k) the
""predicted reduction"" . The closer Tk is to unity, the better the agreement between
the local quadratic model of E (k) and the objective itself is , and ~k is modified

adaptively to reflect this [Conn et al., 1993].
We need to make some brief points here (a complete discussion will be given elsewhere [Tsioutsias & Mjolsness, 1995]):

A Multiscale Attentional Framework for Relaxation Neural Networks

637

? At each spatial scale of our multiscale representation, we optimize the corresponding objective by applying a trust region method. To obtain sufficient
relaxation progress as we move through scales we have to maintain meaningful region sizes, Llk; to that end we use a criterion based on the curvature
of the functionals along a searching direction.
? The dominant relaxation computation within the algorithm is the solution
of eq. (3). We have chosen to solve this subproblem with a preconditioned
conjugate gradient method (PCG) that uses a truncated Newton step to
speed up the computation; steps are accepted when a sufficiently good
approximation to the quasi-Newton step is found. 3 In our case, the norm
in eq. (3) becomes the elliptical norm 1I~llc = ~tc~, where a diagonal
preconditioner to the Hessian is used as the scaling matrix C.

? If the neuronal connectivity pattern of the original objective is sparse (as
happens for most practical combinatorial optimization problems), the pattern of the resulting Hessian can readily be represented by sparse static data
structures,4 as we have done within our framework. Moreover, the partition
matrices, Bia, introduce a moderate fill-in in the coarser objectives and the
sparsity of the corresponding Hessians is again taken into account.

3

EXPERIMENTS

We have applied our proposed optimization framework to a spatially structured
objective from low-level vision, namely smooth 2-D region segmentation with the
inclusion of discontinuity detection processes:

ij

ij

ij

ij

ij

where d is the set of image intensities, j is the real-valued smooth surface to be fit to
the data, lV and lh are the discrete-valued line processes indicating a non-zero value
in the intensity gradient, and ?(x) = -(2g o)-1[lnx+ln(1-x)] is a barrier function
restricting each variable into (0,1) by infinite barriers at the borders. Eq. (4) is
a mixed-nonlinear objective involving both continuous and binary variables ; our
framework optimizes vectors j, lh and lV simultaneously at any given scale as continuous variables, instead of earlier two-step, alternate continuous/discrete-phase
approaches [Terzopoulos, 1986].
We have tested our method on gradually increasing objectives, from a ""small"" size
of N=12,288 variables for a 64x64 image, up to a large size of N=786 ,432 variables
for a 512x512 image; the results seem to coincide with our theoretical expectations:
a significant reduction in computational cost was observed and consistent convergence towards the optimum of the objective was found for various numbers of coarse
scales and FoA sizes. The dimension of the objective at any scale I was chosen via
a power law: N(L-l+1)! L, where L is the total number of scales and N the size of
3

4

The algorithm can also handle directions of negative curvature.
This property becomes important in a neural net implementation.

D. I. TSIOUTSIAS, E. MJOLSNESS

638

the original objective.
The effect of our multiscale optimization with and without a FoA is shown in Fig. 2
for the 128x128 and the 512x512 nets, where E( v*) is the best final configuration
with a one-level no-FoA net , and cumulative cost is an accumulated measure in the
number of connection updates at each scale; a consistent scale-up in computational
efficiency can be noted when L > 1, while the cost measure also reflects the relative
total wall-clock times needed for convergence. Fig. 3 shows part of a comparative
study we made for saliency measures alternative to eq. (1) (e.g. g~IE,il), in order
to investigate the validity of eq. (1) as a predictor of l:!..E: the more prominent
""linearity"" in the left scatterplot seems to justify our choice of saliency.
104

M-'S-'/-_A_T_N_e_t_s,..,,(_12_8_t2-,)_
: _L_=--,1,_2'-,3_ _ _---,

. - -_ _ _

10'

MS/ AT Nets (512t2) : L=1,2,3,4

10'

10'

10'
10'
10'
10'
10'

~ 10 l
~

2

'""

Nl

I

10'

#1

>'

g10 - 1

10-'
10-'

10""
10""
10-4
10-110

10-'

2000

10-' 0

Figure 2: Multiscale Optimization (curves labeled by number of scales used): #numbered curves correspond to nets without a FoA , simply-numbered ones to nets
with a FoA used at all scales. The lowest costs result from the combined use of
multiscale optimization and FoA.

4

CONCLUSION

We have presented a framework for the optimization of large-scale objective functions using neural networks that incorporate a multiscale attentional mechanism.
Our method allows for a continuous adaptation of the system resources to the computational requirements of the relaxation problem through the combined use of
several techniques. The framework was applied to a 2-D image segmentation objective with discontinuities; formulations of this problem with tens to hundreds of
thousands of variables were then successfully solved.
Acknow ledgements
This work was supported partly by AFOSR-F49620-92-J-0465 and the Yale Center
of Theoretical and Applied Neurosci ence.

60000

639

A Multiscale Attentional Framework for Relaxation Neural Networks
10'

(128t2) : Focus on 1st level - proposed saliency

10 '

.. ..

8
~

o

8

,.

00
00

10?

"",00

10'

0

~ 10-'
.!!

o
o

o

0

,.,10-'
o

o

o

..""

o

o

~

0
0
0

8 o 8

~0o;
/.0

: 10'

""

0.

c
.!!
OJ
11l 10-3

0

3

..
o

0
0

o

:0

.8-

0

10'

o

o

:0

(128t2) : Focus on 1st level - absolute gradient

.

0

""1:,00 0
0""

~
c
.!!

. .0

.

~10-1

~

~

!10- 4

10-'

""I
10 -~0~-.-'-'-'u.tl~Oo.,- J....Ll.J""!'1*=0-.-'-'-~1
O:=.-.l.....L..Ll.';t'!loOr-'-~.tO!:.-r-u~1~0-:r'-'~
100

(Average Della-E per block)

10-~0b.--'""-U.~I~""ol_:r'-'.w.m~I""O~
I _r-u.li;~
lo.L_:r'-'-,-""~uI""O~I_?.-'-'-l.l..lLU~l
ulo,,,""
_? .l.....L..Lu.;I""~ol-cr'-'-~
1 00
(Average Della-E per block)

Figure 3: Saliency Comparison: (left), saliency as in eq. (1); (right), the absolute
gradient was used instead.
References
A. Conn , N. Gould, A. Sartanaer, & Ph . Toint. (1993) Global Convergence of a
Class of Trust Region Algorithms for Optimization Using Inexact Projections on
Convex Constraints. SIAM J. of Optimization, 3(1) :164-221.
R. Durbin & D. Willshaw. (1987) An Analogue Approach to the TSP Problem
Using an Elastic Net Method. Nature , 326:689-691.
J. Hopfield & D. W. Tank. (1985) Neural Computation of Decisions in Optimization
Problems. Bioi. Cybernei., 52:141-152.

C. Koch , J. Marroquin & A. Yuille. (1986) Analog 'Neuronal ' Networks in Early
Vision . Proc . of the National Academy of Sciences USA, 83:4263-4267.
C . Koch, & S. Ullman . (1985) Shifts in Selective Visual Attention : Towards the
Underlying Neural Circuitry. Human Neurobiology , 4 :219-227 .
E. Mjolsness, C. Garrett, & W. Miranker. (1991) Multiscale Optimization in Neural
Nets. IEEE Trans. on Neural Networks , 2(2):263-274 .
E. Mjolsness & W. Miranker. (1993) Greedy Lagrangians for Neural Networks:
Three Levels of Optimization in Relaxation Dynamics. YALEU/DCS/TR-945.
(URL file:!!cs.ucsd.edu!pub!emj!papers!yale-TR-945.ps.Z)
B. Olshausen, C. Anderson, & D. Van Essen. (1993) A Neurobiological Model of
Visual Attention and Invariant Pattern Recognition Based on Dynamic Routing of
Information. The Journal of Neuroscience , 13(11):4700-4719 .
D. Terzopoulos. (1986) Regularization of Inverse Visual Problems Involving Discontinuities. IEEE Trans. PAMI, 8:419-429 .
D. I. Tsioutsias & E. Mjolsness. (1995) Global Optimization in Neural Nets: A
Novel Relaxation Framework . To appear as a UCSD-CSE-TR, Dec. 1995.

"
1023,1995,Correlated Neuronal Response: Time Scales and Mechanisms,,1023-correlated-neuronal-response-time-scales-and-mechanisms.pdf,Abstract Missing,"Correlated Neuronal Response:
Time Scales and Mechanisms
Wyeth Bair
Howard Hughes Medical Inst.
NYU Center for Neural Science
4 Washington PI., Room 809
New York, NY 10003

Ehud Zohary
Dept. of Neurobiology
Institute of Life Sciences
The Hebrew University, Givat Ram
Jerusalem, 91904 ISRAEL

Christof Koch
Computation and Neural Systems
Caltech, 139-74
Pasadena, CA 91125

Abstract
We have analyzed the relationship between correlated spike count
and the peak in the cross-correlation of spike trains for pairs of simultaneously recorded neurons from a previous study of area MT
in the macaque monkey (Zohary et al., 1994). We conclude that
common input, responsible for creating peaks on the order of ten
milliseconds wide in the spike train cross-correlograms (CCGs),
is also responsible for creating the correlation in spike count observed at the two second time scale of the trial. We argue that
both common excitation and inhibition may play significant roles
in establishing this correlation.

1

INTRODUCTION

In a previous study of pairs of MT neurons recorded using a single extracellular
electrode, it was found that the spike count during two seconds of visual motion
stimulation had an average correlation coefficient of r = 0.12 and that this correlation could significantly limit the usefulness of pooling across increasingly large
populations of neurons (Zohary et aI., 1994). However, correlated spike count between two neurons could in principle occur at several time-scales. Correlated drifts

Correlated Neuronal Response: Time Scales and Mechanisms

69

in the excitability of the cells, for example due to normal biological changes or
electrode induced changes, could cause correlation at a time scale of many minutes. Alternatively, attentional or priming effects from higher areas could change
the responsivity of the cells at the time scale of an experimental trial. Or, as suggested here, common input that changes on the order of milliseconds could cause
correlation in spike count. The first section determines the time scale at which the
neurons are correlated by analyzing the relationship between the peak in the spike
train cross-correlograms (CCGs) and the correlation between the spike counts using
a construct we call the trial CCG. The second section examines temporal structure
that is indicative of correlated suppression of firing, perhaps due to inhibition, which
may also contribute to the spike count correlation.

2

THE TIME SCALE OF CORRELATION

At the time scale of the single trial, the correlation, r se, of spike counts x and y from
two neurons recorded during nominally identical two second stimuli was computed
using Pearson's correlation coefficient,
rse

=

E[xy] - ExEy
,
uxuy

(1)

where E is expected value and u 2 is variance. If spike counts are converted to
z-scores, i.e., zero mean and unity variance, then rse = E[xy], and rse may be
interpreted as the zero-lag value of the cross-correlation of the z-scored spike counts.
The trial CCGs resulting from this procedure are shown for two pairs of neurons in
Fig. l.
To distinguish between cases like the two shown in Fig. 1, the correlation was broken
into a long-term component, rlt, the average value (computed using a Gaussian
window of standard deviation 4 trials) surrounding the zero-lag value, and a shortterm component, rst, the difference between the zero-lag value and rlt. Across 92
pairs of neurons from three monkeys, the average rst was 0.10 (s.d. 0.17) while rlt
was not significantly different from zero (mean 0.01, s.d. 0.11). The mean of rst
was similar to the overall correlation of 0.12 reported by Zohary et al. (1994).
Under certain assumptions, including that the time scale of correlation is less than
the trial duration, rst can be estimated from the area under the spike train CCG
and the areas under the autocorrelations (derivation omitted). Under the additional
assumption that the spike trains are individually Poisson and have no peak in the
autocorrelation except that which occurs by definition at lag zero, the correlation
coefficient for spike count can be estimated by
rpeak

~ j.AA.ABArea,

(2)

where .AA and .AB are the mean firing rates of neurons A and B, and Area is the area
under the spike train CCG peak, like that shown in Fig. 2 for one pair of neurons.
Taking Area to be the area under the CCG between ?32 msec gives a good estimate
of short-term rst, as shown in Fig. 3. In addition to the strong correlation (r = 0.71)
between rpeak and rst, rpeak is a less noisy measure, having standard deviation (not
shown) on average one fourth as large as those of rst.
We conclude that the common input that causes the peaks in the spike train CCGs is
also responsible for the correlation in spike count that has been previously reported.

W. BAIR. E. ZOHARY. C. KOCH

70

o

80

160

240

320 0

Trial Number

400

800

1200

Trial Number

0.3
0.2

d
U
U 0.1
"";3
.~

~

0

~~------------------~~

ernu090
-0.1 +'-r-~..:...-,:......-~-.-,-~~---,-,~,.--,--,
-100
-50
0
50
100 -50
-25

Lag (Trials)

0

25

50

Lag (Trials)

Figure 1: Normalized responses for two pairs of neurons and their trial crosscorrelograms (CCGs). The upper traces show the z-scored spike counts for all
trials in the order they occurred. Spikes were counted during the 2 sec stimulus,
but trials occurred on average 5 sec apart, so 100 trials represents about 2.5 minutes. The lower traces show the trial CCGs. For the pair of cells in the left panel,
responsivity drifts during the experiment. The CCG (lower left) shows that the drift
is correlated between the two neurons over nearly 100 trials. For the pair of cells
in the right panel, the trial CCG shows a strong correlation only for simultaneous
trials. Thus, the measured correlation coefficient (trial CCG at zero lag) seems to
occur at a long time scale on the left but a short time scale (less than or equal to one
trial) on the right. The zero-lag value can be broken into two components, T st and
Tlt (short term and long term, respectively, see text). The short-term component,
T st, is the value at zero lag minus the weighted average value at surrounding lag
times. On the left, Tst ~ 0, while on the right, Tlt ~ O.

Correlated Neuronal Response: Time Scales and Mechanisms

71

5

o

8

16

24

32

Width at HH (msec)

1

o

emu064P

-100

o

-50

50

100

Time Lag (msec)
Figure 2: A spike train CCG with central peak. The frequency histogram of widths
at half-height is shown (inset) for 92 cell pairs from three monkeys. The area of the
central peak measured between ?32 msec is used to predict the correlation coefficients, rp eak. plotted in Fig. 3. The y-axis indicates the probability of a coincidence
relative to that expected for Poisson processes at the measured firing rates .

0.8

?

0.6
~

~
(1)
~

'-""
~

? ?
? ?

0.4

?

?
0.2 ?

?

.....

? ?

?

?

? ?
?

?

?

?

0 ?
??
-0.2
-0.2

o

0.2

0.4

0.6

0.8

r (Short Term)
Figure 3: The area of the peak of the spike train CCG yields a prediction, rpeak (see
Eqn. 2) , that is strongly correlated (r = 0.71, p < 0.00001), with the short-term
spike count correlation coefficient , rst . The absence of points in the lower right
corner of the plot indicates that there are no cases of a pair of cells being strongly
correlated without having a peak in the spike train CCG.

w. BAIR, E. ZOHARY, C. KOCH

72

In Fig. 3, there are no pairs of neurons that have a short-term correlation and yet
do not have a peak in the ?32 msec range of the spike train CCG.

3

CORRELATED SUPPRESSION

There is little doubt that common excitatory input causes peaks like the one shown
in Fig. 2 and therefore results in the correlated spike count at the time scale of the
trial. However, we have also observed correlated periods of suppressed firing that
may point to inhibition as another contribution to the CCG peaks and consequently
to the correlated spike count.
Fig. 4 A and B show the response of one neuron to coherent preferred and null
direction motion, respectively. Excessively long inter-spike intervals (ISIs), or gaps,
appear in the response to preferred motion, while bursts appear in the response
to null motion. Across a database of 84 single neurons from a previous study
(Britten et aI., 1992), the occurrence of the gaps and bursts has a symmetrical
time course-both are most prominent on average from 600-900 msec post-stimulus
onset, although there are substantial variations from cell to cell (Bair, 1995). The
gaps, roughly 100 msec long, are not consistent with the slow, steady adaptation
(presumably due to potassium currents) which is observed under current injection
in neocortical pyramidal neurons, e.g., the RS 1 and RS2 neurons of Agmon and
Connors (1992).
Fig. 4 C shows spike trains from two simultaneously recorded neurons stimulated
with preferred direction motion. The longest gaps appear to occur at about the
same time. To assess the correlation with a cross-correlogram, we first transform
the spike trains to interval trains, shown in Fig. 4 D for the spike trains in C.
This emphasizes the presence of long ISIs and removes some of the information
regarding the precise occurrence times of action potentials. The interval crosscorrelation (ICC) between each pair of interval trains is computed and averaged
over all trials, and the average shift predictor is subtracted. Fig. 4 E and F show
ICCs (thick lines) for two different pairs of neurons. In 17 of 31 pairs (55%), there
were peaks in the raw ICC that were at least 4 standard errors above the level of the
shift predictor. The peaks were on average centered (mean 4.3 msec, SD 54 msec)
and had mean width at half-height of 139 msec (SD 59 msec).
To isolate the cause of the peaks, the long intervals in the trains were set to the
mean of the short intervals. Long intervals were defined as those that accounted
for 30% of the duration of the data and were longer than all short intervals. Note
that this is only a small fraction of the number of ISIs in the spike train (typically
less than about 10%), since a few long intervals consume the same amount of time
as many short intervals. Data from 300-1950 msec was processed, avoiding the
on-transient and the lack of final interval. With the longest intervals neutralized,
the peaks were pushed down to the level of the noise in the ICC (thin lines, Fig. 4
E, F). Thus, 90% of the action potentials may serve to set a mean rate, while a few
periods of long ISIs dominate the ICC peaks.
The correlated gaps are consistent with common inhibition to neurons in a local
region of cortex, and this inhibition adds area to the spike train CCG peaks in
the form of a broader base (not shown). The data analyzed here is from behaving animals, so the gaps may be related to small saccades (within the 0.5 degree

73

Correlated Neuronal Response: Time Scales and Mechanisms

""""11""1'""''''''""'

""""'11111""'""''

1111 """"""111111111111""'""""
11111'""""111111111""11
""""II.!IIIIIIIIIIII
""11""""""1111111110111111111111111111111""""111'1
II
11111111'""11111111111111111111
111111111'""'""111 IIItIlI!
'""IIIIIII!!I1I11'""1I1I 1I1""'""tll """"UIU
""""'""'""
'""""""""

111""'.""111111111111

III
1111111 '""'
IlIg"" "" ' 111111111111111""11111
111111 1111111111'""1111111 1111111111

111'""""""""""1111

1111""111'""'""1111111

'""""""""""'""11'""111111'
11111111'""1111111111'""'""""""'""'
11""""'""1111111
II'
IIIIIU""'""IIIIIIIIIII.II'
""'''''1111111111 11,.""""""""'""
111111111111111'

1111111111111111111'""'""11111""11111111'11""11""11111111111""1 ' "" ' ' ' ' ' ""
1111111111111""'""""1111111 ""'""111
'""'""1111111'""111111111111111'

I

I

""""1111111111.11. IIUII

111'''.,11111111111111111
1111111'111111""""""11
1111

I

""11'"""" 11111111""1111'""111 111""""
""""""'""111 """" II 111111"""""" 1111'
1111 111'11'111111111 I III 1M II , ' ' ' '
I""
! III
'''.1'111111111111111,,11111111111111111111111111111111111111111111''""1111""
I 11111
I l "" l I l l ! III
1111

""11""11

A

""I

1111111 II! I 1111 III II! I ! ! "" I I! "" ' "" ' "" III III. II 11.11 III II
111M"""" tI III. "" 111""111 I ? "" I """"I I I 11111
.11111111111111 II
11111111111111'"",
""""""""""""""'""111'""1111111111""1""1
""11'11111'1111111111
1111
1111.1.11111111 '""'1 II!!' I I ,,'''''HIII'''!I1''!
""""
11111111111""1
""'""111'""'"" I I I ' "" ! I I "" I II!
1111111.111111111111111111111'
11111111""""'""1111111111111'1111111111 '1IIIII'""IIIIIIII""""""""""""YlI.""""111I

11""""

,""""'',''1''','' ,,'.',
II

I
I
II

I

'I

?
""
?

I
I

""'I I

ta'

I

III

N 1

.111

III

i

II

0

1111
I

"""" ""'

I I

III

I

""11

.1

II
I I

II

'""~

II
II

I""

""'

I'""

1111

""II'""'~IIIIII,II!'""'''',,\

III

,.

II

II

""I II !

I!
11

II.

t""""'IIIIII

'11I'1~I'M'i""IlI'III""'I,IIII""II'""III,III'

I

11111

II

'111'

""'

I

500

1000

B

, I

?

'I!

I'

III

""""
I'

III

1500

2000

msec

1
2

II

11111111111111 1111111111111111 11111111 II 01 n11111111 11111111111111111111111111111111111111111111111111118 11111
11.1111 1111
1111 1 11111111 111111
I 1I11 III I 11I1111I111I111I111I111111I11Im1l11111 II

c

1

D
1000

2000

Time (msec)

E

-1000 -500

F

o

500

1000 -1000 -500

o

500

1000

Time Lag (msec)
Figure 4: (A) The brisk response to coherent preferred direction motion is interrupted by occasional excessively long inter-spike intervals, i.e., gaps. (B) The
suppressed response to null direction motion is interrupted by bursts of spikes. (C)
Simultaneous spike trains from two neurons show correlated gaps in the preferred
direction response. (D) The interval representation for the spike trains in C. (E,F)
Interval cross-correlograms have peaks indicating that the gaps are correlated (see
text).

w. BAIR. E. ZOHARY. C. KOCH

74

fixation window) or eyelid blink. It has been hypothesized that blink suppression
and saccadic visual suppression may operate through the same pathways and are
of neuronal origin (Ridder and Tomlinson, 1993). An alternative hypothesis is that
the gaps and bursts arise in cortex from intrinsic circuitry arranged in an opponent
fashion.

4

CONCLUSION

Common input that causes central peaks on the order of tens of milliseconds wide in
spike train CCGs is also responsible for causing the correlation in spike count at the
time scale of two second long trials. Long-term correlation due to drifts in responsivity exists but is zero on average across all cell pairs and may represent a source of
noise which complicates the accurate measurement of cell-to-cell correlation. The
area of the peak of the spike train CCG within a window of ?32 msec is the basis of
a good prediction of the spike count correlation coefficient and provides a less noisy
measure of correlation between neurons. Correlated gaps observed in the response
to coherent preferred direction motion is consistent with common inhibition and
contributes to the area of the spike train CCG peak, and thus to the correlation
between spike count. Correlation in spike count is an important factor that can
limit the useful pool-size of neuronal ensembles (Zohary et al., 1994; Gawne and
Richmond, 1993).
Acknowledgements

We thank William T. Newsome, Kenneth H. Britten, Michael N. Shadlen, and J.
Anthony Movshon for kindly providing data that was recorded in previous studies
and for helpful discussion. This work was funded by the Office of Naval Research
and the Air Force Office of Scientific Research. W. B. was supported by the L. A.
Hanson Foundation and the Howard Hughes Medical Institute.
References

Agmon A, Connors BW (1992) Correlation between intrinsic firing patterns and
thalamocortical synaptic responses of neurons in mouse barrel cortex. J N eurosci 12:319-329.
Bair W (1995) Analysis of Temporal Structure in Spike Trains of Visual Cortical
Area MT. Ph.D. thesis, California Institute of Technology.
Britten KH, Shadlen MN, Newsome WT, Movshon JA (1992) The analysis of visual
motion: a comparison of neuronal and psychophysical performance. J Neurosci
12:4745-4765.
Gawne T J, Richmond BJ (1993) How independent are the messages carried by
adjacent inferior temporal cortical neurons? J Neurosci 13:2758-2771.
Ridder WH, Tomlinson A (1993) Suppression of contrasts sensitivity during eyelid
blinks. Vision Res 33: 1795- 1802.
Zohary E, Shadlen MN, Newsome WT (1994) Correlated neuronal discharge rate
and its implications for psychophysical performance. Nature 370:140-143.

"
1024,1995,Onset-based Sound Segmentation,,1024-onset-based-sound-segmentation.pdf,Abstract Missing,"Onset-based Sound Segmentation

Leslie S. Smith
CCCN jDepartment of Computer Science
University of Stirling
Stirling FK9 4LA
Scotland

Abstract
A technique for segmenting sounds using processing based on mammalian early auditory processing is presented. The technique is
based on features in sound which neuron spike recording suggests
are detected in the cochlear nucleus. The sound signal is bandpassed and each signal processed to enhance onsets and offsets.
The onset and offset signals are compressed, then clustered both in
time and across frequency channels using a network of integrateand-fire neurons. Onsets and offsets are signalled by spikes, and
the timing of these spikes used to segment the sound.

1

Background

Traditional speech interpretation techniques based on Fourier transforms, spectrum
recoding, and a hidden Markov model or neural network interpretation stage have
limitations both in continuous speech and in interpreting speech in the presence
of noise, and this has led to interest in front ends modelling biological auditory
systems for speech interpretation systems (Ainsworth and Meyer 92; Cosi 93; Cole
et al 95).
Auditory modelling systems use similar early auditory processing to that used in
biological systems. Mammalian auditory processing uses two ears, and the incoming
signal is filtered first by the pinna (external ear) and the auditory canal before it
causes the tympanic membrane (eardrum) to vibrate. This vibration is then passed
on through the bones of the middle ear to the oval window on the cochlea. Inside
the cochlea, the pressure wave causes a pattern of vibration to occur on the basilar
membrane. This appears to be an active process using both the inner and outer hair
cells of the organ of Corti. The movement is detected by the inner hair cells and
turned into neural impulses by the neurons of the spiral ganglion. These pass down
the auditory nerve, and arrive at various parts of the cochlear nucleus. From there,
nerve fibres innervate other areas: the lateral and medial nuclei of the superior olive,

L.S.SMITH

730

and the inferior colliculus, for example. (See (Pickles 88)).
Virtually all modern sound or speech interpretation systems use some form of bandpass filtering, following the biology as far as the cochlea. Most use Fourier transforms to perform a calculation of the energy in each band over some time period,
usually between 25 and 75 ms. This is not what the cochlea does. Auditory modelling front ends differ in the extent and length to which they follow animal early
auditory processing, but the term generally implies at least that wideband filters
are used, and that high temporal resolution is maintained in the initial stages. This
means the use of filtering techniques. rather than Fourier transforms in the bandpass
stage. Such filtering systems have been implemented by Patterson and Holdsworth
(Patterson and Holdsworth 90; Slaney 93), and placed directly in silicon (Lazzaro
and Mead 89; Lazzaro et al 93; Liu et al 93; Fragniere and van Schaik 94).
Some auditory models have moved beyond cochlear filtering. The inner hair cell
has been modelled by either simple rectification (Smith 94) or has been based on
the work of (Meddis 88) for example (Patterson and Holdsworth 90; Cosi 93; Brown
92). Lazzaro has experimented with a silicon version of Licklider's autocorrelation
processing (Licklider 51; Lazzaro and Mead 89). Others such as (Wu et al 1989:
Blackwood et al1990; Ainsworth and Meyer 92; Brown 92; Berthommier 93; Smith
94) have considered the early brainstem nuclei, and their possible contribution,
based on the neurophysiology of the different cell types (Pickles 88; Blackburn and
Sachs 1989; Kim et al 90).
Auditory model-based systems have yet to find their way into mainstream speech
recognition systems (Cosi 93). The work presented here uses auditory modelling
up to onset cells in the cochlear nucleus. It adds a temporal neural network to
clean up the segmentation produced. This part has been filed as a patent (Smith
95). Though the system has some biological plausibility, the aim is an effective
data-driven segmentation technique implement able in silicon.

2

Techniques used

Digitized sound was applied to an auditory front end, (Patterson and Holdsworth
90), which bandpassed the sound into channels each with bandwidth 24.7{4.37Fr; +
I)Hz, where Fe is the centre frequency (in KHz) of the band (Moore and Glasberg
83). These were rectified, modelling the effect of the inner hair cells. The signals
produced bear some resemblance to that in the auditory nerve. The real system
has far more channels and each nerve channel carries spike-coded information. The
coding here models the signal in a population of neighboring auditory nerve fibres.

2.1

The onset-offset filter

The signal present in the auditory nerve is stronger near the onset of a tone than
later (Pickles 88). This effect is much more pronounced in certain cell types of the
cochlear nucleus. These fire strongly just after the onset of a sound in the band to
which they are sensitive, and are then silent. This emphasis on onsets was modelled
by convolving the signal in each band with a filter which computes two averages, a
more recent one, and a less recent one, and subtracts the less recent one from the
more recent one. One biologically possible justification for this is to consider that
a neuron is receiving the same driving input twice, one excitatorily, and the other
inhibitorily; the excitatory input has a shorter time-constant than the inhibitory
input. Both exponentially weighted averages, and averages formed using a Gaussian
filter have been tried (Smith 94), but the former place too much emphasis on the
most recent part of the signal, making the latter more effective.

731

Onset-based Sound Segmentation

The filter output for input signal s(x) is

O(t. k, 'f') =

lot (f(t - x, k) -

f(t - x, k/,r))s(x)dx

(1)

where f(x, y) = vY exp( -yx 2 ). k and 'r determine the rise and fall times of the
pulses of sOlmd that the system is sensitive to. We used A: = 1000, 'r = 1.2, so
that the SD of the Gaussians are 24.49ms and 22.36ms. The convolving filter has
a positive peak at O. crosses 0 at 22.39ms. and is then negative. With these values.
the system is sensitive to energy rises and falls which occm in the envelopes of
everyday sounds. A positive onset-offset signal implies that the bandpassed signal is
increasing in intensity, and a negative onset-offset signal implies that it is decreasing
in intensity. The convolution used is a sound analog of the difference of Gaussians
operator used to extract black/white and white/black edges in monochrome images
(MalT and Hildreth 80). In (Smith 94) we performed sOlmd segmentation directly
on this signal.

2.2

Compressing the onset-offset signal

The onset-offset signal was divided into two positive-going signals, an onset signal
consisting of the positive-going part, and an offset signal consisting of the inverted
negative-going part. Both were compressed logarithmically (where log(x) was taken
as 0 for 0 S x S 1). This increases the dynamical range of the system, and models
compressive biological effects. The compressed onset signal models the output of a
population of onset cells. This technique for producing an onset signal is related to
that of (Wu et al 1989: Cosi 93).

2.3

The integrate-and-fire neural network

To segment the sound using the onset and offset signals, they need to be integrated
across frequency bands and across time. This temporal and tonotopic clustering
was achieved using a network of integrate-and-fire units. An integrate-and-fire unit
accumulates its weighted input over time. The activity of the unit A. is initially O.
and alters according to
dA
(2)
- = I(t) - ""YA
dt
where I(t) is the input to the nemon and ""Y, the dissipation, describes the leakiness
of the integration. When A reaches a threshold. the unit fires (i.e. emits a pulse).
and A is reset to O. After firing, there is a period of insensitivity to input, called the
refractory period. Such nemons are discussed in. e.g. (Mirolla and Strogatz 90).
One integrate-and-fire neuron was used per charmel: this neuron received input either from a single charmel, or from a set of adjacent charmels. all with equal positive
weighting. The output of each neuron was fed back to a set of adjacent neurons,
again with a fixed positive weight, one time step (here 0.5ms) later. Because of the
leaky nature of the accumulation of activity, excitatory input to the neuron arriving
when its activation is near' threshold has a lar'ger effect on the next firing time than
excitatory input arriving when activation is lower. Thus, if similar input is applied
to a set of neurons in adjacent charmels. the effect of the inter-neuron connections
is that when the first one fires, its neighbors fire almost immediately. This allows
a network of such neurons to cluster the onset or offset signals, producing a sharp
burst of spikes across a number of charmels providing unambiguous onsets or offsets.
The external and internal weights of the network were adjusted so that onset or
offset input alone allowed neurons to fire, while internal input alone was not enough

L. S. SMITH

732

to cause firing. The refractory period used was set to 50ms for the onset system,
and 5ms for the offset system. For the onset system, the effect was to produce sharp
onset firing responses across adjacent channels in response to a sudden increase in
energy in some channels, thus grouping onsets both tonotopically and temporally.
This is appropriate for onsets, as these are generally brief and clearly marked. The
output of this stage we call the onset map. Offsets tend to be more gradual. This
is due to physical effects: for example, a percussive sound will start suddenly, as
the vibrating element starts to move. but die away slowly as the vibration ceases
(see (Gaver 93) for a discussion). Even when the vibration does stop suddenly. the
sound will die away more slowly due to echoes. Thus we cannot reliably mark the
offset of a sound: instead. we reduce the refractory period of the offset neurons, and
produce a train of pulses marking the duration of the offset in this channel. We call
the output of this stage the offset map.

3

Results

As the technique is entirely data-driven. it can be applied to sound from any source.
It has been applied to both speech and musical sounds. Figure 1 shows the effect
of applying the techniques discussed to a short piece of speech. Fig lc shows that
the neural network integrates the onset timings across the channels, allowing these
onsets to be used for segmentation. The simplest technique is to divide up the
continuous speech at each onset: however,to ensure that the occasional onset in a
single channel does not confuse the system. and that onsets which occur near to
each other do not result in very short segments we demanded that a segmentation
boundary have at least 6 onsets inside a period of lOms. and the minimum segment
length was set to 25ms.
The utterance Ne'Uml information processing systems has phonetic representation:
/ njtlrl: anfarmeIan prosc:salJ ststalllS /
and is segmented into the following 19 segments:
/n/, jtl/, /r/, /la/, /a/, /nf/. /arm/, /e/,
/t/, /st/, /am/, /s/

/I/,

/an/, /pro/, /os/, /c:s/ , /aIJ/, /s/,

The same text spoken more slowly (over 4.38s, rather than 2.31s) has phonetic
representation:
/ njural:anftrmeIanprosc:stIJ ststams /
Segmenting using this technique gives the following 25 segments:
/n/ , /ju/ , /u/, /r/. /a/ , /al/, /1/, / /, /an/, /f/ , /um/, /e/,
/ro/, /os/, /c:s/, /tIJ/, /s/, /t/ , /st/, /am/, /s/

/I/.

/an/, /n:/, /pr/,

Although some phonemes are broken between segments, the system provides effective segmentation, and is relatively insensitive to speech rate. The system is also
effective at finding speech inside certain types of noise (such as motor-bike noise) ,
as can be seen in fig Ie and f.
The system has been used to segment sound from single musical instruments. Where
these have clear breaks between notes this is straightforward: in (Smith 94) correct
segmentation was achieved directly from the onset-offset signal but was not achieved
for slurred sounds, in which the notes change smoothly. As is visible in figure 2c,
the onsets here are clear using the network, and the segmentation produced is near-

733

Onset-based Sound Segmentation

[E]J

:'.

...
...
'""

GJ . . .
,

..

..

""

. ... .... ..

'""\N',,,,,,.J/'I~'/w,. -""?'''''''''''~v..flt'''''I/!fII~~...A..it.-./'~f\It'''v,i~~~~';''o~\-J..J{iII''r ~'if'I{I/'}/i'...J""'''

~ 'W'hlJ.,j.~~..f""'\/' JJ.A~ ""'v.""""./fI/<II'~rJ~ M '\-'.'~'f..""""""v/

""'I'jNflNlI/V

'1'#.~N~I{'f!II/W/W ?"" /"")~,\/,,

.. ; . .
'

Figure 1: (a-d):Onset and Offset maps from author saying Neural information processing systems rapidly. a: envelope of original sound. b: onset map. from 28
channels. from 100Hz-6KHz. Onset filter parameters as in text; one neuron per
channel, with no interconnection. Neuron refractory period is 50ms. c: as b , but
network has input applied to 6 adjacent channels, and internal feedback to 10 channels. d: offset map produced similarly, with refractory period 5ms. e: envelope of
say, that's a nice bike with motorbike noise in background (lines mark utterance).
f, g: onset, offset maps for e.

perfect. Best results were obtained here when the input to the network is not spread
across channels.

4

Conclusions and further work

An effective data driven segmentation technique based on onset feature detection
and using integrate-and-fire neurons has been demonstrated. The system is relatively immune to broadband noise. Segmentation is not an end in itself: the
effectiveness of any technique will depend on the eventual application.

L. S.SMITH

734

..'~----

.'

Figure 2: a: slurred flute sound. with vertical lines showing boundary between
notes. b: onsets found using a single neuron per channel, and no interconnection.
c: as b, but with internal feedback from each channel to 16 adjacent channels d:
offsets found with refractory period 5ms.

The segmentation is currently not using the information on which bands the onsets
occur in. We propose to extend this work by combining the segmentation described
here with work streaming bands sharing same-frequency amplitude modulation.
The aim of this is to extract sound segments from some subset of the bands, allowing
segmentation and streaming to run concurrently.

Acknowledgements
Many thanks are due to the members of the Centre for Cognitive and Computational
Neuroscience at the University of Stirling.

References
Ainsworth W. Meyer G. Speech analysis by means of a physiologically-based model
of the cochlear nerve and cochlear nucleus. in Visual r'e presentations of speech
signals. Cooke M. Beet S. eds. 1992.
Berthommier F .. Modelling nelll'rul'eSpOllSes of t.he int.ermediate auditory system, in
Mathematics applied to biology and medicine, Demongeot .I, Capa..'!so V, Wuertz
Publishing, Canada, 1993.
Blackburn C.C .. Sachs M.B. Classification of unit types in the anteroventral cochlear
nucleus: PST hist.ograms and regularity analysis, . J. Neurophys'iology, 62, 6,
1989.

Onset-based Sound Segmentation

735

Blackwood N .. Meyer G., Aimsworth W. A Model of the processing of voiced plosives
in the auditory nerve and cochlear nucleus, Proceedings Inst of Acoustics, 12,
10, 1990.
Brown G. Computational Auditory Scene Analysis, TR CS-92-22, Department of
Computing Science, University of Sheffield, England, 1992.
Cole R .. et al, The challenge of spoken language systems: research directions of the
90's. IEEE Trans Speech and Audio Pmcessing, 3. 1, 1995.
Cosi P. On the use of auditory models in speech technology, in Intelligent Perceptual
Models, LNCS 745, Springer Verlag, 1993.
Fragniere E., van Schaik A .. Lineal' predictive coding of the speech signal using an
analog cochlear modeL MANTRA Internal Report, 94/2, MANTRA Center for
Neuro-mimetic systems, EPFL, Lausanne, Switzerland, 1994.
Gaver W.W. What in the world do we hear?: an ecological approach to auditory
event perception. Ecological Psychology, 5(1). 1-29, 1993.
Kim D.O. ,Sirianni .T.G., Chang S.O .. Responses of DCN-PVCN neurons and auditory nerve fibres in lmanesthetized decerebrate cats to AM and pure tones:
analysis with autocorrelation/power-spectrum, Hearing Research. 45, 95-113.
1990.
Lazzaro .T., Mead C., Silicon modelling of pitch perception, Proc Natl. Acad Sciences, USA, 86. 9597-9601, 1989.
Lazzaro .T., Wawrzynek .T .. Mahowald M. , Sivilotti M., Gillespie D .. Silicon auditory
processors as computer peripherals. IEEE Trans on Neural Networks, 4, 3, May
1993.
Licklider .T.C.R, A Duplex theory of pitch perception, Experentia, 7. 128-133, 1951.
Liu W .. Andreou A.G., Goldstein M.H., Analog cochlear model for multiresolution
speech analysis, Advances in Neural Information Processing Systems 5, Hanson
S ..T., Cowan .T.D., Lee Giles C. (eds), Morgan Kaufmann, 1993.
Marl' D., Hildreth E. Theory of edge detection, Proc. Royal Society of London B,
207. 187-217, 1980.
Meddis R .. Simulation of auditory-neural transduction: further studies. J. Acollst
Soc Am. 83. 3, 1988.
Moore B.C ..J.. Glasberg B.R. Suggested formulae for calculating auditory-filter
bandwidths and excitation patterns, J Acoust Soc America, 74. 3, 1983.
Mirollo RE. , Strogatz S.H. Synchronization of pulse-coupled biological oscillators,
SIAM J. Appl Math, 50, 6, 1990.
Patterson R. Holdsworth .T. (1990). An Introd'IJ,ction to A1J,ditory Sensation Processing. in AAM HAP. Vol 1. No 1.
Pickles .T.O. (1988). An Introd'u ction to the PhyS'iology of Hearing, 2nd Edition,
Academic Press.
Slaney M .. An efficient implementation of the Patterson-Holdsworth auditory filter
bank, Apple technical report No 35, Apple Computer Inc, 1993.
Smith L.S. SO\illd segmentation using onsets and offsets, J of New Music Research,
23, 1, 1994.
Smith L.S. Onset/offset coding for interpretation and segmentation of sound, UK
patent no 9505956.4. March 1995.
Wu Z.L., Schwartz .T.L .. Escudier P. A theoretical study of neural mechanisms
specialized in the detection of articulatory-acoustic events, Proc Eurospeech
89. ed Tubach .T.P., Mariani .T ..T., Paris, 1989.

"
1025,1995,A model of transparent motion and non-transparent motion aftereffects,,1025-a-model-of-transparent-motion-and-non-transparent-motion-aftereffects.pdf,Abstract Missing,"A model of transparent motion and
non-transparent motion aftereffects

Alexander Grunewald*
Max-Planck Institut fur biologische Kybernetik
Spemannstrafie 38
D-72076 Tubingen, Germany

Abstract
A model of human motion perception is presented. The model
contains two stages of direction selective units. The first stage contains broadly tuned units, while the second stage contains units
that are narrowly tuned. The model accounts for the motion aftereffect through adapting units at the first stage and inhibitory
interactions at the second stage. The model explains how two populations of dots moving in slightly different directions are perceived
as a single population moving in the direction of the vector sum,
and how two populations moving in strongly different directions are
perceived as transparent motion. The model also explains why the
motion aftereffect in both cases appears as non-transparent motion.

1

INTRODUCTION

Transparent motion can be studied using displays which contain two populations of
moving dots. The dots within each population have the same direction of motion,
but directions can differ between the two populations. When the two directions are
very similar, subjects report seeing dots moving in the average direction (Williams &
Sekuler, 1984). However, when the difference between the two directions gets large,
subjects perceive two overlapping sheets of moving dots. This percept is called
transparent motion. The occurrence of transparent motion cannot be explained by
direction averaging, since that would result in a single direction of perceived motion.
Rather than just being a quirk of the human visual system, transparent motion is
an important issue in motion processing. For example, when a robot is moving its
? Present address: Caltech, Mail Code 216-76, Pasadena, CA 91125.

838

A. GRUNEWALD

motion leads to a velocity field. The ability to detect transparent motion within
that velocity field enables the robot to detect other moving objects at the same time
that the velocity field can be used to estimate the heading direction of the robot.
Without the ability to code mUltiple directions of motion at the same location,
i.e. without the provision for transparent motion, this capacity is not available.
Traditional algorithms have failed to properly process transparent motion, mainly
because they assigned a unique velocity signal to each location, instead of allowing
the possibility for multiple motion signals at a single location. Consequently, the
study of transparent motion has recently enjoyed widespread interest.

STIMULUS

PERCEPT
Test

Figure 1: Two populations of dots moving in different directions during an adaptation phase are perceived as transparent motion. Subsequent viewing of randomly
moving dots during a test phase leads to an illusory percept of unidirectional motion,
the motion aftereffect (MAE). Stimulus and percept in both phases are shown.
After prolonged exposure to an adaptation display containing dots moving in one direction, randomly moving dots in a test display appear to be moving in the opposite
direction (Hiris & Blake, 1992; Wohlgemuth, 1911). This illusory percept of motion
is called the motion aftereffect (MAE). Traditionally this is explained by assuming
that pairs of oppositely tuned direction selective units together code the presence
of motion. When both are equally active, no motion is seen. Visual motion leads
to stronger activation of one unit, and thus an imbalance in the activity of the two
units. Consequently, motion is perceived. Activation of that unit causes it to fatigue, which means its response weakens. After motion offset, the previously active
unit sends out a reduced signal compared to its partner due to adaptation. Thus
adaptation generates an imbalance between the two units, and therefore illusory
motion, the MAE, is perceived. This is the ratio model (Sutherland, 1961).
Recent psychophysical results show that after prolonged exposure to transparent
motion, observers perceive a MAE of a single direction of motion, pointing in the
vector average of the adaptation directions (Mather, 1980; Verstraten, Fredericksen, & van de Grind, 1994). Thus adaptation to transparent motion leads to a
non-transparent MAE. This is illustrated in Figure 1. This result cannot be accounted for by the ratio model, since the non-transparent MAE does not point in
the direction opposite to either of the adaptation directions. Instead, this result
suggests that direction selective units of all directions interact and thus contribute
to the MAE. This explanation is called the distribution-shift model (Mather, 1980).
However, thus far it has only been vaguely defined, and no demonstration has been
given that shows how this mechanism might work.

A Model of Transparent Motion and Non-transparent Motion Aftereffects

839

This study develops a model of human motion perception based on elements from
both the ratio and the distribution-shift models for the MAE. The model is also
applicable to the situation where two directions of motion are present. When the
directions differ slightly, only a single direction is perceived. When the directions
differ a lot, transparent motion is perceived. Both cases lead to a unitary MAE.

2

OUTLINE OF THE MODEL

The model consists of two stages. Both stages contain units that are direction
selective. The architecture of the model is shown in Figure 2.
~----~--~,~r---~---'---\

Stage 2

CD080CD

-,

86)

+----+~--+~--~----~--~--~--~~

Figure 2: The model contains two stages of direction selective units. Units at stage
1 excite units of like direction selectivity at stage 2, and inhibit units of opposite
directions. At stage 2 recurrent inhibition sharpens directional motion responses.
The grey level indicates the strength of interaction between units. Strong influence
is indicated by black arrows, weak influence is indicated by light grey arrows.
Units in stage 1 are broadly tuned motion detectors. In the present study the precise
mechanism of motion detection is not central, and hence it has not been modeled. It
is assumed that the bandwidth of motion detectors at this stage is about 30 degrees
(Raymond, 1993; Williams, Tweten, & Sekuler, 1991). In the absence of any visual
motion, all units are active at a baseline level; this is equivalent to neuronal noise.
Whenever motion of a particular direction is present in the input, the activity of
the corresponding unit (Vi) is activated maximally (Vi = 9), and units of similar
direction selectivity are weakly activated (Vi = 3). The activities of all other units
decrease to zero. Associated with each unit i at stage 1 is a weight Wi that denotes
the adaptational state of unit i to fire a unit at stage 2. During prolonged exposure
to motion these weights adapt, and their strength decreases. The equation governing
the strength of the weights is given below:
dWi

- dt = R(1- w?)
~

V?W
?
~~,

where R = 0.5 denotes the rate of recovery to the baseline weight. When Wi = 1
the corresponding unit is not adapted. The further Wi is reduced from 1, the more

840

A. GRUNEWALD

the corresponding unit is adapted. The products ViWi are transmitted to stage 2.
Each unit of stage 1 excites units coding similar directions at stage 2, and inhibits
units coding opposite directions of motion. The excitatory and inhibitory effects
between units at stages 1 and 2 are caused by kernels, shown in Figure 3.
Feedback kernels

Feedforward kernels
1

-

0.8

-

0.6

0.6

-

0.4

0.4 r-

-

0.2

0.2 f--

-

1

excitatory
0.8

-------

I

1excitatory
inhibitory

---~---

inhibitory

0

--- -- --180

-----0

180

0

-

------+

--------

o

180

-180

Figure 3: Kernels used in the model. Left: excitatory and inhibitory kernels between
stages 1 and 2; right: excitatory and inhibitory feedback kernels within stage 2.
Activities at stage 2 are highly tuned for the direction of motion. The broad activation of motion signals at stage 1 is directionally sharpened at stage 2 through
the interactions between recurrent excitation and inhibition. Each unit in stage 2
excites itself, and interacts with other units at stage 2 through recurrent inhibition.
This inhibition is maximal for close directions, and falls off as the directions become more dissimilar. The kernels mediating excitatory and inhibitory interactions
within stage 2 are shown in Figure 3. Through these inhibitory interactions the
directional tuning of units at stage 2 is sharpened; through the excitatory feedback
it is ensured that one unit will be maximally active. Activities of units at stage 2
are given by Mi = max4 (mi' 0), where the behavior of mi is governed by:

F/ and Fi- denote the result of convolving the products of the activities at stage
1 and the corresponding adaptation level, VjWj , with excitatory and inhibitory
feedforward kernels respectively. Similarly, Bt and Bj denote the convolution of
the activities M j at stage 2 with the feedback kernels.

3

SIMULATIONS OF PSYCHOPHYSICAL RESULTS

In the simulations there were 24 units at each stage. The model was simulated
dynamically by integrating the differential equations using a fourth order RungeKutta method with stepsize H = 0.01 time units. The spacing of units in direction
space was 15 degrees at both stages. Spatial interactions were not modeled. In
the simulations shown, a motion stimulus is present until t = 3. Then the motion
stimulus ceases. Activity at stage 2 after t = 3 corresponds to a MAE.

A Model of Transparent Motion and Non-transparent Motion Aftereffects

3.1

841

UNIDIRECTIONAL MOTION

When adapting to a single direction of motion, the model correctly generates a
motion signal for that particular direction of motion. After offset of the motion
input, the unit coding the opposite direction of motion is activated, as in the MAE.
A simulation of this is shown in Figure 4.
Stage 1

Stage 2

act

act

360

360

Figure 4: Simulation of single motion input and resulting MAE. Motion input is
presented until t = 3.
During adaptation the motion stimulus excites the corresponding units at stage 1,
which in turn activate units at stage 2. Due to recurrent inhibition only one unit
at stage 2 remains active (Grossberg, 1973), and thus a very sharp motion signal
is registered at stage 2. During adaptation the weights associated with the units
that receive a motion input decrease. After motion offset, all units receive the same
baseline input. Since the weights of the previously active units are decreased, the
corresponding cells at stage 2 receive less feedforward excitation. At the same time,
the previously active units receive strong feedforward inhibition, since they receive
inhibition from units tuned to very different directions of motion and whose weights
did not decay during adaptation. Similarly, the units coding the opposite direction
of motion as those previously active receive more excitation and less inhibition.
Through recurrent inhibition the unit at stage 2 coding the opposite direction to that
which was active during adaptation is activated after motion offset: this activity
corresponds to the MAE. Thus the MAE is primarily an effect of disinhibition.

3.2

TRANSPARENT MOTION: SIMILAR DIRECTIONS

Two populations of dots moving in different, but very similar, directions lead to
bimodal activation at stage 1. Since the feedforward excitatory kernel is broadly
tuned, and since the directions of motion are similar, the ensuing distribution of
activities at stage 2 is unimodal, peaking halfway between the two directions of
motion. This corresponds to the vector average of the directions of motion of the
two populations of dots. A simulation of this is shown in Figure 5.
During adaptation the units at stage 1 corresponding to the input adapt. As before
this means that after motion offset the previously active units receive less excitatory
input and more inhibitory input. As during adaptation this signal is unimodal. Also,
the unit at stage 2 coding the opposite direction to that of the stimulus receives

842

A. GRUNEWALD

Stage 2

Stage 1

act

60 120

60 120 180
direction 240

180
direction 240

Figure 5: Simulation of two close directions of motion. Stage 2 of the network model
registers unitary motion and a unitary MAE.
less inhibition and more excitation. Through the recurrent activities within stage
2, that unit gets maximally activated. A unimodal MAE results.

3.3

TRANSPARENT MOTION: DIFFERENT DIRECTIONS

When the directions of the two populations of dots in a transparent motion display
are sufficiently distinct, the distribution of activities at stage 2 is no longer unimodal,
but bimodal. Thus, recurrent inhibition leads to activation of two units at stage 2.
They correspond to the two stimulus directions. A simulation is shown in Figure 6.
Stage 1

Stage 2

act

60 120
180
direction 240

Figure 6: Simulation of two distinct directions of motion. Stage 2 of the model
registers transparent motion during adaptation, but the MAE is unidirectional.
Feedforward inhibition is tuned much broader than feedforward excitation, and as a
consequence the inhibitory signal during adaptation is unimodal, peaking at the unit
of stage 2 coding the opposite direction of the average of the two previously active
directions. Therefore that unit receives the least amount of inhibition after motion
offset. It receives the same activity from stage 1 as units coding nearby directions,
since the corresponding weights at stage 1 did not adapt. Due to recurrent activities
at stage 2 that unit becomes active: non-transparent motion is registered.

A Model of Transparent Motion and Non-transparent Motion Aftereffects

4

843

DISCUSSION

Recently Snowden, Treue, Erickson, and Andersen (1991) have studied the effect
of transparent motion stimuli on neurons in areas VI and MT of macaque monkey.
They simultaneously presented two populations of dots, one of which was moving
in the preferred direction of the neuron under study, and the other population was
moving in a different direction. They found that neurons in VI were barely affected
by the second population of dots. Neurons in MT, on the other hand, were inhibited
when the direction of the second population differed from the preferred direction,
and inhibition was maximal when the second population was moving opposite to the
preferred direction. These results support key mechanisms of the model. At stage
1 there is no interaction between opposing directions of motion. The feedforward
inhibition between stages 1 and 2 is maximal between opposite directions. Thus
activities of units at stage 1 parallel neural activities recorded at VI, and activities
of units at stage 2 parallels those neural activities recorded in area MT.
Acknowledgments
This research was carried out under HFSP grant SF-354/94.

Reference
Grossberg, S. (1973). Contour enhancement, short term memory, and constancies in
reverberating neural networks. Studies in Applied Mathematics, LII, 213-257.
Hiris, E., & Blake, R. (1992). Another perspective in the visual motion aftereffect.
Proceedings of the National Academy of Sciences USA, 89, 9025-9028.
Mather, G. (1980). The movement aftereffect and a distribution-shift model for
coding the direction of visual movement. Perception, 9, 379-392.
Raymond, J. E. (1993). Movement direction analysers: independence and bandwidth. Vision Research, 33(5/6), 767-775.
Snowden, R. J ., Treue, S., Erickson, R. G., & Andersen, R. A. (1991). The response
of area MT and VI neurons to transparent motion. Journal of Neuroscience,
11 (9), 2768-2785.
Sutherland, N. S. (1961). Figural after-effects and apparent size. Quarterly Journal
of Experimental Psychology, 13, 222-228.
Verstraten, F. A. J., Fredericksen, R. E., & van de Grind, W. A. (1994). Movement
aftereffect of bi-vectorial transparent motion. Vision Research, 34, 349-358.
Williams, D., Tweten, S., & Sekuler, R. (1991). Using metamers to explore motion
perception. Vision Research, 31 (2), 275-286.
Williams, D. W., & Sekuler, R. (1984). Coherent global motion percept from
stochastic local motions. Vision Research, 24 (1), 55-62.
Wohlgemuth, A. (1911). On the aftereffect of seen movement. British Journal of
Psychology (Monograph Supplement), 1, 1-117.

"
1026,1995,A Model of Auditory Streaming,,1026-a-model-of-auditory-streaming.pdf,Abstract Missing,"A MODEL OF AUDITORY STREAMING
Susan L. McCabe & Michael J. Denham
Neurodynamics Research Group
School of Computing
University of Plymouth
Plymouth PL4 8AA, u.K.

ABSTRACT
An essential feature of intelligent sensory processing is the ability to
focus on the part of the signal of interest against a background of
distracting signals, and to be able to direct this focus at will. In this
paper the problem of auditory scene segmentation is considered and a
model of the early stages of the process is proposed. The behaviour of
the model is shown to be in agreement with a number of well known
psychophysical results. The principal contribution of this model lies in
demonstrating how streaming might result from interactions between
the tonotopic patterns of activity of input signals and traces of previous
activity which feedback and influence the way in which subsequent
signals are processed.

1 INTRODUCTION
The appropriate segmentation and grouping of incoming sensory signals is important in
enabling an organism to interact effectively with its environment (Llinas, 1991). The
formation of associations between signals, which are considered to arise from the same
external source, allows the organism to recognise significant patterns and relationships
within the signals from each source without being confused by accidental coincidences
between unrelated signals (Bregman, 1990). The intrinsically temporal nature of sound
means that in addition to being able to focus on the signal of interest, perhaps of equal
significance, is the ability to predict how that signal is expected to progress; such
expectations can then be used to facilitate further processing of the signal. It is important
to remember that perception is a creative act (Luria, 1980). The organism creates its
interpretation of the world in response to the current stimuli, within the context of its
current state of alertness, attention, and previous experience. The creative aspects of
perception are exemplified in the auditory system where peripheral processing
decomposes acoustic stimuli. Since the frequency spectra of complex sounds generally

A Model of Auditory Streaming

53

overlap, this poses a complicated problem for the auditory system : which parts of the
signal belong together, and which of the subgroups should be associated with each other
from one moment to the next, given the extra complication of possible discontinuities
and occlusion of sound signals? The process of streaming effectively acts to to associate
those sounds emitted from the same source and may be seen as an accomplishment,
rather than the breakdown of some integration mechanism (Bregman, 1990).
The cognitive model of streaming, proposed by (Bregman, 1990), is based primarily on
Gestalt principles such as common fate, proximity, similarity and good continuation.
Streaming is seen as a mUltistage process, in which an initial, preattentive process
partitions the sensory input, causing successive sounds to be associated depending on the
relationship between pitch proximity and presentation rate. Further refinement of these
sound streams is thought to involve the use of attention and memory in the processing of
single streams over longer time spans.
Recently a number of computational models which implement these concepts of
streaming have been developed. A model of streaming in which pitch trajectories are
used as the basis of sequential grouping is proposed by (Cooke, 1992). In related work,
(Brown, 1992) uses data-driven grouping schema to form complex sound groups from
frequency components with common periodicity and simultaneous onset. Sequential
associations are then developed on the basis of pitch trajectory. An alternative approach
suggests that the coherence of activity within networks of coupled oscillators, may be
interpreted to indicate both simultaneous and sequential groupings (Wang, 1995),
(Brown, 1995), and can, therefore, also model the streaming of complex stimuli. Sounds
belonging to the same stream, are distinguished by synchronous activity and the
relationship between frequency proximity and stream formation is modelled by the
degree of coupling between oscillators.
A model, which adheres closely to auditory physiology, has been proposed by (Beauvois,
1991). Processing is restricted to two frequency channels and the streaming of pure
tones. The model uses competitive interactions between frequency channels and leaky
integrator model neurons in order to replicate a number of aspects of human
psychophysical behaviour. The model, described here, used Beauvois' work as a starting
point, but has been extended to include multichannel processing of complex signals. It
can account for the relationship streaming and frequency difference and time interval
(Beauvois, 1991), the temporal development and variability of streaming perceptions
(Anstis, 1985), the influence of background organisation on foreground perceptions
(Bregman, 1975), as well as a number of other behavioural results which have been
omitted due to space limitations.

2

THEMODEL

We assume the existence of tonotopic maps, in which frequency is represented as a
distributed pattern of activity across the map. Interactions between the excitatory
tonotopic patterns of activity reflecting stimulus input, and the inhibitory tonotopic
masking patterns, resulting from previous activity, form the basis of the model. In order
to simulate behavioural experiments, the relationship between characteristic frequency
and position across the arrays is determined by equal spacing within the ERB scale
(Glasberg, 1990). The pattern of activation across the tonotopic axis is represented in
terms of a Gaussian function with a time course which reflects the onset-type activity
found frequently within the auditory system.

s. L. MCCABE, M. J. DENHAM

54

Input signals therefore take the form :
i(x,t) = CI (t - t Onset)e-c2(t-t Ortut )e 2~2lfc(x}-r.)2
[1]
where i(x.t) is the probability of input activity at position x, time t. C} and C; are
constants, tan.m is the starting time of the signal, fc (x) is the characteristic frequency at
position x,/. is the stimulus frequency, and a determines the spread of the activation.

In models where competitive interactions within a single network are used to model the
streaming process, such as (Beauvois, 1991), it is difficult to see how the organisation of
background sounds can be used to improve foreground perceptions (Bregman, 1975)
since the strengthening of one stream generally serves to weaken others. To overcome
this problem, the model of preattentive streaming proposed here, consists of two
interacting networks, the foreground and background networks, F and B; illustrated in
figure 1. The output from F indicates the activity, if any, in the foreground, or attended
stream, and the output from B reflects any other activity. The interaction between the
two eventually ensures that those signals appearing in the output from F, i.e. in the
foreground stream, do not appear in the output from B, the background; and vice versa.
In the model, strengthening of the organisation of the background sounds, results in the
'sharpening' of the foreground stream due to the enhanced inhibition produced by a more
coherent background.
rre

rnR

Figure 1 : Connectivity of the Streaming Networks.
Neurons within each array do not interact with each other but simply perform a
summation of their input activity. A simplified neuron model with low-pass filtering of
the inputs, and output representing the probability offiring, is used:
p(x, t) = cr[~ Vj(x, t)], where cr(y)

= I+~_Y

[2]

J

The inputs to the foreground net are :
VI (x,t) = (1- ::)VI (x,t-dt) + VI . ~(i(x,t?.dt

[3]

V2(X,t)

= (1- ::)V2(X, t- dt) + V2?mFi(x,t- dt?

.dt

[4]

V3(X, t)

= (1- ~)v3(x,t-dt) + V3 . ~(mB(x,t- dt?.dt

[5]

where x is the position across the array, time t, sampling rate dt. ""tj are time constants
which determine the rate of decay of activity, V; are weights on each of the inputs, and
t/J(y) is a function used to simulate the stochastic properties of nerve firing which returns
a value of J or 0 with probability y.

55

A Model of Auditory Streaming

The output activity pattern in the foreground net and its 'inverse', mF(x,f) and mFi(x,t),
are found by :
mF(x, t) = cr[v\ (x, t) -l'\(V2(X, t), n) -l'\(V3(X, t), n)]

[6]

N

mFi(x, t) = max {[~ ~ mF(xi' t - dt)] - mF(x, t- dt), O}

[7]

i=\

where 17(v(x,f),n) is the mean of the activity within neighbourhood n of position x at time
t and N is the number of frequency channels. Background inputs are similarly calculated.
To summarise, the current activity in response to the acoustic stimulus forms an
excitatory input to both the foreground and background streaming arrays, F and B. In
addition, F receives inhibitory inputs reflecting the current background activity, and the
inverse of the current foreground activity. The interplay between the excitatory and
inhibitory activities causes the model to gradually focus the foreground stream and
exclude extraneous stimuli. Since the patterns of inhibitory input reflect the distributed
patterns of activity in the input, the relationship between frequency difference and
streaming, results simply from the graded inhibition produced by these patterns. The
relationship between tone presentation rate and streaming is determined by the time
constants in the model which can be tuned to alter the rate of decay of activity.
To enable comparisons with psychophysical results, we view the judgement of coherence
or streaming made by the model as the difference between the strength of the foreground
response to one set of tones compared to the other. The strength of the response to a
given frequency, Resp(f,t), is a weighted sum of the activity within a window centred on
the frequency :
Respif, t) =

W

~

i=-W

mF(x(j) + i, t) * e

_k...
2(12

[8]

where W determines the size of the window centred on position, x(/), the position in the
map corresponding to frequency f, and a determines the spread of the weighting
function about position x(/).
The degree of coherence between two tones, say hand h' is assumed to depend on the
difference in strength of foreground response to the two :
C hif

o

I:
\,j 2,

t) = 1 _/ Resp(fj .t)--Resp{j2,t) /
Resp(fj . t}+Resp(/2,t)

[9]

where Coh(f;,h,t) ranges between 0, when Resp(f;,t) or Resp(h,t) vanishes and the
difference between the responses is a maximum, indicating maximum streaming, and 1,
when the responses are equal and maximally coherent. Values between these limits are
interpreted as the degree of coherence, analogous to the probability of human subjects
making ajudgement of coherence (Anstis, 1985), (Beauvois, 1991).

3

RESULTS

Experiments exploring the effect of frequency interval and tone presentation rate and
streaming are described in (Beauvois, 1991). Subjects were required to listen to an
alternating sequence of tones, ABABAB ... for 15 seconds, and then to judge whether at
the end of the sequence they perceived an oscillating, trill-like, temporally coherent
sequence, or two separate streams, one of interrupted high tones, the other of interrupted

s. L. MCCABE, M. J. DENHAM

56

low tones. Their results showed clearly an increasing tendency towards stream
segmentation both with increasing frequency difference between A and B, and
increasing tone presentation rate, results the model manages substantially to reproduce;
as 100r---~c_--~--~--------_,
may be seen in figure 2.
100r---~~--------~--~----'
4.76 tones/sec

I eo

60

~ 60
~

11

~

E20
0
1000

OL---~----~----~--~--~

1100

1200

1300

1<400

1500

1000

1100

1200

!

60

1500

7.69 tones/sec

5.88 tones/sec
~

1?>0

100~~~----------~--------'

100
80

1300

80

~

?

'li
~

?>

E 20
0
1000

20
oL---~----~----~--~----~

1100

1200

1?Xl

1500

1000

1100

1200

1300

1?>0

1500

100r---~----------~--~----'

100

20 tones/sec

11.11 tones/sec

?
S
? .co
l!

1300

80

~

60

1J
~

eo

~

20

20

o
1000

OL---~----~----~--~--~

1100

1200

1300

1?Xl

1500

1000

1100

1200

1300

1.ao

1500

Figure 2 : Mean Psychophysical '0' and Model ,*, Responses to the Stimulus ABAB ...
(A=lOOO Hz, B as indicated along X axis (Hz), tone presentation rates, as shown.)
In investigating the temporal development of stream segmentation, (Anstis, 1985) used a
similar stimulus to the experiment described above, but in this case subjects were
required to indicate continuously whether they were perceiving a coherent or streaming
signal. As can be seen in figure 3, the model clearly reproduces the principal features
found in their experiments, i.e. the probability of hearing a single, fused, stream declines
during each run, the more rapid the tone presentation rate, the quicker stream
segmentation occurs, and the judgements made were quite variable during each run.
In an experiment to investigate whether the organisation of the background sounds
affects the foreground, subjects were required to judge whether tone A was higher or
lower than B (Bregman, 1975). This judgement was easy when the two tones were
presented in isolation, but performance degraded significantly when the distractor tones,
X, were included. However, when a series of 'captor' tones, C, with frequency close to X
were added, the judgement became easier, and the degree of improvement was inversely
related to the difference in frequency between X and C. In the experiment, subjects
received an initial priming AB stimulus, followed by a set of 9 tones : CCCXABXCC.
The frequency of the captor tones, was manipulated to investigate how the proximity of
'captor' to 'distractor' tones affected the required AB order judgement.

57

A Model of Auditory Streaming

Figure 3 : The Probability of Perceptual Coherence as a Function of Time in Response
to Two Alternating Tones. Symbols: '.' 2 tones/s, '0' 4 tones/s, '+' 8 tones/so
In order to model this experiment and the effect of priming, an 'attentive' input, focussed
on the region of the map corresponding to the A and B tones, was included. We assume,
as argued by Bregman, that subjects' performance in this task is related to the degree to
which they are able to stream the AB pair separately. His D parameter is a measure of
the degree to which ABIBA can be discriminated. The model's performance is then
given by the strength of the foreground response to the AB pair as compared to the
distractor tones, and Coh([A B],X) is used to measure this difference. The model exhibits
a similar sensitivity to the distractor/captor frequency difference to that of human
subjects, and it appears that the formation of a coherent background stream allows the
model to distinguish the foreground group more clearly.

A)

B)' ~------~------~----~
09

t.4eM toherente XAB.X.

08

2S00

0)

N

06

I!OO

E

05
04
_____ . . _____ e -- --

G?????..??c?????????(???? ..?? ....?...._..........???co.... ?..??,

03 "" .. ? .. ?? .... &egm ..... Op....""...,.

02
01

o

500

' 000

'500

Cap'Of hoquoncy jHz)

TIME

Figure 4 : A) Experiment to Demonstrate the Formation ofMuItiple Streams,
(Bregman, 1975). B) Model Response; '?'Mean Degree of Doherence to XABX, '0',
Bregman's D Parameter, '+' Model's Judgement of Coherence.

4

DISCUSSION

The model of streaming which we have presented here is essentially a very simple one,
which can, nevertheless, successfully replicate a wide range of psychophysical
experiments. Embodied in the model is the idea that the characteristics of the incoming
sensory signals result in activity which modifies the way in which subsequent incoming

58

s. L. MCCABE, M. J. DENHAM

signals are processed. The inhibitory feedback signals effectively comprise expectations
against which later signals are processed. Processing in much of the auditory system
seems to be restricted to processing within frequency 'channels'. In this model, it is
shown how local interactions, restricted almost entirely to within-channel activity, can
form a global computation of stream formation. It is not known where streaming occurs
in the auditory system, but feedback projections both within and between nuclei are
extensive, perhaps allowing an iterative refinement of streams. Longer range projections,
originating from attentive processes or memory, may modify local interactions to
facilitate the extraction of recognised or interesting sounds.
The relationship between streaming and frequency interval, could be modelled by
systematically graded inhibitory weights between frequency channels. However, in the
model this relationship arises directly from the distributed incoming activity patterns,
which seems a more robust and plausible solution, particularly if one takes the need to
cope with developmental changes into account. Although to simplify the simulations
peripheral auditory processing was not included in the model, the activity patterns
assumed as input can be produced by the competitive processing of the output from a
cochlear model.
An important aspect of intelligent sensory processing is the ability to focus on signals of
interest against a background of distracting signals, thereby enabling the perception of
significant temporal patterns. Artificial sensory systems, with similar capabilities, could
act as robust pre-processors for other systems, such as speech recognisers, fault detection
systems, or any other application which required the dynamic extraction and temporal
linking of subsets of the overall signal.
Values Used For Model Parameters
a=.005, c)=75, c2=100, V=[lOO 5 5 5 5], T=[.05 .6 .6 .6 .6], n=2, N=lOO
References
Anstis, S., Saida, S., J. (1985) Exptl Psych, 11(3), pp257-271
Beauvois, M.W., Meddis, R (1991) J. Exptl Psych, 43A(3), pp517-541
Bregman, AS., Rudnicky, AI. (1975) J. ExptJ Psych, 1(3), pp263-267
Bregman, A.S. (1990) 'Auditory scene analysis', MIT Press
Brown, GJ. (1992) University of Sheffield Research Reports, CS-92-22
Brown, GJ., Cooke, M. (1995) submitted to IJCAI workshop on Computational
Auditory Scene Analysis
Cooke, M.P. (1992) Computer Speech and Language 6, pp 153-173
Glasberg, B.R., Moore, B.C.J. (1990) Hearing Research, 47, pp103-138
L1inas, RR, Pare, D. (1991) Neuroscience, 44(3), pp521-535
Luria, A (1980) 'Higher cortical functions in man', NY:Basic
van Noorden, L.P.AS. (1975) doctoral dissertation, published by Institute for Perception
Research, PO Box 513, Eindhoven, NL
Wang, D.L. (1995) in 'Handbook of brain theory and neural networks', MIT Press

PART II
NEUROSCIENCE

"
1027,1995,REMAP: Recursive Estimation and Maximization of A Posteriori Probabilities - Application to Transition-Based Connectionist Speech Recognition,,1027-remap-recursive-estimation-and-maximization-of-a-posteriori-probabilities-application-to-transition-based-connectionist-speech-recognition.pdf,Abstract Missing,"REMAP: Recursive Estimation and
Maximization of A Posteriori
Probabilities - Application to
Transition-Based Connectionist Speech
Recognition

Yochai Konig, Herve Bourlard~ and Nelson Morgan
{konig, bourlard,morgan }@icsi.berkeley.edu
International Computer Science Institute
1947 Center Street Berkeley, CA 94704, USA.

Abstract
In this paper, we introduce REMAP, an approach for the training
and estimation of posterior probabilities using a recursive algorithm
that is reminiscent of the EM-based Forward-Backward (Liporace
1982) algorithm for the estimation of sequence likelihoods. Although very general, the method is developed in the context of a
statistical model for transition-based speech recognition using Artificial Neural Networks (ANN) to generate probabilities for Hidden Markov Models (HMMs). In the new approach, we use local
conditional posterior probabilities of transitions to estimate global
posterior probabilities of word sequences. Although we still use
ANNs to estimate posterior probabilities, the network is trained
with targets that are themselves estimates of local posterior probabilities. An initial experimental result shows a significant decrease
in error-rate in comparison to a baseline system.

1

INTRODUCTION

The ultimate goal in speech recognition is to determine the sequence of words that
has been uttered. Classical pattern recognition theory shows that the best possible system (in the sense of minimum probability of error) is the one that chooses
the word sequence with the maximum a posteriori probability (conditioned on the

*Also affiliated with with Faculte Poly technique de Mons, Mons, Belgium

REMAP: Recursive Estimation and Maximization of A Posteriori Probabilities

389

evidence). If word sequence i is represented by the statistical model M i , and the
evidence (which, for the application reported here, is acoustical) is represented by
a sequence X = {Xl, ... , X n , ... , X N }, then we wish to choose the sequence that
corresponds to the largest P(MiIX). In (Bourlard & Morgan 1994), summarizing
earlier work (such as (Bourlard & Wellekens 1989)), we showed that it was possible to compute the global a posteriori probability P(MIX) of a discriminant form
of Hidden Markov Model (Discriminant HMM), M, given a sequence of acoustic
vectors X. In Discriminant HMMs, the global a posteriori probability P(MIX) is
computed as follows: if r represents all legal paths (state sequences ql, q2, ... , qN)
in Mi, N being the length of the sequence, then

P(Mi IX) =

L P(Mi, ql, q2, ... , qNIX)
r

=

in which ~n represents the specific state hypothesized at time n, from the set Q
{ql, ... , q , qk, ... , qK} of all possible HMM states making up all possible models

Mi. We can further decompose this into:
P(Mi, ql, q2,???, qNIX) = P(ql, q2,???, qNIX)P(Milql, q2,???, qN, X)
Under the assumptions stated in (Bourlard & Morgan 1994) we can compute
N

P(ql, q2,???, qNIX)

= II p(qnlqn-l, xn)
n=l

The Discriminant HMM is thus described in terms of conditional transition probabilities p(q~lq~-l' xn), in which q~ stands for the specific state ql of Q hypothesized
at time n and can be schematically represented as in Figure 1.
P(IkIIIkI, x)

p(/aell/ael, x)

P(/aelllkl, x)

P(ltIlltI, x)

P(ltll/ael, x)

Figure 1: An example Discriminant HMM for the word ""cat"". The variable
to a specific acoustic observation Xn at time n.

X

refers

Finally, given a state sequence we assume the following approximation:

P(Milql, q2,???, qN, X) : : : : P(Milql, q2,???, qN)
We can estimate the right side of this last equation from a phonological model (in
the case that a given state sequence can belong to two different models). All the
required (local) conditional transition probabilities p(q~lq~-l> xn) can be estimated
by the Multi-Layer Perceptron (MLP) shown in Figure 2.
Recent work at lesl has provided us with further insight into the discriminant
HMM, particularly in light of recent work on transition-based models (Konig &
Morgan 1994j Morgan et al. 1994). This new perspective has motivated us to further
develop the original Discriminant HMM theory. The new approach uses posterior
probabilities at both local and global levels and is more discriminant in nature.
In this paper, we introduce the Recursive Estimation-Maximization of A posteriori

390

Y. KONIG, H. BOURLARD, N. MORGAN
P(CurrenCstlte I Acoustics, Prevlous_stlte)

t t t t

t???? .. t
0.1 ?? 0
Previous
Stlte

Acoustics

Figure 2: An MLP that estimates local conditional transition probabilities.
Probabilities (REMAP) training algorithm for hybrid HMM/MLP systems. The
proposed algorithm models a probability distribution over all possible transitions
(from all possible states and for all possible time frames n) rather than picking a
single time point as a transition target. Furthermore, the algorithm incrementally
increases the posterior probability of the correct model, while reducing the posterior
probabilities of all other models. Thus, it brings the overall system closer to the
optimal Bayes classifier.
A wide range of discriminant approaches to speech recognition have been studied
by researchers (Katagiri et al. 1991; Bengio et al. 1992; Bourlard et al. 1994). A
significant difficulty that has remained in applying these approaches to continuous
speech recognition has been the requirement to run computationally intensive algorithms on all of the rival sentences. Since this is not generally feasible, compromises
must always be made in practice. For instance, estimates for all rival sentences can
be derived from a list of the ""N-best"" utterance hypotheses, or by using a fully
connected word model composed of all phonemes.

2
2.1

REMAP TRAINING OF THE DISCRIMINANT HMM
MOTIVATIONS

The discriminant HMM/MLP theory as described above uses transition-based probabilities as the key building block for acoustic recognition. However, it is well known
that estimating transitions accurately is a difficult problem (Glass 1988). Due to
the inertia of the articulators, the boundaries between phones are blurred and overlapped in continuous speech. In our previous hybrid HMM/MLP system, targets
were typically obtained by using a standard forced Viterbi alignment (segmentation). For a transition-based system as defined above, this procedure would thus
yield rigid transition targets, which is not realistic.
Another problem related to the Viterbi-based training of the MLP presented in
Figure 2 and used in Discriminant HMMs, is the lack of coverage of the input space
during training. Indeed, during training (based on hard transitions), the MLP only
processes inputs consisting of ""correct"" pairs of acoustic vectors and correct previous
state, while in recognition the net should generalize to all possible combinations of

REMAP: Recursive Estimation and Maximization of A Posteriori Probabilities

391

acoustic vectors and previous states, since all possible models and transitions will be
hypothesized for each acoustic input. For example, some hypothesized inputs may
correspond to an impossible condition that has thus never been observed, such as
the acoustics of the temporal center of a vowel in combination with a previous state
that corresponds to a plosive. It is unfortunately possible that the interpolative
capabilities of the network may not be sufficient to give these ""impossible"" pairs a
sufficiently low probability during recognition.
One possible solution to these problems is to use a full MAP algorithm to find transition probabilities at each frame for all possible transitions by a forward-backwardlike algorithm (Liporace 1982), taking all possible paths into account.

2.2

PROBLEM FORMULATION

As described above, global maximum a posteriori training of HMMs should find the
optimal parameter set e maximizing
J

II P(Mj IXj, e)

(1)

j=1

in which Mj represents the Markov model associated with each training utterance
Xj, with j = 1, ... , J.
Although in principle we could use a generalized back-propagation-like gradient
procedure in e to maximize (1) (Bengio et al. 1992), an EM-like algorithm should
have better convergence properties, and could preserve the statistical interpretation of the ANN outputs. In this case, training of the discriminant HMM by a
global MAP criterion requires a solution to the following problem: given a trained
MLP at iteration t providing a parameter set e t and, consequently, estimates of
P(q~lxn' q~-I' et ), how can we determine new MLP targets that:
1. will be smooth estimates of conditional transition probabilities q~-1
Vk,f E [1, K] and ""In E [1, N],

-+

q~,

2. when training the MLP for iteration t+ 1, will lead to new estimates of et+l
and P(q~lxn' q~-I' et+1) that are guaranteed to incrementally increase the
global posterior probability P(MiIX, e)?
In (Bourlard et al. 1994), we prove that a re-estimate of MLP targets that guarantee
convergence to a local maximum of (1) is given by1:

(2)
where we have estimated the left-hand side using a mapping from the previous
state and the local acoustic data to the current state, thus making the estimator
realizable by an MLP with a local acoustic window .2 Thus, we will want to estimate
1 In most of the following, we consider only one particular training sequence X associated
with one particular model M. It is, however, easy to see that all of our conclusions remain
valid for the case of several training sequences Xj, j
1, ... , J. A simple way to look
at the problem is to consider all training sequences as a single training sequence obtained
by concatenating all the X,'s with boundary conditions at every possible beginning and
ending point.
2Note that, as done in our previous hybrid HMM/MLP systems, all conditional on Xn
can be replaced by X;::!: = {x n - c , ., ?. , X n , .?? , Xn+d} to take some acoustic context into
account.

=

392

Y. KONIG, H. BOURLARD, N. MORGAN

the transition probability conditioned on the local data (as MLP targets) by using
the transition probability conditioned on all of the data.
In (Bourlard et al. 1994), we further prove that alternating MLP target estimation
(the ""estimation"" step) and MLP training (the"" maximization"" step) is guaranteed
to incrementally increase (1) over t. 3 The remaining problem is to find an efficient
algorithm to express P(q~IX, q~-l' M) in terms of P(q~lxn, q~-l) so that the next
iteration targets can be found. We have developed several approaches to this estimation, some of which are described in (Bourlard et al. 1994). Currently, we are
implementing this with an efficient recursion that estimates the sum of all possible
paths in a model, for every possible transition at each possible time. From these
values we can compute the desired targets (2) for network training by

P( t IX M k )
qn , , qn-l

2.3

=

P(M, q~, ~~_lIX)
J
k
IX)
DJ
,qn,
qn-l
~ . P(M

(3)

REMAP TRAINING ALGORITHM

The general scheme of the REMAP training of hybrid HMM/MLP systems can be
summarized as follow:
1. Start from some initial net providing P(q~lxn' q~-l' e t ), t = 0, V possible
(k,?)-pairs4.
2. Compute MLP targets P(q~IXj,q~_l,et,Mj) according to (3), V training
sentences Xj associated with HMM Mj, V possible (k, ?) state transition
pairs in Mj and V X n , n
1, ... , N in Xj (see next point).

=

3. For every Xn in the training database, train the MLP to minimize the
relative entropy between the outputs and targets. See (Bourlard et ai,
1994) for more details. This provides us with a new set of parameters t ,
for t
t + 1.

=

e

4. Iterate from 2 until convergence.
This procedure is thus composed of two steps: an Estimation (E) step, corresponding to step 2 above, and a Maximization (M) step, corresponding to step 3 above.
In this regards, it is reminiscent of the Estimation-Maximization (EM) algorithm
as discussed in (Dempster et al. 1977). However, in the standard EM algorithm,
the M step involves the actual maximization of the likelihood function. In a related
approach, usually referred to as Generalized EM (GEM) algorithm, the M step does
not actually maximize the likelihood but simply increases it (by using, e.g., a gradient procedure). Similarly, REMAP increases the global posterior function during
the M step (in the direction of targets that actually maximize that global function),
rather than actually maximizing it. Recently, a similar approach was suggested for
mapping input sequences to output sequences (Bengio & Frasconi 1995).
3Note here that one ""iteration"" does not stand for one iteration of the MLP training
but for one estimation-maximization iteration for which a complete MLP training will be
required.
4This can be done, for instance, by training up such a net from a hand-labeled database
like TIMIT or from some initial forward-backward estimator of equivalent local probabilities (usually referred to as ""gamma"" probabilities in the Baum-Welch procedure).

REMAP: Recursive Estimation and Maximization of A Posteriori Probabilities

System
DHMM, pre-REMAP
1 REMAP iteration
2 REMAP iterations

393

Error Rate
14.9%
13.6%
13.2%

Table 1: Training and testing on continuous numbers, no syntax, no durational
models.

3

EXPERIMENTS AND RESULTS

For testing our theory we chose the Numbers'93 corpus. It is a continuous speech
database collected by CSLU at the Oregon Graduate Institute. It consists of numbers spoken naturally over telephone lines on the public-switched network (Cole
et al. 1994). The Numbers'93 database consists of 2167 speech files of spoken numbers produced by 1132 callers. We used 877 of these utterances for training and
657 for cross-validation and testing (200 for cross-validation) saving the remaining
utterances for final testing purposes. There are 36 words in the vocabulary, namely
zero, oh, 1, 2, 3, ... ,20, 30, 40, 50, ... ,100, 1000, a, and, dash, hyphen, and double.
All our nets have 214 inputs: 153 inputs for the acoustic features, and 61 to represent the previous state (one unit for every possible previous state, one state per
phoneme in our case). The acoustic features are combined from 9 frames with 17
features each (RASTA-PLP8 + delta features + delta log gain) computed with an
analysis window of 25 ms computed every 12.5 ms (overlapping windows) and with
a sampling rate of 8 Khz . The nets have 200 hidden units and 61 outputs.
Our results are summarized in Table 1. The row entitled ""DHMM, pre-REMAP""
corresponds to a Discriminant HMM using the same training approach, with hard
targets determined by the first system, and additional inputs to represent the previous state The improvement in the recognition rate as a result of REMAP iterations
is significant at p < 0.05. However all the experiments were done using acoustic
information alone. Using our (baseline) hybrid system under equal conditions, i.e.,
no duration information and no language information, we get 31.6% word error;
adding the duration information back we get 12.4% word error. We are currently
experimenting with enforcing minimum duration constraints in our framework.

4

CONCLUSIONS

In summary:
? We have a method for MAP training and estimation of sequences.
? This can be used in a new form of hybrid HMM/MLP. Note that recurrent
nets or TDNNs could also be used. As with standard HMM/MLP hybrids,
the network is used to estimate local posterior probabilities (though in this
case they are conditional transition probabilities, that is, state probabilities
conditioned on the acoustic data and the previous state). However, in the
case of REMAP these nets are trained with probabilistic targets that are
themselves estimates of local posterior probabilities.
? Initial experiments demonstrate a significant reduction in error rate for this
process.

394

Y. KONIG, H. BOURLARD, N. MORGAN

Acknowledgments

We would like to thank Kristine Ma and Su-Lin Wu for their help with the Numbers'93 database. We also thank OGI, in particular to Ron Cole, for providing the
database. We gratefully acknowledge the support of the Office of Naval Research,
URI No. N00014-92-J-1617 (via UCB), the European Commission via ESPRIT
project 20077 (SPRACH), and ICSI and FPMs in general for supporting this work.

References
BENGIO, Y., & P. FRASCONI. 1995. An input output HMM architecture.
In Advances in Neural Information Processing Systems, ed. by G. Tesauro,
D. Touretzky, & T. Leen, volume 7. Cambridge: MIT press.
- - , R. DE MORI, G. FLAMMIA, & R. KOMPE. 1992. Global optimization of a
neural network-hidden Markov model hybrid. IEEE trans. on Neural Networks
3.252-258.
BOURLARD, H., Y. KONIG, & N. MORGAN. 1994. REMAP: Recursive estimation
and maximization of a posteriori probabilities, application to transition-based
connectionist speech recognition. Technical Report TR-94-064, International
Computer Science Institute, Berkeley, CA.
--, & N. MORGAN. 1994. Connectionist Speech Recognition - A Hybrid Approach.
Kluwer Academic Publishers.
--, & C. J. WELLEKENS. 1989. Links between Markov models and multilayer
perceptrons. In Advances in Neural Information Processing Systems 1, ed. by
D.J. Touretzky, 502-510, San Mateo. Morgan Kaufmann.
COLE, R.A., M. FANTY, & T. LANDER. 1994. Telephone speech corpus development at CSL U. In Proceedings Int 'I Conference on Spoken Language Processing,
Yokohama, Japan.
DEMPSTER, A. P., N. M. LAIRD, & D. B. RUBIN. 1977. Maximum likelihood
from incomplete data via the EM algorithm. Journal of the Royal Statistical
Society, Series B 34.1-38.
GLASS, J. R., 1988. Finding Acoustic Regularities in Speech Applications to Phonetic Recognition. M.LT dissertation.
KATAGIRI, S., C.H. LEE, & JUANG B.H. 1991. New discriminative training
algorithms based on the generalized probabilistic decent method. In Proc. of
the IEEE Workshop on Neural Netwroks for Signal Processing, ed. by RH.
Juang, S.Y. Kung, & C.A. Kamm, 299-308 .
KONIG, Y., & N. MORGAN. 1994. Modeling dynamics in connectionist speech
recognition - the time index model. In Proceedings Int'l Conference on Spoken
Language Processing, 1523-1526, Yokohama, Japan.
LIPORACE, L. A. 1982. Maximum likelihood estimation for multivariate observations of markov sources. IEEE Trans. on Information Theory IT-28.729-734.
MORGAN, N., H. BOURLARD, S. GREENBERG, & H. HERMANSKY. 1994. Stochastic perceptual auditory-event-based models for speech recognition. In Proceedings Int'l Conference on Spoken Language Processing, 1943-1946, Yokohama,
Japan.

"
1028,1995,Exponentially many local minima for single neurons,,1028-exponentially-many-local-minima-for-single-neurons.pdf,Abstract Missing,"Exponentially many local minima for single
neurons

Peter Auer

Manfred K. Warmuth

Mark Herbster

Department of Computer Science
Santa Cruz, California
{pauer,mark,manfred} @cs.ucsc.edu

Abstract
We show that for a single neuron with the logistic function as the transfer
function the number of local minima of the error function based on the
square loss can grow exponentially in the dimension.

1 INTRODUCTION
Consider a single artificial neuron with d inputs. The neuron has d weights w E Rd. The
output of the neuron for an input pattern x E Rd is y = ?(x? w), where ? : R -+ R
is a transfer function. For a given sequence of training examples ((Xt, Yt))I<t<m, each
consisting of a pattern Xt E R d and a desired output Yt E R, the goal of the training phase
for neural networks consists of minimizing the error function with respect to the weight
vector w E Rd. This function is the sum of the losses between outputs of the neuron and
the desired outputs summed over all training examples. In notation, the error function is
m

E(w) =

L L(Yt, ?(Xt . w))

,

t=1

where L : R x R

-+

[0,00) is the loss function.

A common example of a transfer function is the logistic function logistic( z) = I+!-' which
has the bounded range (0, 1). In contrast, the identity function id(z) = z has unbounded
(y - Y)2. Other
range. One of the most common loss functions is the square loss L(y, y)
examples are the absolute loss Iy - yl and the entropic1oss yin? + (1 - y) In

=

::::l

We show that for the square loss and the logistic function the error function of a single
neuron for n training examples may have Ln / dJ d local minima. More generally, this holds
for any loss and transfer function for which the composition of the loss function with the
transfer function (in notation L(y, ?(x . w)) is continuous and has bounded range. This

Exponentially Many Local Minima for Single Neurons

Figure 1:

317

Error Function with 25 Local Minima (16 Visible), Generated by 10 TwoDimensional Examples.

proves that for any transfer function with bounded range exponentially many local minima
can occur when the loss function is the square loss.
The sequences of examples that we use in our proofs have the property that they are nonrealizable in the sense that there is no weight vector W E R d for which the error function
is zero, i.e. the neuron cannot produce the desired output for all examples. We show with
some minimal assumptions on the loss and transfer functions that for a single neuron there
can be no local minima besides the global minimum if the examples are realizable.

If the transfer function is the logistic function then it has often been suggested in the
literature to use the entropic loss in artificial neural networks in place of the square loss
[BW88, WD88, SLF88, Wat92]. In that case the error function of a single neuron is
convex and thus has only one minimum even in the non-realizable case. We generalize this
observation by defining a matching loss for any differentiable increasing transfer functions
?:

1

,p-l(y)

L</>(y, f)) =

(?(z) - y) dz .
</>-l(y)
The loss is the area depicted in Figure 2a. If ? is the identity function then L</> is the square
loss likewise if ? is the logistic function then L</> is the entropic loss. For the matching loss
the gradient descent update for minimizing the error function for a sequence of examples
is simply
Wnew := Wold

-1]

(f)?(Xt .

Wold) - Yt)Xt) ,

t=1

where 1] is a positive learning rate. Also the second derivatives are easy to calculate for
this general setting: L4>(Y~:v~<:Wt;W)) = ?'(Xt . W)Xt,iXt,j. Thus, if Ht(w) is the Hessian
of L</>(Yt, ?(Xt . w)) with respect to W then v T Ht(w)v = ?'(Xt . w)(v . Xt)2. Thus

318

P. AUER. M. HERBSTER, M. K. WARMUTH

0.8

wO.I

0.4

0.2

.-1

(9)

(a)

Figure 2:

=w? x

...

-2

...o
(b)

(a) The Matching Loss Function L</>.
(b) The Square Loss becomes Saturated, the Entropic Loss does not.

H t is positive semi-definite for any increasing differentiable transfer function. Clearly
L:~I Ht(w) is the Hessian of the error function E(w) for a sequence of m examples and

it is also positive semi-definite. It follows that for any differentiable increasing transfer
function the error function with respect to the matching loss is always convex.
We show that in the case of one neuron the logistic function paired with the square loss
can lead to exponentially many minima. It is open whether the number of local minima
grows exponentially for some natural data. However there is another problem with the
pairing of the logistic and the square loss that makes it hard to optimize the error function
with gradient based methods. This is the problem of flat regions. Consider one example
(x, y) consisting of a pattern x (such that x is not equal to the all zero vector) and the
desired output y. Then the square loss (Iogistic(x . w) - y)2, for y E [0, I] and w E R d ,
turns flat as a function of w when f) = logistic( x . w) approaches zero or one (for example
see Figure 2b where d = I and y = 0). It is easy to see that for all bounded transfer
functions with a finite number of minima and corresponding bounded loss functions, the
of the square
same phenomenon occurs. In other words, the composition L(y, ?(x .
loss with any bounded transfer function ? which has a finite number of extrema turns flat as
Ix . w I becomes large. Similarly, for multiple examples the error function E( w) as defined
above becomes flat. In flat regions the gradients with respect to the weight vector w are
small, and thus gradient-based updates of the weight vector may have a hard time moving
the weight vector out of these flat regions. This phenomenon can easily be observed in
practice and is sometimes called ""saturation"" [Hay94]. In contrast, if the logistic function
is paired with the entropic loss (see Figure 2b), then the error function turns flat only at the
global minimum. The same holds for any increasing differentiable transfer function and its
matching loss function.

w?

A number of previous papers discussed conditions necessary and sufficient for mUltiple
local minima of the error function of single neurons or otherwise small networks [WD88,
SS89, BRS89, Blu89, SS91, GT92]. This previous work only discusses the occurrence of
multiple local minima whereas in this paper we show that the number of such minima can
grow exponentially with the dimension. Also the previous work has mainly been limited
to the demonstration of local minima in networks or neurons that have used the hyperbolic
tangent or logistic function with the square loss. Here we show that exponentially many
minima occur whenever the composition of the loss function with the transfer function is
continuous and bounded.
The paper is outlined as follows. After some preliminaries in the next section, we gi ve formal

Exponentially Many Local Minima for Single Neurons

319

11

04$

O.
0.9
036
0.1
03
07
026
WOI

02

as -- --- ------------ __

0.t5

O.

0.1

03

0.06

02

0
-2

,-

,

,
'

,
\,

'-

,/ ' ..

""

01 L......L-~~-~~~~-'--~-~
-8-e-~-2
0

-1

log ..

(b)

(a)

Figure 3:

(a) Error Function for the Logistic Transfer Function and the
Square Loss with Examples ((10, .55), (.7, .25?)
(b) Sets of Minima can be Combined.

statements and proofs of the results mentioned above in Section 3. At first (Section 3.1) we
show that n one-dimensional examples might result in n local minima of the error function
(see e.g. Figure 3a for the error function of two one-dimensional examples). From the local
minima in one dimension it follows easily that n d-dimensional examples might result in
Ln/ dJ d local minima of the error function (see Figure 1 and discussion in Section 3.2).
We then consider neurons with a bias (Section 4), i.e. we add an additional input that is
((Xt, Yt?)I<t<m is
clamped to one. The error function for a sequence of examples S
now

=

m

Es(B, w) =

I: L(Yt, r/>(B + WXt?,
t=1

where B denotes the bias, i.e. the weight of the input that is clamped to one. We can prove
that the error function might have Ln/2dJ d local minima if loss and transfer function are
symmetric. This holds for example for the square loss and the logistic transfer function .
The proofs are omitted due to space constraints. They are given in the full paper [AHW96] ,
together with additional results for general loss and transfer functions.
Finally we show in Section 5 that with minimal assumptions on transfer and loss functions
that there is only one minimum of the error function if the sequence of examples is realizable
by the neuron.
The essence of the proofs is quite simple. At first observe that ifloss and transfer function are
bounded and the domain is unbounded, then there exist areas of saturation where the error
function is essentially flat. Furthermore the error function is ""additive"" i.e. the error function
produced by examples in SUS' is simply the error function produced by the examples in
S added to the error function produced by the examples in S', Esusl Es + ESI. Hence
the local minima of Es remain local minima of Esus 1 if they fall into an area of saturation
of Es. Similarly, the local minima of ESI remain local minima of Esusl as well (see
Figure 3b). In this way sets of local minima can be combined.

=

2 PRELIMINARIES '
We introduce the notion of minimum-containing set which will prove useful for counting
the minima of the error function.

320

P. AUER, M. HERBSTER, M. K. WARMUTH

Definition 2.1 Let f : Rd_R be a continuous function. Then an open and bounded set
U E Rd is called a minimum-containing set for f if for each w on the boundary of U there
is a w'"" E U such that f(w""') < f(w).
Obviously any minimum-containing set contains a local minimum of the respecti ve function.
Furthermore each of n disjoint minimum-containing sets contains a distinct local minimum.
Thus it is sufficient to find n disjoint minimum-containing sets in order to show that a
function has at least n local minima.

3 MINIMA FOR NEURONS WITHOUT BIAS
We will consider transfer functions ? and loss functions L which have the following
property:
(PI): The transfer function ? : R-R is non-constant. The loss function L : ?(R) x
?(R)-[O, 00) has the property that L(y, y) = 0 and L(y, f)) > 0 for all y f.
f) E ?(R). FinallythefunctionL(?,?(?)): ?(R) x R-[O,oo) is continuous and

bounded.

3.1

ONE MINIMUM PER EXAMPLE IN ONE DIMENSION

Theorem 3.1 Let ? and L satisfy ( PI). Then for all n ~ I there is a sequence of n
examples S = (XI, y), ... , (x n , y)), Xt E R, y E ?(R), such that Es(w) has n distinct
local minima.
Since L(y, ?( w)) is continuous and non-constant there are w- , w""', w+ E R such that the
values ?( w-), ?( w""'), ?( w+) are all distinct. Furthermore we can assume without loss
of generality that 0 < w- < w'"" < w+. Now set y = ?(w""'). If the error function
L(y, ?(w)) has infinitely many local minima then Theorem 3.1 follows immediately, e.g.
by setting XI = ... = Xn = 1. If L(y, ?(w)) has only finitely many minima then
limw ..... oo L(y, ?(w)) = L(y, ?(oo)) exists since L(y, ?(w)) is bounded and continuous.
We use this fact in the following lemma. It states that we get a new minimum-containing
set by adding an example in the area of saturation of the error function.
Lemma 3.2 Assume that limw..... oo L(y, ?( w)) exists. Let S = (XI, YI), ... , (x n , Yn))
be a sequence of examples and 0 < WI < wi < wt < ... < w;; < w~ < w~
such that Es(w t ) > Es(wn and Es(wn < Es(wt) for t = 1, ... , n. Let S' =
(xo, y} (XI, Yd, ... , (x n, Yn)) where Xo is sufficiently large. Furthermoreletwo = w'"" /xo
and Wo = w?/xo (where w-, w""', w+, Y = ?(w""') are as above). Then 0 < we; < Wo <
wt < WI < wi < wt < ... < w;; < w~ < w~ and

Proof. We have to show that for all Xo sufficiently large condition (l) is satisfied, i.e. that

(2)
We get
lim ESI(WO) = L(y, ?(w""'))

~

..... oo

recalling that Wo

+

lim Es(w'"" /xo) = L(y, ?(w""'))

~-oo

= w'"" /xo and S' = S u (xo, y) . Analogously
lim ESI(w~) = L(y,?(w?)) + Es(O).
x
0""'"" 00

+ Es(O),

321

Exponentially Many Local Minima for Single Neurons

Thus equation (2) holds for t = 0. For t = 1, ... , n we get
lim ESI(w;) = lim L(y, ?(w;xo))
:1:0-+00

:1:0-+00

+ Es(wn

= L(y, ?(oo))

+ Es(wn

and

Since Es (w;)

< Es (w;) for t

= 1, ... , n, the lemma follows.

o

Proof of Theorem 3.1. The theorem follows by induction from Lemma 3.2 since each
0
interval ( wi, wi) is a minimum-containing set for the error function .
Remark. Though the proof requires the magnitude of the examples to be arbitrarily large I
in practice local minima show up for even moderately sized w (see Figure 3a).
3.2

CURSE OF DIMENSIONALITY: THE NUMBER OF MINIMA MIGHT
GROW EXPONENTIALLY WITH THE DIMENSION

We show how the I-dimensional minima of Theorem 3.1 can be combined to obtain ddimensional minima.
Lemma 3.3 Let I : R -+ R be a continuous function with n disjoint minimum-containing
sets UI , .?. ,Un. Then the sets UtI x ... X Utd , tj E {I, ... , n}, are n d disjoint minimumcontaining sets for the function 9 : Rd -+ R, g(XI, . .. , Xd) = l(xI) + ... + I(xd).

o

Proof. Omitted.

Theorem 3.4 Let ? and L satisfy ( PI). Then for all n ~ 1 there is a sequence of examples
S = (XI,Y),""""(xn,y)), Xt E Rd, y E ?(R), such that Es(w) has l~Jd distinct local
minima.
By Lemma 3.2 there exists a sequence of one-dimensional examples S' =
(xI,y)"""" , (xLcrJ'Y)) such that ESI(w) has L~J disjoint minimum-containing sets.
Thus by Lemma 3.3 the error function Es (w) has l ~ Jd disjoint minimum-containing
sets where S = ((XI, 0, .. . ,0), y), ... , ?xLcrJ' 0, . .. ,0), y), .. . , ?0, ... , xI), y), .. . ,
?0, .. . , xLcrJ), y)).
0
Proof.

4 MINIMA FOR NEURONS WITH A BIAS
Theorem 4.1 Let the transfer function ? and the loss function L satisfy ?( Bo + z) - ?o =
?o - ?(Bo - z) and L(?o + y, ?o + y)
L(?o - y, ?o - y)for some B o, ?o E R and all
z E R, y, Y E ?(R). Furthermore let ? have a continuous second derivative and assume
that the first derivative of ? at Bo is non-zero. At last let ~L(y, y) be continuous in y

=

Then for all n ~ 1
there is a sequence of examples S = (XI, YI), . .. , (xn, Yn)), Xt E R d, Yt E ?(R), such

and y, L(y, y) = 0for all y E ?(R), and
that Es (B, w) has

(~L(Y, y)) (?o, ?o) > 0.

l ~ Jd distinct local minima.

Note that the square loss along with either the hyperbolic or logistic transfer function
satisfies the conditions of the theorem.
IThere is a parallel proof where the magnitudes of the examples may be arbitrarily small.

P. AUER, M. HERBSTER, M. K. WARMUTH

322

5 ONE MINIMUM IN THE REALIZABLE CASE
We show that when transfer and loss function are monotone and the examples are realizable
then there is only a single minimal surface. A sequence of examples S is realizable if
Es(w) = 0 for some wE Rd.

Theorem 5.1 Let 4> and L satisfy (P1). Furthermore let 4> be mOriotone and L such that
L(y, y + rl) ~ L(y, y + r2) for 0 ~ rl ~ r2 or 0 ~ rl ~ r2. Assume that for some
sequence of examples S there is a weight vectorwo E Rd such that Es(wo) = O. Thenfor
each WI E Rd the function h( a) = Es (( 1 - a )wo + aWl) is increasing for a ~ O.
Thus each minimum WI can be connected with Wo by the line segment WOWI such that
Es(w) = 0 for all W on WOWI.

Proof of Theorem 5.1.
Let S = ((XI, yd, ... , (xn, Yn)).
Then h(a)
E~=I L(yt, 4>(WOXt + a(wl - wo)xt}). Since Yt = 4>(WOXt) it suffices to show that
L(4)(z), 4>(z+ar)) is monotonically increasing in a ~ ofor all Z, r E R. Let 0 ~ al ~ a2.
Since 4> is monotone we get 4>(z + aIr) = 4>(z) + rl, 4>(z + a2r)
4>(z) + r2 where
o ~ rl ~ r2 or 0 ~ rl ~ r2? Thus L(4)(z), 4>(z + aIr)) ~ L(4)(z), 4>(z + a2r)).
0

=

Acknowledgments
We thank Mike Dooley, Andrew Klinger and Eduardo Sontag for valuable discussions. Peter Auer
gratefully acknowledges support from the FWF, Austria, under grant J01028-MAT. Mark Herbster
and Manfred Warmuth were supported by NSF grant IRI-9123692.

References
[AHW96] P. Auer, M. Herbster, and M. K. Warmuth. Exponentially many local minima for single
neurons. Technical Report UCSC-CRL-96-1, Univ. of Calif. Computer Research Lab,
Santa Cruz, CA, 1996. In preperation.
[Blu89]

E.K. Blum. Approximation of boolean functions by sigmoidal networks: Part i: Xor and
other two-variable functions . Neural Computation, 1:532-540, February 1989.

[BRS89]

M.L. Brady, R. Raghavan, and J. Slawny. Back propagation fails to separate where
perceptrons succeed. IEEE Transactions On Circuits and Systems, 36(5):665-674, May
1989.

[BW88]

E. Baum and F. Wilczek. Supervised learning of probability distributions by neural
networks . In D.Z. Anderson, editor, Neural Information Processing Systems, pages 5261, New York, 1988. American Insitute of Physics.

[GT92]

Marco Gori and Alberto Tesi. On the problem of local minima in backpropagation. IEEE
Transaction on Pattern Analysis and Machine Intelligence, 14(1):76-86, 1992.

[Hay94]

S. Haykin. Neural Networks: a Comprehensive Foundation. Macmillan, New York, NY,
1994.

[SLF88]

S. A. Solla, E. Levin, and M. Fleisher. Accelerated learning in layered neural networks.
Complex Systems, 2:625-639,1988.

[SS89]

E.D. Sontag and H.l. Sussmann. Backpropagation can give rise to spurious local minima
even for networks without hidden layers. Complex Systems, 3(1):91-106, February 1989.

[SS91]

E.D. Sontag and H.l. Sussmann. Back propagation separates where perceptrons do. Neural
Networks,4(3),1991.

[Wat92]

R. L. Watrous. A comparison between squared error and relative entropy metrics using
several optimization algorithms. Complex Systems, 6:495-505, 1992.

[WD88]

B.S. Wittner and J .S. Denker. Strategies for teaching layered networks classification tasks.
In D.Z. Anderson, editor, Neural Information Processing Systems, pages 850--859, New
York, 1988. American Insitute of Physics.

"
1029,1995,A Practical Monte Carlo Implementation of Bayesian Learning,,1029-a-practical-monte-carlo-implementation-of-bayesian-learning.pdf,Abstract Missing,"A Practical Monte Carlo Implementation
of Bayesian Learning

Carl Edward Rasmussen
Department of Computer Science
University of Toronto
Toronto, Ontario, M5S 1A4, Canada
carl@cs.toronto.edu

Abstract
A practical method for Bayesian training of feed-forward neural
networks using sophisticated Monte Carlo methods is presented
and evaluated. In reasonably small amounts of computer time this
approach outperforms other state-of-the-art methods on 5 datalimited tasks from real world domains.

1

INTRODUCTION

Bayesian learning uses a prior on model parameters, combines this with information
from a training set , and then integrates over the resulting posterior to make predictions. With this approach, we can use large networks without fear of overfitting,
allowing us to capture more structure in the data, thus improving prediction accuracy and eliminating the tedious search (often performed using cross validation) for
the model complexity that optimises the bias/variance tradeoff. In this approach
the size of the model is limited only by computational considerations.
The application of Bayesian learning to neural networks has been pioneered by
MacKay (1992), who uses a Gaussian approximation to the posterior weight distribution. However, the Gaussian approximation is poor because of multiple modes in
the posterior. Even locally around a mode the accuracy of the Gaussian approximation is questionable, especially when the model is large compared to the amount
of training data.
Here I present and test a Monte Carlo method (Neal, 1995) which avoids the
Gaussian approximation. The implementation is complicated, but the user is not required to have extensive knowledge about the algorithm. Thus, the implementation
represents a practical tool for learning in neural nets.

599

A Practical Monte Carlo Implementation of Bayesian Learning

1.1

THE PREDICTION TASK

=

The training data consists of n examples in the form of inputs x
{x(i)} and
corresponding outputs y = {y(i)} where i = 1 ... n. For simplicity we consider
only real-valued scalar outputs. The network is parametrised by weights w, and
hyperparameters h that control the distributions for weights, playing a role similar
to that of conventional weight decay. Weights and hyperparameters are collectively
termed 0, and the network function is written as F/I (x), although the function value
is only indirectly dependent on the hyperparameters (through the weights).
Bayes' rule gives the posterior distribution for the parameters in terms of the likelihood, p(ylx, 0), and prior, p(O):

p

(Olx

,y

) = p(O)p(ylx, O)
p(ylx)

To minimize the expected squared error on an unseen test case with input
we use the mean prediction

x(n+l),

(1)

2

MONTE CARLO SAMPLING

The following implementation is due to Neal (1995). The network weights are
updated using the hybrid Monte Carlo method (Duane et al. 1987). This method
combines the Metropolis algorithm with dynamical simulation. This helps to avoid
the random walk behavior of simple forms of Metropolis, which is essential if we
wish to explore weight space efficiently. The hyperparameters are updated using
Gibbs sampling.

2.1

NETWORK SPECIFICATION

The networks used here are always of the same form: a single linear output unit, a
single hidden layer of tanh units and a task dependent number of input units. All
layers are fully connected in a feed forward manner (including direct connections
from input to output). The output and hidden units have biases.
The network priors are specified in a hierarchical manner in terms of hyperparameters; weights of different kinds are divided into groups, each group having it's own
prior. The output-bias is given a zero-mean Gaussian prior with a std. dev. of
u = 1000, so it is effectively unconstrained.
The hidden-biases are given a two layer prior: the bias b is given a zero-mean
Gaussian prior b '"" N(O, ( 2 ); the value of u is specified in terms of precision r = u- 2 ,
which is given a Gamma prior with mean p = 400 (corresponding to u = 0.05) and
shape parameter a = 0.5; the Gamma density is given by p(r) '"" Gamma(p, a) ex:
r Ol / 2 - 1 exp( -ra/2p). Note that this type of prior introduces a dependency between
the biases for different hidden units through the common r. The prior for the
hidden-to-output weights is identical to the prior for the hidden-biases, except that
the variance of these weights under the prior is scaled down by the square root
of the number of hidden units, such that the network output magnitude becomes
independent of the number of hidden units. The noise variance is also given a
Gamma prior with these parameters.

600

C. E. RASMUSSEN

The input-to-hidden weights are given a three layer prior: again each weight is
given a zero-mean Gaussian prior w rv N(O, (12); the corresponding precision for
the weights out of input unit i is given a Gamma prior with a mean J.l and a shape
parameter a1 = 0.5: Ti rv Gamma(J.l, a1). The mean J.l is determined on the top
level by a Gamma distribution with mean and shape parameter ao = 1: J.li rv
Gamma(400,ao). The direct input-to-output connections are also given this prior.
The above-mentioned 3 layer prior incorporates the idea of Automatic Relevance
Determination (ARD), due to MacKay and Neal, and discussed in Neal (1995) . The
hyperparameters, Ti, associated with individual inputs can adapt according to the
relevance of the input; for an unimportant input, Ti can grow very large (governed
by the top level prior), thus forcing (1i and the associated weights to vanish.
2.2

MONTE CARLO SPECIFICATION

Sampling from the posterior weight distribution is performed by iteratively updating
the values of the network weights and hyperparameters. Each iteration involves two
components: weight updates and hyperparameter updates. A cursory description
of these steps follows.
2.2.1

Weight Updates

Weight updates are done using the hybrid Monte Carlo method . A fictitious dynamical system is generated by interpreting weights as positions, and augmenting
the weights w with momentum variables p. The purpose of the dynamical system
is to give the weights ""inertia"" so that slow random walk behaviour can be avoided
during exploration of weight space. The total energy, H, of the system is the sum
of the kinetic energy, I<, (a function of the momenta) and the potential energy, E.
The potential energy is defined such that p(w) ex exp( -E). We sample from the
joint distribution for wand p given by p(w,p) ex exp(-E - I<), under which the
marginal distribution for w is given by the posterior. A sample of weights from the
posterior can therefore be obtained by simply ignoring the momenta.
Sampling from the joint distribution is achieved by two steps: 1) finding new points
in phase space with near-identical energies H by simulating the dynamical system
using a discretised approximation to Hamiltonian dynamics, and 2) changing the
energy H by doing Gibbs sampling for the momentum variables.
Hamiltonian Dynamics. Hamilton's first order differential equations for Hare
approximated by a series of discrete first order steps (specifically by the leapfrog
method). The first derivatives of the network error function enter through the
derivative of the potential energy, and are computed using backpropagation. In
the original version of the hybrid Monte Carlo method the final position is then
accepted or rejected depending on the final energy H'"" (which is not necessarily
equal to the initial energy H because of the discretisation). Here we use a modified
version that uses an average over a window of states instead. The step size of the
discrete dynamics should be as large as possible while keeping the rejection rate
low. The step sizes are set individually using several heuristic approximations, and
scaled by an overall parameter c. We use L = 200 iterations, a window size of 20
and a step size of c = 0.2 for all simulations.
Gibbs Sampling for Momentum Variables. The momentum variables are
updated using a modified version of Gibbs sampling, allowing the energy H to
change. A ""persistence"" of 0.95 is used; the new value of the momentum is a
weighted sum of the previous value (weight 0.95) and the value obtained by Gibbs
sampling (weight (1 - 0.95 2)1/2). With this form of persistence, the momenta

A Practical Monte Carlo Implementation of Bayesian Learning

601

changes approx. 20 times more slowly, thus increasing the ""inertia"" of the weights,
so as to further help in avoiding random walks. Larger values of the persistence will
further increase the weight inertia, but reduce the rate of exploration of H. The
advantage of increasing the weight inertia in this way rather than by increasing L is
that the hyperparameters are updated at shorter intervals, allowing them to adapt
to the rapidly changing weights.
2.2.2

Hyperparameter Updates

The hyperparameters are updated using Gibbs sampling. The conditional distributions for the hyperparameters given the weights are of the Gamma form, for which
efficient generators exist, except for the top-level hyperparameter in the case of the
3 layer priors used for the weights from the inputs; in this case the conditional
distribution is more complicated and a form of rejection sampling is employed.
2.3

NETWORK TRAINING AND PREDICTION

The network training consists of two levels of initialisation before sampling for
networks used for prediction. At the first level of initialisation the hyperparameters
(variance of the Gaussians) are kept constant at 1, allowing the weights to grow
during 1000 leapfrog iterations. Neglecting this phase can cause the network to get
caught for a long time in a state where weights and hyperparameters are both very
small.
The scheme described above is then invoked and run for as long as desired, eventually producing networks from the posterior distribution. The initial 1/3 of these
nets are discarded, since the algorithm may need time to reach regions of high posterior probability. Networks sampled during the remainder of the run are saved for
making predictions.
The predictions are made using an average of the networks sampled from the posterior as an approximation to the integral in eq. (1). Since the output unit is linear
the final prediction can be seen as coming from a huge (fully connected) ensemble
net with appropriately scaled output weights. All the results reported here were
for ensemble nets with 4000 hidden units. The size of the individual nets is given
by the rule that we want at least as many network parameters as we have training
examples (with a lower limit of 4 hidden units). We hope thereby to be well out of
the underfitting region. Using even larger nets would probably not gain us much
(in the face of the limited training data) and is avoided for computational reasons.
All runs used the parameter values given above. The only check that is necessary
is that the rejection rate stays low, say below 5%; if not, the step size should
be lowered. In all runs reported here, c = 0.2 was adequate. The parameters
concerning the Monte Carlo method and the network priors were all selected based
on intuition and on experience with toy problems. Thus no parameters need to be
set by the user.

3

TESTS

The performance of the algorithm was evaluated by comparing it to other state-ofthe-art methods on 5 real-world regression tasks. All 5 data sets have previously
been studied using a 10-way cross-validation scheme (Quinlan 1993). The tasks
in these domains is to predict price or performance of an object from various discrete and real-valued attributes. For each domain the data is split into two sets
of roughly equal size, one for training and one for testing. The training data is

602

C. E. RASMUSSEN

further subdivided into full-, half-, quarter- and eighth-sized subsets, 15 subsets in
total. Networks are trained on each of these partitions, and evaluated on the large
common test set . On the small training sets, the average performance and one
std. dev. error bars on this estimate are computed.

3.1

ALGORITHMS

The Monte Carlo method was compared to four other algorithms. For the three
neural network methods nets with a single hidden layer and direct input-output
connections were used. The Monte Carlo method was run for 1 hour on each of the
small training sets, and 2,4 and 8 hours respectively on the larger training sets. All
simulations were done on a 200 MHz MIPS R4400 processor. The Gaussian Process
method is described in a companion paper (Williams & Rasmussen 1996).
The Evidence method (MacKay 1992) was used for a network with separate hyperparameters for the direct connections, the weights from individual inputs (ARD),
hidden biases, and output biases. Nets were trained using a conjugate gradient
method, allowing 10000 gradient evaluations (batch) before each of 6 updates of
the hyperparameters. The network Hessian was computed analytically. The value
of the evidence was computed without compensating for network symmetries, since
this can lead to a vastly over-estimated evidence for big networks where the posterior Gaussians from different modes overlap. A large number of nets were trained for
each task, with the number of hidden units computed from the results of previous
nets by the following heuristics: The min and max number of hidden units in the 20%
nets with the highest evidences were found. The new architecture is picked from a
Gaussian (truncated at 0) with mean (max - min)/2 and std. dev. 2 + max - min,
which is thought to give a reasonable trade-off between exploration and exploitation. This procedure is run for 1 hour of cpu time or until more than 1000 nets have
been trained. The final predictions are made from an ensemble of the 20% (but a
maximum of 100) nets with the highest evidence.
An ensemble method using cross-validation to search over a 2-dimensional grid for
the number of hidden units and the value of a single weight decay parameter has
been included, as an attempt to have a thorough version of ""common practise"".
The weight decay parameter takes on the values 0, 0.01, 0.04, 0.16 , 0.64 and 2.56.
Up to 6 sizes of nets are used, from 0 hidden units (a linear model) up to a number
that gives as many weights as training examples. Networks are trained with a
conjugent gradient method for 10000 epochs on each of these up to 36 networks,
and performance was monitored on a validation set containing 1/3 of the examples,
selected at random. This was repeated 5 times with different random validation
sets, and the architecture and weight decay that did best on average was selected.
The predictions are made from an ensemble of 10 nets with this architecture, trained
on the full training set. This algorithm took several hours of cpu time for the largest
training sets.
The Multivariate Adaptive Regression Splines (MARS) method (Friedman 1991)
was included as a non-neural network approach. It is possible to vary the maximum
number of variables allowed to interact in the additive components of the model.
It is common to allow either pairwise or full interactions. I do not have sufficient
experience with MARS to make this choice. Therefore, I tried both options and
reported for each partition on each domain the best performance based on the
test error, so results as good as the ones reported here might not be obtainable in
practise. All other parameters of MARS were left at their default values. MARS
always required less than 1 minute of cpu time.

A Practical Monte Carlo Implementation of Bayesian Learning

603

Auto price

Cpu

0.6

2

0.5
1.5

0.4

+

0.3

1
0*

+

0.5

o~------~----~----~---

20

40

IS!

*

X

0.1

x
10

o

0.2

80

OL-~----~------~----~--

13

26

House

52

104

Mpg

0.25
0.6

t

0.5

0.4

0.2
0.15

0.3

0.1

>?1>*
+ IS!

0.2

*

Xo+

IS!

0.05

0.1
o~~----~------~----~--

32

64

128

OL-~----~----~----~--

256

24

48

96

192

Servo
Geometric mean

1
x Monte Carlo

0.283

o Gaussian Evidence

0.364

0.6

+ Backprop

0.339

0.4

*

MARS

0.371

IS!

Gaussian Process

0.304

0.8

OtIS!

0.2

X

*

o~~------~----~----~---

11

22

44

88

Figure 1: Squared error on test cases for the five algorithms applied to the five problems.
Errors are normalized with respect to the variance on the test cases. The x-axis gives the
number of training examples; four different set sizes were used on each domain. The error
bars give one std. dev. for the distribution of the mean over training sets. No error bar is
given for the largest size, for which only a single training set was available. Some of the
large error bars are cut of at the top. MARS was unable to run on the smallest partitions
from the Auto price and the servo domains; in these cases the means of the four other
methods were used in the reported geometric mean for MARS.

604

C. E. RASMUSSEN

domain
Auto Price
Cpu
House
Mpg
Servo

3.2

Table 1: Data Sets
# training cases # test cases # binary inputs
80
104
256
192
88

79
105
250
200
79

0
0
1
6
10

# real inputs
16
6
12
3
2

PERFORMANCE

The test results are presented in fig . 1. On the servo domain the Monte Carlo
method is uniformly better than all other methods, although the difference should
probably not always be considered statistically significant. The Monte Carlo method
generally does well for the smallest training sets. Note that no single method does
well on all these tasks. The Monte Carlo method is never vastly out-performed by
the other methods.
The geometric mean of the performances over all 5 domains for the the 4 different
training set sizes is computed. Assuming a Gaussian distribution of prediction
errors, the log of the error variance can (apart from normalising constants) be
interpreted as the amount of information unexplained by the models. Thus, the
log of the geometric means in fig. 1 give the average information unexplained by
the models. According to this measure the Monte Carlo method does best, closely
followed by the Gaussian Process method . Note that MARS is the worst, even
though the decision between pairwise and full interactions were made on the basis
of the test errors.

4

CONCLUSIONS

I have outlined a black-box Monte Carlo implementation of Bayesian learning in
neural networks, and shown that it has an excellent performance. These results suggest that Monte Carlo based Bayesian methods are serious competitors for practical
prediction tasks on data limited domains.

Acknowledgements
I am grateful to Radford Neal for his generosity with insight and software. This research
was funded by a grant to G. Hinton from the Institute for Robotics and Intelligent Systems.

References
S. Duane, A. D. Kennedy, B. J. Pendleton & D. Roweth (1987) ""Hybrid Monte Carlo"",
Physics Letters B, vol. 195, pp. 216-222.
J . H. Friedman (1991) ""Multivariate adaptive regression splines"" (with discussion) , Annals
of Statistics , 19,1-141 (March) . Source: http://lib.stat.cmu.edu/general/mars3.5.

D. J. C. MacKay (1992) ""A practical Bayesian framework for backpropagation networks"",
Neural Computation, vol. 4, pp. 448- 472.

R. M. Neal (1995) Bayesian Learning for Neural Networks, PhD thesis, Dept. of Computer
Science, University of Toronto, ftp: pub/radford/thesis. ps. Z from ftp. cs . toronto. edu.
J. R. Quinlan (1993) ""Combining instance-based and model-based learning"", Proc . ML '93

(ed P.E. Utgoff), San Mateo: Morgan Kaufmann.
C. K. I. Williams & C. E. Rasmussen (1996). ""Regression with Gaussian processes"", NIPS
8, editors D. Touretzky, M. Mozer and M. Hesselmo. (this volume) .

"
103,1988,An Adaptive Network That Learns Sequences of Transitions,,103-an-adaptive-network-that-learns-sequences-of-transitions.pdf,Abstract Missing,"653

AN ADAPTIVE NETWORK THAT LEARNS
SEQUENCES OF TRANSITIONS
C. L. Winter
Science Applications International Corporation
5151 East Broadway, Suite 900
Tucson, Auizona 85711

ABSTRACT
We describe an adaptive network, TIN2, that learns the transition
function of a sequential system from observations of its behavior. It
integrates two subnets, TIN-I (Winter, Ryan and Turner, 1987) and
TIN-2. TIN-2 constructs state representations from examples of
system behavior, and its dynamics are the main topics of the paper.
TIN-I abstracts transition functions from noisy state representations
and environmental data during training, while in operation it produces
sequences of transitions in response to variations in input. Dynamics
of both nets are based on the Adaptive Resonance Theory of Carpenter
and Grossberg (1987). We give results from an experiment in which
TIN2 learned the behavior of a system that recognizes strings with an
even number of l's .

INTRODUCTION
Sequential systems respond to variations in their input environment with sequences of
activities. They can be described in two ways. A black box description characterizes a
system as an input-output function, m = B(u), mapping a string of input symbols, ll,
into a single output symbol, m. A sequential automaton description characterizes a
system as a sextuple (U, M, S, SO, f, g) where U and M are alphabets of input and output
symbols, S is a set of states, sO is an initial state and f and g are transition and output
functions respectively. The transition function specifies the current state, St, as a
function of the last state and the current input, Ut,
(1)

In this paper we do not discuss output functions because they are relatively simple. To
further simplify discussion, we restrict ourselves to binary input alphabets, although the
neural net we describe here can easily be extended to accomodate more complex alphabets.

654

Winter

A common engineering problem is to identify and then simulate the functionality of a
system from observations of its behavior. Simulation is straightforward when we can
actually observe the internal states of a system, since then the function f can be specified
by learning simple associations among internal states and external inputs. In robotic
systems, for instance, internal states can often be characterized by such parameters as
stepper motor settings, strain gauge values, etc., and so are directly accessible. Artificial
neural systems have peen found useful in such simulations because they can associate
large, possibly noisy state space and input variables with state and output variables (Tolat
and Widrow, 1988; Winter, Ryan and Turner, 1987).
Unfortunately, in many interesting cases we must base simulations on a limited set of
examples of a system's black box behavior because its internal workings are
unobservable. The black box description is not, by itself, much use as a simulation tool
since usually it cannot be specified without resorting to infinitely large input-output
tables. As an alternative we can try to develop a sequential automaton description of the
system by observing regularities in its black box behavior. Artificial neural systems can
contribute to the development of physical machines dedicated to system identification
because i) frequently state representations must be derived from many noisy input
variables, ii) data must usually be processed in continuous time and iii) the explicit
dynamics of artificial neural systems can be used as a framework for hardware
implementations.
In this paper we give a brief overview of a neural net, TIN2, which learns and processes
state transitions from observations of correct black box behavior when the set of
observations is large enough to characterize the black box as an automaton. The TIN2
net is based on two component networks. Each uses a modified adaptive resonance circuit
(Carpenter and Grossberg, 1987) to associate heterogeneous input patterns. TIN-1
(Winter, Ryan and Turner, 1987) learns and executes transitions when given state
representations. It has been used by itself to simulate systems for which explicit state
representations are available (Winter, 1988a). TIN-2 is a highly parallel, continuous time
implementation of an approach to state representation first outlined by Nerode (1958).
Nerode's approach to system simulation relies upon the fact that every string, l!. moves a
machine into a particular state, s(y). once it has been processed. The s(y) state can be
characterized by putting the system initially into s(u) (by processing y) and then
presenting a set of experimental strings. (~1 .... , ~n)' for further processing.
Experiments consist of observing the output mi = BUt?~i) where ? indicates
concatenation. A state can then be represented by the entries in a row of a state
characterization table, C (Table 1). The rows of C are indexed by strings, lI, its columns
are indexed by experiments. Wi. and its entries are mi. In Table 1 annotations in
parentheses indicate nodes (artificial neurons) and subnetworks of TIN-2 equivalent to the
corresponding C table entry. During experimentation C expands as states are

Adaptive Network That Learns Sequences of Transitions

distinguished from one another. The orchestration of experiments, their selection, the
TABLE 1. C Table Constructed by TIN-2

A.

A.
1
0
10

1 (Node 7)

o(Node 6)
o(Node 1)
o(Node 3)

o(Assembly 1)

1 (Assembly 2)

o(Node 2)
o(Node 9)

o(Node 5)

1 (Node 6)
o(Node 2)

1 (Node 1)
o(Node 4)
o(Node 0)

role of teachers and of the environment have been investigated by Arbib and Zeiger
(1969), Arbib and Manes (1974), Gold (1972 and 1978) and Angluin (1987) to name a
few. TIN-2 provides an architecture in which C can be embedded and expanded as
necessary. Collections of nodes within TIN-21earn to associate triples (mi, 11, ~i) so that
inputting II later results in the output of the representation (m 1, ... , mn)n of the state
associated with 11.

TIN-2
TIN-2 is composed of separate assemblies of nodes whose dynamics are such that each
assembly comes to correspond to a column in the state characterization table C. Thus we
call them column-assemblies. Competition among column-assemblies guarantees that
nodes of only one assembly, say the ith, learn to respond to experimental pattern ~i'
Hence column-assemblies can be labelled ~1' ~2 and so on, but since labelings are not
assigned ahead of time, arbitrarily large sets of experiments can be learned.
The theory of adaptive resonance is implemented in TIN-2 column-assemblies through
partitioned adaptive resonance circuits (cf. Ryan, Winter and Turner, 1987). Adaptive
resonance circuits (Grossberg and Carpenter, 1987; Ryan and Winter, 1987) are composed
of four collections of nodes: Input, Comparison (FI), Recognition (F2) and Reset. In
TIN-2 Input, Comparison and Reset are split into disjoint m,.u and ~ partitions. The net
runs in either training or operational mode, and can move from one to the other as
required. The training dynamics of the circuit are such that an F2 node is stimulated by
the overall triple (m. n,~, but can be inhibited by a mismatch with any component.
During operation input of.u recalls the state representation s(u) = (m 1.... , mn)n'
Node activity for the kth FI partition, FI ,k' k = m, u,

W,

is governed by

(2)
Here t < 1 scales time, Ii,k is the value of the ith input node of partition k, xi,k is

655

656

Winter

activity in the corresponding node of FI and f is a sigmoid function with range [0. 1].
The elements of I are either 1. -lor O. The dynamics of the TIN-2 circuit are such that 0
indicates the absence of a symbol, while 1 and -1 represent elements of a binary alphabet.
The adaptive feedback filter. T. is a matrix (Tji) whose elements. after training. are also
1.-1 orO.
Activity, yj. in the jth F2 node is driven by

+ L meFl,m Bmj h(xm)] - 4[ ~*j f(YTl) + Ruj + Rw] .

(3)

The feedforward fllter B is composed of matrices (Buj)' (Bmj) and (Bw) whose elements
are normalized to the size of the patterns memorized. Note that (Bw) is the same for
every node in a given column-assembly. i.e. the rows of (Bw) are all the same. Hence all
nodes within a column-assembly learn to respond to the same experimental pattern. w.
and it is in this sense that an assembly evolves to become equivalent to a column in table
C. During training the sum ~*j f(YTl) in (3) runs through the recognition nodes of all
TIN-2 column-assemblies. Thus. during training only one F2 node. say the Jth. can be
active at a time across all assemblies. In operation. on the other hand. we remove
inhibition due to nodes in other assemblies so that at any time one node in each
column-assembly can be active. and an entire state representation can be recalled.
The Reset terms Ru,j and Rw in (3) actively inhibit nodes of F2 when mismatches
between memory and input occur. Ruj is specific to the jth F2 node.
dRujldt = -Ruj + f(Yj) f(v 1I1u II - II ?I.u II) .

(4)

Rw affects all F2 nodes in a column-assembly and is driven by
dRw/dt = -Rw + [LjeF2 f(Yj)] f(v IIlw II-II fI.w II).

(5)

v < 1 is a vigilance parameter (Carpenter and Grossberg. 1987): for either (4) or (5) R > 0
at equilibrium just when the intersection between memory and input. PI T n I. is

=

relatively small, i.e. R > 0 when v 11111 > II PI II. When the system is in operation. we
fix Rw = 0 and input the pattern Iw = O. To recall the row in table C indexed by 11, we
input 11 to all column-assemblies. and at equilibrium xi.m = Lje F2 Tjif(Yj). Thus xi,m
represents the memory of the element in C corresponding to 11 and the column in C with
the same label as the column-assembly. Winter (1988b) discusses recall dynamics in
more detail.

657

Adaptive Network That Learns Sequences of Transitions

At equilibrium in either training or operational mode only the winning F2 node has YJ *-

O. so LjTjif(Yj) = TJi in (2). Hence xi.k = 0 if TJi = -li.k. i.e. if memory and input
mismatch; IXi.kl = 2 if TJi = Ii,k. i.e. when memory and input match; and IXi.kl = 1 if
TJ.i =O. Ii.k *- 0 or ifTJ.i *- O. Ii.k = O. The F1 output function h in (3) is defined so
that hex) = 1 if x> 1. hex) = -1 if x < -1 and hex) = 0 if -1 S x S 1. The output pattern
~1 = (h(x1) ..... h(xnl? reflects IJ ('\ Ik. as h(xi) *- 0 only if TJi = Ii.k.
The adaptive filters (Buj) and (Bmj) store normalized versions of those patterns on FI.u
and F1.m which have stimulated the jth F2 node. The evolution of Bij for u E FI.u or
F1 m is driven by

?

(6)

On the other hand (Bw) stores a normalized version of the experiment w which labels the
entire column-assembly. Thus all nodes in a column-assembly share a common memory
of~.

(7)

where w E F1 w .

?

The feedback mters (Tuj). (Tmj) and (Tw) store exact memories of patterns on partitions
ofFI:
(8)

for i

E

FI.u ? F1.m ? and

(9)

for i E FI.w' In operation long-term memory modification is suspended.

EXPERIMENT
Here we report partial results from an experiment in which TlN-2 learns a state
characterization table for an automaton that recognizes strings containing even numbers of

.

.

658

Winter

both I's and O's. More details can be found in Winter (1988b). For notational
convenience in this section we will discuss patterns as if they were composed of l's and
O's, but be aware that inside TIN-2 every 0 symbol is really a -1. Data is provided in the
form of triples eM, ll, YD by a teacher; the data set for this example is given in Table 1.
Data were presented to the net in the order shown. The net consisted of three
column-assemblies. Each F2 collection contained ten nodes. Although the strings that
can be processed by an automaton of this type are in principle arbitrarily long, in practice
some limitation on the length of training strings is necessary if for no other reason than
that the memory capacity of a computer is finite. For this simple example Input and F I
partitions contain eight nodes, but in order to have a special symbol to represent A..
strings are limited to at most six elements. With this restriction the A. symbol can be
distinguished from actual input strings through vigilance criteria. Other solutions to the
problem of representing A. are being investigated, but for now the special eight bit
symbol, 00000011, is used to represent A. in the strings A.-yt.
The net was trained using fast-learning (Carpenter and Grossberg, 1987): a triple in Table
1 was presented to the net. and all nodes were allowed to come to their equilibrium values
where they were held for about three long-term time units before the next triple was
presented. Consider the processing that follows presentation of (0, 1,0) the first datum
in Table 1. The net can obtain equivalents to two C table entries from (0, 1,0): the entry
in row 1l = 10, column Yi. = A. and the entry in row II =1, column w = O. The string 10
and the membership value 0 were displayed on the A. assembly's input slabs, and in this
case the 3rd F2 node learned the association among the two patterns. When the pattern
(0, 1, 0) was input to other column-assemblies, one F2 node (in this case the 9 th in
column-assembly 1) learned to associate elements of the triple. Of course a side effect of
this was that column-assembly 1 was labelled by Yi.. = 0 thereafter. When (1. 1, 1) was
input next, node 9 in column-assembly 1 tried to respond to the new triple, all nodes in
column-assembly 1 were then inhibited by a mismatch on Yi.., and finally node 1 on
column-assembly 2 learned (1, 1, 1). From that point on column-assembly 2 was
labelled by 1.

LEARNING TRANSITIONS
The TIN-I net (Winter. Ryan and Turner, 1987) is composed of i) a partitioned adaptive
resonance circuit with dynamics similar to (2) - (9) for learning state transitions and ii) a
Control Circuit which forces transitions once they have been learned. Transitions are
unique in the sense that a previous state and current input completely determine the
current state. The partitioned adaptive resonance circuit has three input fields: one for the
previous state, one for the current input and one for the next state. TIN-l's F2 nodes
learn transitions by associating patterns in the three input fields. Once trained. TIN-l
processes strings sequentially. bit-by-bit.

Adaptive Network That Learns Sequences of Transitions

1L~r--T-'N---2-~:t ~

TIN-l

TIN-2

I~ u.eu

Figure 1. Training TIN2.
The architecture of TIN2, the net that integrates TIN-2 and TIN-I. is shown in Figure 1.
The system resorts to the TIN-2 nets only to learn transitions. If TIN-2 has learned a C
table in which examples of all transitions appear, TIN-I can easily learn the automaton's
state transitions. A C table contains an example of a transition from state si to state Sj
forced by current input u, if it contains i) a row labelled by a string lli which leaves the
automaton in si after processing and ii) a row labelled by the string lltu which leaves the
automaton in Sj. To teach TIN-l the transition we simply present lli to the lower TIN-2
in Figure I, llieu to the upper TIN-2 net and u to TIN-I.

CONCLUSIONS
We have described a network, TIN-2, which learns the equivalent of state characterization
tables (Gold, 1972). The principle reasons for developing a neural net implementation are
i) neural nets are intrinsically massively parallel and so provide a nice model for systems
that must process large data sets, ii) although in the interests of brevity we have not
stressed the point, neural nets are robust against noisy data, iii) neural nets like the
partitioned adaptive resonance circuit have continuous time activity dynamics and so can
be synchronized with other elements of a larger real-time system through simple scaling
parameters, and iv) the continuous time dynamics and precise architectural specifications
of neural nets provide a blueprint for hardware implementations.
We have also sketched a neural net, TIN2, that learns state transitions by integrating
TIN-2 nets with the TIN-I net (Winter, Ryan and Turner, 1987). When a complete state
characterization table is available from TIN-2, TIN2 can be taught transitions from
examples of system behavior. However, the ultimate goal of a net like this lies in
developing a system that ""or,rates acceptably"" with a partial state characterization table.
To operate acceptably TIN must perform transitions correctly when it can, recognize
when it cannot, signal for new data when it is required and expand the state charcterization
taole when it must. Happily TIN2 already provides the first two capabilities, and
combinations of TIN2 with rule-based controllers and with auxiliary control networks are
currently being explored as approachws to satisfy the latter (Winter, 1988b).
Nets like TIN2 may eventually prove useful as control elements in physical machines
because sequential automata can respond to unpredictable environments with a wide range
of behavior. Even very simple automata can repeat activities and make decisions based
upon environmental variations. Currently, most physical machines that make decisions
are dedicated to a single task; applying one to a new task requires re-programming by a

659

660

Winter

skilled technician. A programmer must, furthermore, determine a priori precisely which
machine state - environment associations are significant enough to warrant insertion in
the control structure of a given machine. TIN2, on the other hand, is trained, not
programmed, and can abstract significant associations from noisy input. It is a ""blank
slate"" that learns the structure of a particular sequential machine from examples.

References
D. Angluin, ""Learning Regular Sets from Queries and Counterexamples"", Information
and Computation, 75 (2), 1987.
M. A. Arbib and E. G. Manes, ""Machines in a Category: an Expository Introduction"",
SIAM Review, 16 (2), 1974.
M. A. Arbib and H. P. Zeiger, ""On the Relevance of Abstract Algebra to Control
Theory"", Automatica, 5, 1969.
G. Carpenter and S. Grossberg, ""A Massively Parallel Architecture for a Self-Organizing
Neural Pattern Recognition Machine"", Comput. Vision Graphics Image Process. 37 (54),
1987.
E. M. Gold, ""System Identification Via State Characterization"", Automatica, 8, 1972.
E. M. Gold, ""Complexity of Automaton Identification from Given Data"", Info. and
Control, 37, 1978.
A. Neroda, ""Linear Automaton Transformations"", Proc. Am. Math. Soc., 9, 1958.
T. W. Ryan and C. L. Winter, ""Variations on Adaptive Resonance"", in Proc. 1st IntI.
Conf. on Neural Networks, IEEE, 1987.
T. W. Ryan, C. L. Winter and C. J. Turner, ""Dynamic Control of an Artificial Neural
System: the Property Inheritance Network"", Appl. Optics, 261 (23) 1987.
V. V. Tolat and B. Widrow, ""An Adaptive Neural Net Controller with Visual Inputs"",
Neural Networks, I, S upp I, 1988.
C. L. Winter, T. W. Ryan and C. J. Turner, ""TIN: A Trainable Inference Network"", in
Proc. 1st Inti. Conf. on Neural Networks, 1987.
C. L. Winter, ""An Adaptive Network that Flees Pursuit"", Neural Networks, I, Supp.l,
1988a.
C. L. Winter, ""TIN2: An Adaptive Controller"", SAIC Tech. Rpt., SAIC, 5151 E.
Broadway, Tucson, AZ, 85711, 1988b.

Part V
Implementation

"
1030,1995,Neuron-MOS Temporal Winner Search Hardware for Fully-Parallel Data Processing,,1030-neuron-mos-temporal-winner-search-hardware-for-fully-parallel-data-processing.pdf,Abstract Missing,"Neuron-MOS Temporal Winner Search
Hardware for Fully-Parallel Data
Processing

Tadashi SHIBATA, Tsutomu NAKAI, Tatsuo MORIMOTO
Ryu KAIHARA, Takeo YAMASHITA, and Tadahiro OHMI
Department of Electronic Engineering
Tohoku University
Aza-Aoba, Aramaki, Aobaku, Sendai 980-77 JAPAN

Abstract
A unique architecture of winner search hardware has been developed using a novel neuron-like high functionality device called
Neuron MOS transistor (or vMOS in short) [1,2] as a key circuit
element. The circuits developed in this work can find the location
of the maximum (or minimum) signal among a number of input
data on the continuous-time basis, thus enabling real-time winner
tracking as well as fully-parallel sorting of multiple input data. We
have developed two circuit schemes. One is an ensemble of selfloop-selecting v MOS ring oscillators finding the winner as an oscillating node. The other is an ensemble of vMOS variable threshold
inverters receiving a common ramp-voltage for competitive excitation where data sorting is conducted through consecutive winner
search actions. Test circuits were fabricated by a double-polysilicon
CMOS process and their operation has been experimentally verified.

1

INTRODUCTION

Search for the largest (or the smallest) among a number of input data, Le., the
winner-take-all (WTA) action, is an essential part of intelligent data processing
such as data retrieval in associative memories [3], vector quantization circuits [4],
Kohonen's self-organizing maps [5] etc. In addition to the maximum or minimum
search, data sorting also plays an essential role in a number of signal processing
such as median filtering in image processing, evolutionary algorithms in optimizing
problems [6] and so forth . Usually such data processing is carried out by software
running on general purpose computers, but the computation time increases explo-

686

T. SHIBATA, T. NAKAI, T. MORIMOTO, R. KAIHARA, T. YAMASHITA, T. OHMI

sively with the increase in the volume of data. In order to build electronic systems
having a real-time-response capability, the direct implementation of fully parallel
algorithms on the integrated circuits hardware is critically demanded.
A variety of WTA [4, 7, 8) circuits have been implemented so far based on analog
current-mode circuit technologies. A number of cells, each composed of a current
source, competitively share the total current specified by a global current sink and
the winner is identified through the current concentration toward the cell via tacit
positive feedback mechanisms. The circuit implementations using MOSFET's operating in the subthreshold regime [4, 7) are ideal for large scale integration due to
its ultra low power nature. Although they are inherently slow at circuit levels, the
performance at a system level is far superior to digital counterparts owing to the
flexible computing algorithms of analog. In order to achieve a high speed operation, MOSFET's biased at strong inversion is also utilized in Ref. [8). However,
cost must be traded off for increased power.
What we are presenting in this paper is a unique WTA architecture implemented
by vMOS technology [1,2]. In vMOS circuits the summation of multiples of voltage
signals is conducted on the vMOS floating gate (or better be called ""temporary floating gate"" when used in a clocked scheme [9]) via charge sharing among capacitors,
and the result of the summation controls the transistor action. The voltage-mode
summation capability of vMOS has been uniquely utilized to produce the WTA
action. No DC current flows for the sum operation itself in contrast to the Kirchhoff sum. In vMOS transistors, however, DC current flows in a CMOS inverter
configuration when the floating gate is biased in the transition region. Therefore
the power consumption is larger than in the subthreshold circuitries. However, the
vMOS WTA's presented in this article will give an opportunity of high speed operation at much less power consumption than current-mode circuitries operating in the
strong inversion mode. In the following we present two kinds of winner search hardware featuring very fast operation. The winner can be tracked in a continuous-time
regime with a detection delay time of about lOOpsec, while the sorting of multiple
data is conducted in a fixed frame of time of about 100nsec.

2

NEURON-MOS CONTINUOUS-TIME WTA

Fig. 1(a) shows a schematic circuit diagram of a vMOS continuous-time WTA
for four input signals. Each signal is fed to an input-stage vMOS inverter-A: a

,ole 'Lc
0:71' .
V,.,-VA4

VA'-V ...

V ?? 1

Vs

l

~

o~
:
Va ~..
V.
V ?? 1
: v. (c)

(b)

'ole
VAI-VA4

::~fw

(a)

o~: v.

Voc1

l

~??

(d)

Figure 1: (a) Circuit diagram of vMOS continuous-time WTA circuit. (b)lV(d)
Response of VAl V A4 as & function of the floating-gate potential of vMOS inverterIV

A.

Neuron-MOS Temporal Winner Search Hardware for Fully-parallel Data Processing

687

CMOS inverter in which the common gate is made floating and its potential ,pFA
is determined via capacitance coupling with three input terminals. VI ('"" 'V4) and
VR are equally coupled to the floating gate and a small capacitance pulls down the
floating gate to ground. The vMOS inverter-B is designed to turn on when the
number of l's in its inputs (VAl'"" VA4) is more than 1. When a feedback loop is
formed as shown in the figure, it becomes a ring oscillator composed of odd-numbers
of inverter stages.

=

=

When Vi '"" V4
0, the circuit is stable with VR
1 because inverter-A's do not
turn on. This is because the small grounded capacitor pulls down the floating gate
potential ,pFA a little smaller than its inverting threshold (VDD/2) (see Fig. l(b)).
If non-zero signals are given to input terminals, more-than-one inverter-A's turn on
(see Fig. l(c)) and the inverter-B also turns on, thus initiating the transition of VR
from VDD to O. According to the decrease in VR, some of the inverter-A's turn off
but the inverter-B (number 1 detector) still stays at on-state until the last inverterA turns off. When the last inverter-A, the one receiving the largest voltage input,
turns off, the inverter-B also turns off and VR begins to increase. As a result, ring
oscillation occurs only in the loop including the largest-input inverter-A(Fig. l(d)).
In this manner, the winner is identified as an oscillating node. The inverter-B can
be altered to a number ""2"" detector or a number ""3"" detector etc. by just reducing
the input voltage to the largest coupling capacitor. Then it is possible for top two
or top three to be winners.

o

4()

.0

lOD

120

10

140

~~

31)

20

40

50

:~
VAi ???f]
? ? .: . fl?. . .., . ...-.
-

. . . ..

',

4fl;...

~

!

70

60

2~

. . . . . .. . ... .. . .. . . ...
;

o~, .. .

I

""

?

I.

.

. . 1.

'

..
..

w

CJ

<
....
-'

""

o

>

....

2""~
4

""'

... ...,. VA. :

J.

f ~..J.!~.........
o t..
i ~-'-'-'
! ,~.....i~...J
o
10
20
30
40
50

(a)

.

-~

o

TIME [JIaec]

~

~

TIME [nsec)

I

l

I

...

l

60

I

l

I

..

j
70

(b)

Figure 2: (a) Measured wave forms of four-input WTA as depicted in Fig. 1(80)
(bread board experoment) . (b) Simulation results for non-oscillating WTA explained in Fig. 3.
Fig. 2(80) demonstrates the measured wave forms of a bread-board test circuit
composed of discrete components for verifying the circuit idea. It is clearly seen that
ring oscillation occurs only at the temporal winner. However, the ring oscillation
increases the power dissipation, and therefore, non-oscillating circuitry would be
preferred. An example of simulation results for such a non-oscillating circuit is
demonstrated in Fig. 2(b).
Fig.

3(80) gives the circuit diagram of a non-oscillating version of the vMOS

688

T. SHIBATA. T. NAKAI. T. MORIMOTO. R. KAIHARA. T. YAMASHITA. T. OHMI

'1
~: I~I I

vMOS Inv.,.. r-A
Vt
Va

~

Va
V.

vMOS Inv_r-B

~T
V..

aD
? No,,-olCillalinl mod,

o Olcillatl,. mod,

1 aD

;0,2

0

!

()

.;=-

?

f ??
,,0.1i-

00
0

0

0

?

0

.R.O 1111

1[>.1>:

0

R

COXT~

VA

a

?RI'0)? ?

2000

10

??0

4000

CUT/c...

(b)
(c)
Figure 3: (a) Circuit diagram of non-oscillating-mode WTA. HSPICE simulation
results: (b) combinations of R and CEXT for non-oscillating mode; (c) winner
detection delay as a function of capacitance load.

(a)

continuous-time WTA. In order to suppress the oscillation, the loop gain is reduced
by removing the two-stage CMOS inverters in front of the inverter-B and RC delay
element is inserted in the feedback loop. The small grounded capacitors were removed in inverter-A's. The waveforms demonstrated in Fig. 2(b) are the HSPICE
simulation results with R = 0 and CEXT = 20Cgote(Cgote: input capacitance of
elemental CMOS inverter=5.16f.F) . The circuit was simulated assuming a typical
double-poly 0.5-pm CMOS process. Fig. 3(b) indicates the combinations of Rand
C EXT yielding the non-oscillating mode of operation obtained by HSPICE simulation. It is important to note that if CEXT ~ 15Cgote , non-oscillating mode appears
with R = O. This me8JlS the output resistance of the inverter-B plays the role of
R. When the number of inverter-A's is increased, the increased capacitance load
serves as CEXT. Therefore, WTA having more than 19 input signals C8Jl operate in
the non-oscillating mode. Fig. 3(c) represents the detection delay as a function of
CEXT. It is known that the increase in CEXT, therefore the increase in the number
of input signals to the WTA, does not significantly increase the detection delay and
that the delay is only in the r8Jlge of 100 to 200psec.
A photomicrograph of a test circuit of the non-oscillating mode WTA fabricated
by Tohoku-University st8Jldard double-polysilicon CMOS process on 3-pm design
rules, and the measurement results are shown in Fig. 4(80) and (b), respectively.
~

I-

v

/
:--- """"

1

V [\( Y?. ~ V
I""-

INPU T OAl ~
~~

o

(a)

I--

~

r-""---'V

OUTP TO ~TA

(b)

~

~

TIM E

V
v.

V

""-

I'-- /

""""""

....

VA'

[2511uc/dlv)

Figure 4: (a) Photomicrograph of a test circuit for 4-input continuous-time WTA.
Chip size is 800pmx500pm including all peripherals (3-pm rules). The core circuit
of Fig. 3(80) occupies approximately 0.12 mm2 ? (b) Measured wave forms.

Neuron-MOS Temporal Winner Search Hardware for Fully-parallel Data Processing

3

689

NEURON-MOS DATA SORTING CIRCUITRY

The elemental idea of this circuit was first proposed at ISSCC '93 [3] as an application of the vMOS WTA circuit. In the present work, a clocked-vMOS technique [9]
was introduced to enhance the accuracy and reliability of vMOS circuit operation
and test circuits were fabricated and their operation have been verified.
Fig. 5(80) shows the circuit diagram of a test circuit for sorting three analog data VA,
VB, and Vc , and a photomicrograph of a fabricated test circuit designed on 3-pm
rules is shown in Fig. 5(b). Each input stage is a vMOS inverter: a CMOS inverter
in which the common gate is made floating and its potential fjJ F is determined
by two input voltages via equa.lly-weighted capacitance coupling, namely fjJF =
(VA + VRAMP)/2. The reset signal forces the floating node be grounded, thus
cancelling the charge on the vMOS floating gate each time before sorting. This is
quite essential in achieving long-term reliability of vMOS operation. In the second
stage are flip-flop memory cells to store sorting results. The third stage is a circuit
which counts the number of 1's at its three input terminals and outputs the result in
binary code. The concept of the vMOS A/D converter design [10] has been utilized
in the circuit.

(a)

(b)

...............

(j) vMOS

@ Data latch

~

@ Counter

.

Inverter

Figure 5: (a) Circuit diagram of vMOS data-soring circuit. (b) Photomicrograph
of a test circuit fabricated by Tohoku Univ. Standard double-polysillicon CMOS
process (3-pm rules). Chip size is 1250pmxBOOpm including a.ll peripherals.
The sorting circuit is activated by ramping up VRAMP from OV to VDD. Then the
vMOS inverter receiving the largest input turns on first and the output data of the
counter at this moment (0,0) is latched in the respective memory cells. The counter
output changes to (0,1) after gate delays in the counter and this code is latched
when the vMOS inverter receiving the second largest turns on. Then the counter
counts up to (1,0). In this manner, the all input data are numbered according to
the order of their magnitudes after a ramp voltage scan is completed.
The measurement results are demonstrated in Fig. 6(80) in comparison with the
HSPICE simulation results. Simulation was carried out on the same architecture
circuit designed on O.5-pm design rules and operated under 3V power supply. For
three analog input voltages: VA = 5V, VB = 4V, and Vc = 2V, (0,0), (0,1),

T. SHIBATA, T. NAKAI, T. MORIMOTO, R. KAIHARA, T. YAMASHITA, T. OHMI

690

40

MEASUREMENT

~30

S

3-INPUT

SORnNG CIRCUIT

20

j 1:

~

4
8
6
2
SortIng Accuracy (bit ]

r

(b)

Ii

_100

L

~80
-eo

r

15-INPUT
SORTING CIRCUIT

S40
c:
~

20
0

2

6

4

8

SortIng Accuracy (bit ]

r '

10~/div

20nsec/civ

(a)

(c)

Figure 6: (a) Wave forms of the test circuit shown in Fig. 5(a) measured without
buffer circuitry (left) and simulation results of a circuit designed with 0.5-pm rules
(right). (b) Minimum scan time vs. sorting accuracy for a three-input sorter. (c)
Minimum scan time vs. sorting accuracy for a 15-input sorter.

and (1,0) are latched, respectively, after the ramp voltage scan, thus accomplishing
correct sorting. Slow operation of the test circuit is due to the loading effect caused
by the direct probing of the node voltage without output buffer circuitries. The
simulation with a 0.5-pm-design-rule circuit indicates the sorting is accomplished
within the scan time of 4Onsec.
In Fig. 6(b), the minimum scan time obtained by simulation is plotted as a function of the bit accuracy in sorting analog data. N -bit accuracy means the minimum
voltage difference required for winner discrimination is VDD/2 2 ? If the ramp rate
is too fast, the vMOS inverter receiving the next largest data turns on before the
correct counting results become available, leading to an erroneous operation. The
scan time/accuracy relation in Fig. 6(b) is primarily determined by the response
delay in the counter. It should be noted that the number of inverter stages in the
counter (vMOS A/D converter) is always three indifferent to the number of output
bits, namely, the delay would not increase significantly by the increase in the number of input data. In order to investigate this, a 15-input counter was designed and
the delay time was evaluated by HSPICE simulation. It was 312 psec in comparison
with 110 psec of the 3-input counter of Fig. 5(a). The scan time/accuracy relation
for the 15-input sorting circuit is shown in Fig. 6( c), indicating the sorting of 15
input data can be accomplished in 100 nsec with 8-bit accuracy.

Neuron-MOS Temporal Winner Search Hardware for Fully-parallel Data Processing

4

691

CONCLUSIONS

A novel neuron-like functional device liMOS has been successfully utilized in constructing intelligent electronic circuits which can carry out search for the temporal
winner. As a result, it has become possible to perform data sorting as well as
winner search in an instance, both requiring very time-consuming sequential data
processing on a digital computer. The hardware algorithms presented here are typical examples of the liMOS binary-multivalue-analog merged computation scheme,
which would play an important role in the future flexible data processing.
Acknowledgements
This work was partially supported by Grant-in-Aid for Scientific Research
(06402038) from the Ministry of Education, Science, Sports, and Culture, Japan. A
part of this work was carried out in the Super Clean Room of Laboratory for Electronic Intelligent Systems, Research Institute of Electrical communication, Tohoku
University.
References
[1] T. Shibata and T . Ohmi, ""A functional MOS transistor featuring gate-level
weighted sum and threshold operations,"" IEEE Trans. Electron Devices, Vol. 39,
No.6, pp.1444-1455 (1992).
[2] T. Shibata, K. Kotani, T. Yamashita, H. Ishii, H. Kosaka, and T. Ohmi, ""Implementing interlligence on silicon using neuron-like functional MOS transistors,"" in
Advances in Neural Information Processing Systems 6 (San Francisco, CA: Morgan
Kaufmann 1994) pp. 919-926.
[3] T. Yamashita, T. Shibata, and T. Ohmi, ""Neuron MOS winner-take-all circuit
and its application to associative memory,"" in ISSCC Dig. Tech. Papers, Feb. 1993,
FA 15.2, pp. 236-237.
[4] G. Gauwenberghs and V. Pedroni, "" A charge-based CMOS parallel analog vector
quantizer,"" in Advances in Neural Information Processing Systems 7 (Cambridge,
MA: The MIT Press 1995) pp. 779-786.
[5] T. Kohonen, Self-Organization and Associative Memory, 2nd ed. (New York:
Springer-Verlag 1988).
[6] M. Kawamata, M. Abe, and T. Higuchi, ""Evolutionary digital filters,"" in Proc.
Int. Workshop on Intelligent Signal Processing and Communication Systems, seoul,
Oct., 1994, pp. 263-268.
[7] J. Lazzaro, S. Ryckebusch, M. A. Mahowald, and C. A. Mead, ""Winner-TakeAll networks of O(N) complexity,"" in Advances in Neural Information Processing
Systems 1 (San Mateo, CA: Morgan Kaufmann 1989) pp. 703-711.
[8] J . Choi and B. J. Sheu, ""A high-precision VLSI winner-take-all circuit for selforganizing neural networks,"" IEEE J. Solid State Circuits, Vol. 28, No.5, pp.576584 (1993).
[9] K. Kotani, T. Shibata, M. Imai, and T. Ohmi, ""Clocked-Neuron-MOS logic
circuits employing auto-threshold-adjustment,"" in ISSCC Dig. Technical Papers,
Feb. 1995, FA 19.5, pp. 320-321.
[10] T. Shibata and T. Ohmi, ""Neuron MOS binary-logic integrated circuits: Part
II, Simplifying techniques of circuit configuration and their practical applications,""
IEEE Trans. Electron Devices, Vol. 40, No.5, 974-979 (1993).

"
1031,1995,Dynamics of Attention as Near Saddle-Node Bifurcation Behavior,,1031-dynamics-of-attention-as-near-saddle-node-bifurcation-behavior.pdf,Abstract Missing,"Dynamics of Attention as Near
Saddle-Node Bifurcation Behavior

Hiroyuki Nakahara""

Kenji Doya

General Systems Studies
U ni versi ty of Tokyo
3-8-1 Komaba, Meguro
Tokyo 153, Japan
nakahara@vermeer.c.u-tokyo.ac.jp

ATR Human Information Processing
Research Laboratories
2-2 Hikaridai, Seika, Soraku
Kyoto 619-02, Japan
doya@hip.atr.co.jp

Abstract
In consideration of attention as a means for goal-directed behavior in non-stationary environments, we argue that the dynamics of
attention should satisfy two opposing demands: long-term maintenance and quick transition. These two characteristics are contradictory within the linear domain. We propose the near saddlenode bifurcation behavior of a sigmoidal unit with self-connection
as a candidate of dynamical mechanism that satisfies both of these
demands. We further show in simulations of the 'bug-eat-food'
tasks that the near saddle-node bifurcation behavior of recurrent
networks can emerge as a functional property for survival in nonstationary environments.

1

INTRODUCTION

Most studies of attention have focused on the selection process of incoming sensory
cues (Posner et al., 1980; Koch et al., 1985; Desimone et al., 1995). Emphasis was
placed on the phenomena of causing different percepts for the same sensory stimuli.
However, the selection of sensory input itself is not the final goal of attention. We
consider attention as a means for goal-directed behavior and survival of the animal.
In this view, dynamical properties of attention are crucial. While attention has
to be maintained long enough to enable robust response to sensory input, it also
has to be shifted quickly to a novel cue that is potentially important. Long-term
maintenance and quick transition are critical requirements for attention dynamics.
?currently at Dept. of Cognitive Science and Institute for Neural Computation,
U. C. San Diego, La Jolla CA 92093-0515. hnakahar@cogsci.ucsd.edu

39

Dynamics of Attention as Near Saddle-node Bifurcation Behavior

We investigate a possible neural mechanism that enables those dynamical characteristics of attention.
First, we analyze the dynamics of a network of sigmoidal units with self-connections.
We show that both long-term maintenance and quick transition can be achieved
when the system parameters are near a ""saddle-node bifurcation"" point . Then, we
test if such a dynamical mechanism can actually be helpful for an autonomously
behaving agent in simulations of a 'bug-eat-food' task. The result indicates that
near saddle-node bifurcation behavior can emerge in the course of evolution for
survival in non-stationary environments.

2

NEAR SADDLE-NODE BIFURCATION BEHAVIOR

When a pulse-like input is given to a linear system, the rising and falling phases
of the response have the same time constants. This means that long-term maintenance and quick transition cannot be simultaneously achieved by linear dynamics.
Therefore, it is essential to consider a nonlinear dynamical mechanism to achieve
these two demands.

2.1

DYNAMICS OF A SELF-RECURRENT UNIT

First, we consider the dynamics of a single sigmoidal unit with the self-connection
weight a and the bias b.

y(t

+ 1)

F(ay(t)

+ b) ,

(1)

F(x)

1
1 + exp( -x)'

(2)

The parameters (a, b) determine the qualitative behavior of the system such as the
number of fixed points and their stabilities. As we change the parameters , the
qualitative behavior of the system may suddenly change. This is referred to as
""bifurcation"" (Guckenheimer, et al., 1983). A typical example is a ""saddle-node
bifurcation"" in which a pair of fixed points, one stable and one unstable, emerges.
In our system, this occurs when the state transition curve y(t + 1) = F(ay(t) + b) is
tangent to y(t + 1) = y(t). Let y* be this point of tangency. We have the following
condi tion for saddle-node bifurcation.

F(ay*
dF(ay + b)
dy

+ b)

I

b =

(3)

1

( 4)

y=y.

These equations can be solved, by noting F'(x)
a

y*

= F(x)(l- F(x)), as

1

(5)

y* (1 - y*)
1

F-1(y*) - ay* = F-l(y*) - - I - y*

(6)

By changing the fixed point value y* between a and 1, we can plot a curve in the
parameter space (a, b) on which saddle-node bifurcation occurs, as shown in Figure
1 (left). A pair of a saddle point and a stable fixed point emerges or disappears
when the parameters pass across the cusp like curve (cases 2 and 4) . The system
has only one stable fixed point when the parameters are outside the cusp (case 1)
and three fixed points inside the cusp (case 3).

H.NAKAHARA,K.DOYA

40

y ( t+ l )

b

Bifurcat ion Diagr am

y(t +1 J

CASE 1

~

06,

0 61

"",

041

O~/

.,

H

-'0

""

0

../

,,'

""i

""

""""0~
1O:"".,,,,0';;;-0;;-;
8 lY lt )

Y'~tL
? "" CA
SE3

Y',t."" CASE?

o. a:

0 B

(I

,,/

'

't1x.d p t ., ""' 3
6
'

,

OJ'
o

- lO

8:tixed pts,'

0'1/

0.20. 4 0.60 8 ly(tJ

.

- 15

CASE 2

""

C. 8f::.x.d Pt ?. , ~' 1

,/

0 6

,'

0""
0 2:

'0 20 40 60 8 1 y( t )

.,,'.

""

.

f lxed p ts "" . 2
'
'

0 2('

4~

60 8: 1 y (tJ

Figure 1: Bifurcation Diagram of a Self-Recurrent Unit . Left : the curve in the
parameter space (a , b) on which saddle-node bifurcation is seen. Right : state transition diagrams for four different cases.
y ( t ? l)

Y
(t:l)ll.ll11
1d
=~9

o'~=Lil
l. 1111 b =- 7. 9

o.

0.6

O.

I

0. 4

0 .2
a

""

0.20. 4 0 . 60. 81

I
I

O.
O.

I

... ""

b

y et)

I

o

o . 20 . 4 0

yet)
60. 8 1

yet)

y et )

L

o.~

do. &

o.

0.21

o.
o.

o

""

,I

5

10 15 2 (f i me (t)

0 . 41

o

5

1 0 l S 2a:'i rne l t )

Figure 2: Temporal Responses of Self-Recurrent Units. Left : near saddle-node
bifurcation. Right : far from bifurcation.
An interesting behavior can be seen when the parameters are just outside the cusp,
as shown in Figure 2 (left) . The system has only one fixed point near Y = 0, but
once the unit is activated (y ~ 1) , it stays ""on"" for many time steps and then goes
back to the fixed point quickly. Such a mechanism may be useful in satisfying the
requirements of attention dynamics: long-term maintenance and quick transition.

2.2

NETWORK OF SELF-RECURRENT UNITS

Next, we consider the dynamics of a network of the above self-recurrent units.

Yi(t

+ 1) = F[aYi(t) + b + L

CijYj(t)

+ diUi(t)],

(7)

j,jti

where a is the self connection weight , b is the bias, Cij is the cross connection weight,
and di is the input connection weight , and Ui(t) is the external input. The effect of
lateral and external inputs is equivalent to the change in the bias, which slides the
sigmoid curve horizontally without changing the slope.
For example, one parameter set of the bifurcation at y* = 0.9 is a = 11.11 and
b ~ -7.80. Let b = -7.90 so that the unit has a near saddle-node bifurcation
behavior when there is no lateral or external inputs. For a fixed a = 11.11, as we
increase b, the qualitative behavior of the system appears as case 3 in Figure 1, and

41

Dynamics of Attention as Near Saddle-node Bifurcation Behavior
Sensory
Inputs
,
,
'olr lood

Actions

Network Structure

""

- '--,~ ,- \, :/~ - - - .

r-.~ ""==:-:""~-

~~ .......

'on:'oocl

~~

-

.....

Creature

. . ...

...

Creature

Inpol. IJrI .111'2

Figure 3: A Creature's Sensory Inputs(Left), Motor System(Center) and Network
Architecture(Right)
then, it changes again at b:::::: -3.31, where the fixed point at Y = 0.1, or another
bifurcation point , appears as case 4 in Figure L Therefore , ifthe input sum is large
enough, i.e . L j ,j;Ci CijYj + diuj > -3.31- (-7.90) :::::: 4.59, the lower fixed point
at Y = 0.1 disappears and the state jumps up to the upper fixed point near Y = 1,
quickly turning the unit ""on"". If the lateral connections are set properly, this can
in turn suppress the activation of other units. Once the external input goes away,
as we see in Figure 2 (left), the state stays ""on"" for a long time until it returns to
the fixed point near Y = O.

3

EVOLUTION OF NEAR BIFURCATION DYNAMICS

In the above section, we have theoretically shown the potential usefulness of near
saddle-node bifurcation behavior for satisfying demands for attention dynamics. We
further hypothesize that such behavior is indeed useful in animal behaviors and can
be found in the course of learning and evolution of the neural system.
To test our hypothesis, we simulated a 'bug-eat-food ' task . Our purpose in t.his
simulation was to see whether the attention dynamics discussed in the previous
section would help obtain better performance in a non-stationary environment. Vve
used evolutionary programming (Fogel et aI, 1990) to optimize the performance of
recurrent networks and feedforward networks.

3.1

THE BUG AND THE WORLD

In our simulation, a simple creature traveled around a non-stationary environment.
In the world, there were a certain number of food items. Each item was fixed at a
certain place in the world but appeared or disappeared in a stochastic fashion, as
determined by a two-state Markov system. In order to survive, A creature looked
for food by traveling the world . The amount of food a creature found in a certain
time period was the measure of its performance.
A creature had five sensory inputs, each of which detected food in the sector of 45
degrees (Figure 3, right). Its output level was given by L J' .l..,
where Tj ,""vas the
rJ
distance to the j-th food item within the sector. Note that the format of the input
contained information about distance and also that the creature could only receive
the amount of the input but could not distinguish each food from others.
For the sake of simplicity, we assumed that the creature lived in a grid-like world .
On each time step, it took one of three motor commands: L: turn left (45 degrees),

H. NAKAHARA, K. DOYA

42

Density of Food
Markov Transition Matrix
of each food
Random Walk
Nearest Visible
FeedForward
Recurrent
Nearest Visible/Invisible

0.05
.5 .5 .8 .8
.5 .5 .2 .2
7.0
6.9
42.7 18.6
58.6 37.3
65.7 43.6
97.7 97.1

0.10
.5 .5
.8 .8
.2 .2
.5 .5
13.8
13.9
65.3
32.4
84.8
60.0
94.0
66.1
129.1 128.8

Table 1: Performances of the Recurrent Network and Other Strategies.

C: step forward, and R: turn right (Figure 3, center). Simulations were run with
different Markov transition matrices of food appearance and with different food
densities. A creature got the food when it reached the food, whether it was visible
or invisible. When a creature ate a food item, a new food item was placed randomly.
The size of the world was 10x10 and both ends were connected as a torus.
A creature was composed of two layers: visual layer and motor layer (Figure 3,
left). There were five units 1 in visual layer, one for each sensory input, and their
dynamics were given by Equation (7). The self-connection a, the bias b and the
input weight di were the same for all units. There were three units in motor layer ,
each coding one of three motor commands, and their state was given by

ek

+ L: fkiYi(t),

exp(xk(t))

L:/ exp(x/(t)) '

(8)

(9)

where ek was the bias and fki was the feedforward connection weight. 2 One of the
three motor commands (L,C,R) was chosen stochastically with the probability Pk
(k=L,C,R). The activation pattern in visual layer was shifted when the creature
made a turn, which should give proper mapping between the sensory input and the
working memory.

3.2

EVOLUTIONARY PROGRAMMING

Each recurrent network was characterized by the parameters (a,b,Cij,di,ek,lkd,
some of which were symmetrically shared, e.g. C12 = C21. For comparison, we
also tested feedforward networks where recurrent connections were removed, i.e.
a
Cij
O.

=

=

A population of 60 creatures was tested on each generation. The initial population
was generated with random parameters. Each of the top twenty scoring creatures
produced three offspring; one identical copy of the parameters of the parent's and
two copies of these parameters with a Gaussian fluctuation. In this paper, we report
the result after 60 generations.

3.3

PERFORMANCE

1 We denote each unit in visual layer by Ul, U2, U3, U4, Us from the left to the right for
the later convenience
2In this simulation reported here, we set ek = O.

Dynamics of Attention as Near Saddle-node Bifurcation Behavior

-,

-, ,
-,

-, ,
-,

-7 ,

_7

,

- 10
- 12 . 5

,

43

......: ....
""

-L25

a

b

""Transition matrix

= ( :~

.5 )

.5

bTransition matrix = (

:~

:~ )

Figure 4: The Convergence of the Parameter of (a , b) by Evolutionary Programming
Plotted in the Bifurcation Diagram. The food density is 0.10 in both examples
above .
Table 1 shows the average of food found after 60 generations. As a reference of
performance level, we also measured the performances of three other simple algorithms: 1) random walk : one of the three motor commands is taken randomly with
equal probability. 2) nearest visible: move toward the nearest food visible at the
time within the creature's field of view of (U2, U3, U4). 3) nearest visible/invisible:
move toward the nearest food within the view of (U2, U3, U4) no matter if it is visible
or not, which gives an upper bound of performance.
The performance of recurrent network is better than that of feedforward network
and 'nearest visible'. This suggests that the ability of recurrent network to remember the past is advantageous.
The performance of feedforward network is better than that of 'nearest visible '.
One reason is that feedforward network could cover a broader area to receive inputs
than 'nearest visible' . In addition, two factors, the average time in which a creature
reaches the food and the average time in which the food disappears, may influence
the performance of feedforward network and 'nearest visible'. Feedforward network
could optimize its output to adapt two factors with its broader view in evolution
while 'nearest visible' did not have such adaptability.
It should be noted that both of 'nearest visible/invisible ' and 'nearest visible' explicitly assumed the higher-order sensory processing: distinguishing each food item from
the others and measuring the distance between each food and its body. Since its performance is so different regardless of its higher-order sensory processing, it implies
the importance of remembering the past. We can regard recurrent network as compromising two characteristics, remembering the past as 'nearest visible/invisible'
did and optimizing the sensitivity as feedforward network did , although recurrent
network did not have a perfect memory as 'nearest visible/invisible' .

3.4

CONVERGENCE TO NEAR-BIFURCATION REGIME

We plotted the histogram of the performance in each generation and the history of
the performance of a top-scoring creature over generations. Though they are not
shown here, the performance was almost optimal after 60 generations.
Figure 4 shows that two examples of a graph in which we plotted the parameter

H. NAKAHARA, K. DOYA

44

set (a , b) of top twenty scoring creatures in the 60th generation in the bifurcation
diagram. In the left graph, we can see the parameter set has converged to a regime
that gives a near saddle-node bifurcation behavior. On the other hand, in the right
graph, the parameter set has converged into the inside of cusp. It is interesting
to note that the area inside of the cusp gives bistable dynamics. Hence, if the
input is higher than a repelling point, it goes up and if the input is lower , it goes
down . The reason of the convergence to that area is because of the difference of
the world setting, that is, a Markov transition matrix. Since food would disappear
more quickly and stay invisible longer in the setting of the right graph, it should
be beneficial for a creature to remember the direction of higher inputs longer . In
most of cases reported in Table 1, we obtained the convergence into our predicted
regime and/or the inside of the cusp.

4

DISCUSSION

Near saddle-node bifurcation behavior can have the long-term maintenance and
quick transition, which characterize attention dynamics. A recurrent network
has better performance than memoryless systems for tasks in our simulated nonstationary environment. Clearly, near saddle-node bifurcation behavior helped a
creature's survival and in fact, creatures actually evolved to our expected parameter regime . However, we also obtained the convergence into another unexpected
regime which gives bistable dynamics . How the bistable dynamics are used remains
to be investigated.
Acknowledgments
H.N . is grateful to Ed Hutchins for his generous support, to John Batali and David
Fogel for their advice on the implementation of evolutionary programming and to
David Rogers for his comments on the manuscript of this paper.
References
R. Desimone, E. K. Miller , L. Chelazzi, & A. Lueschow. (1995) Multiple Memory
Systems in the Visual Cortex. In M. Gazzaniga (ed .) , The Cognitive Neurosciences,
475-486. MIT Press.
D. B. Fogel, L. J. Fogel, & V. W . Porto. (1990) Evolving Neural Networks. Biological cybernetics 63:487-493.
J. Guckenheimer & P. Homes. (1983) Nonlinear Oscillations, Dynamical Systems,
and Bifurcation of Vector Fields

C. Koch & S. Ullman . (1985) Shifts in selective visual attention:towards the underlying neural circuitry. Human Neurobiology 4:219-227 .
M. Posner , C .. R .R. Snyder, & B. J. Davidson. (1980) Attention and the detection
of signals. Journal of Experimental Psychology: General 109:160-174

"
1032,1995,VLSI Model of Primate Visual Smooth Pursuit,,1032-vlsi-model-of-primate-visual-smooth-pursuit.pdf,Abstract Missing,"VLSI Model of Primate Visual Smooth Pursuit

Ralph Etienne-Cummings

Jan Van der Spiegel

Department of Electrical Engineering,
Southern Illinois University, Carbondale,
IL 62901

Moore School of Electrical Engineering,
University of Pennsylvania, Philadelphia,
PA 19104

Paul Mueller
Corticon, Incorporated,
3624 Market Str, Philadelphia,
PA 19104

Abstract
A one dimensional model of primate smooth pursuit mechanism has
been implemented in 2 11m CMOS VLSI. The model consolidates
Robinson's negative feedback model with Wyatt and Pola's positive
feedback scheme, to produce a smooth pursuit system which zero's the
velocity of a target on the retina. Furthermore, the system uses the
current eye motion as a predictor for future target motion. Analysis,
stability and biological correspondence of the system are discussed. For
implementation at the focal plane, a local correlation based visual
motion detection technique is used. Velocity measurements, ranging
over 4 orders of magnitude with < 15% variation, provides the input to
the smooth pursuit system. The system performed successful velocity
tracking for high contrast scenes. Circuit design and performance of the
complete smooth pursuit system is presented.

1 INTRODUCTION
The smooth pursuit mechanism of primate visual systems is vital for stabilizing a region
of the visual field on the retina. The ability to stabilize the image of the world on the
retina has profound architectural and computational consequences on the retina and visual
cortex, such as reducing the required size, computational speed and communication
hardware and bandwidth of the visual system (Bandera, 1990; Eckert and Buchsbaum,
1993). To obtain similar benefits in active machine vision, primate smooth pursuit can
be a powerful model for gaze control. The mechanism for smooth pursuit in primates
was initially believed to be composed of a simple negative feedback system which
attempts to zero the motion of targets on the fovea, figure I (a) (Robinson, 1965).
However, this scheme does not account for many psychophysical properties of smooth

707

VLSI Model of Primate Visual Smooth Pursuit

pursuit, which led Wyatt and Pola (1979) to proposed figure l(b), where the eye
movement signal is added to the target motion in a positive feed back loop. This
mechanism results from their observation that eye motion or apparent target motion
increases the magnitude of pursuit motion even when retinal motion is zero or constant.
Their scheme also exhibited predictive qualities, as reported by Steinbach (1976). The
smooth pursuit model presented in this paper attempts the consolidate the two models
into a single system which explains the findings of both approaches.
Target
Moticn

Eye
Motion

Retinal
Motion

e~

lee

G

ee = e t G+l
~;

>

I
G ~ co G r

Target
Motion

Eye
Motion

e~~

>

=0
(b)

(a)

Figure I: System Diagrams of Primate Smooth Pursuit Mechanism.
(a) Negative feedback model by Robinson (1965). (b) Positive
feedback model by Wyatt and Pola (1979).
The velocity based smooth pursuit implemented here attempts to zero the relative velocity
of the retina and target. The measured retinal velocity, is zeroed by using positive
feedback to accumulate relative velocity error between the target and the retina, where the
accumulated value is the current eye velocity. Hence, this model uses the Robinson
approach to match target motion, and the Wyatt and Pola positive feed back loop to
achieve matching and to predict the future velocity of the target. Figure 2 shows the
system diagram of the velocity based smooth pursuit system. This system is analyzed
and the stability criterion is derived. Possible computational blocks for the elements in
figure I (b) are also discussed. Furthermore, since this entire scheme is implemented on a
single 2 /lm CMOS chip, the method for motion detection, the complete tracking circuits
and the measured results are presented.
Retinal
Motion

Eye
Motion

er

Figure 2: System Diagram of VLSI Smooth Pursuit Mechanism.
is target velocity in space, Bt is projected target velocity, Be is the eye
velocity and Br is the measured retinal velocity.

2 VELOCITY BASED SMOOTH PURSUIT
Although figure I (b) does not indicate how retinal motion is used in smooth pursuit, it
provides the only measurement of the projected target motion. The very process of
calculating retinal motion realizes negative feed back between the eye movement and the
target motion, since retinal motion is the difference between project target and eye
motion. If Robinson's model is followed, then the eye movement is simply the
amplified version of the retinal motion. If the target disappears from the retina, the eye
motion would be zero. However, Steinbach showed that eye movement does not cea~
when the target fades off and on, indicating that memory is used to predict target motion.
Wyatt and Palo showed a direct additive influence of eye movement on pursuit. However,
the computational blocks G' and a of their model are left unfilled.

R. ETIENNE-CUMMINGS, J. VAN DER SPIEGEL, P. MUELLER

708

In figure 2, the gain G models the internal gain of the motion detection system , and the
internal representation of retinal velocity is then Vr. Under zero-slip tracking, the retinal
velocity is zero. This is obtained by using positive feed back to correct the velocity error
and eye,
The delay element represents a memory of the last eye
between target,
velocity while the current retinal motion is measured. If the target disappears, the eye
motion continues with the last value, as recorded by Steinbach, thus anticipating the
position of the target in space. The memory also stores the current eye velocity during
perfect pursuit. The internal representation of eye velocity, Ve , is subsequently amplified
by H and used to drive the eye muscles. The impulse response of the system is given in
equations (I). Hence, the relationship between eye velocity and target velocity is recursive
and given by equations (2). To prove the stability of this system, the retinal velocity can
be expressed in terms of the target motion as given in equations (3a). The ideal condition
for accurate performance is for GH = 1. However, in practice, gains of different amplifiers

er,

()

z-)

=GH--_-)

-.f..(z)

1- Z

(}r

ee.

()

(a); ~(I1)
(}r

=GH[-8(11) + u(n)]

(I)

(b)
n-)

(}e(n)

= (},(n) -

(}r(n)

=GH[-8(n) + u(n)] * (}r(n) = GHL(},.(k)

(2)

k=O
() r ( 11)

() r (n)

= (),( n ) (1
11

~

00

)

11

- GH)
0

if 11 -

=> () r( 1l )

I

GH < 1

= 0 if

GH

= 1 =>

() in)

= (),( 11 )

=> 0 < GH < 2 for stability

(

a)

(3)

( b)

are rarely perfectly matched. Equations (3b) shows that stability is assured for O<GH< 2.
Figure 3 shows a plot of eye motion versus updates for various choices of GH. At each
update, the retinal motion is computed. Figure 3(a) shows the eye's motion at the on-set
of smooth pursuit. For GH = 1, the eye movement tracks the target's motion exactly,
and lags slightly only when the target accelerates. On the other hand, if GH? I, the
eye's motion always lags the target's. If GH -> 2, the system becomes increasing
unstable, but converges for GH < 2. The three cases presented correspond to the smooth
pursuit system being critically, over and under damped, respectively.

3 HARDWARE IMPLEMENTATION
Using the smooth pursuit mechanism described, a single chip one dimensional tracking
system has been implemented. The chip has a multi-layered computational architecture,
similar to the primate's visual system. Phototransduction, logarithmic compression,
edge detection, motion detection and smooth pursuit control has been integrated at the
focal-plane. The computational layers can be partitioned into three blocks, where each
block is based on a segment of biological oculomotor systems.

3.1

IMAGING AND PREPROCESSING

The first three layers of the system mimics the photoreceptors, horizontal cells arx:l
bipolar cells of biological retinas. Similar to previous implementations of silicon
retinas, the chip uses parasitic bipolar transistors as the photoreceptors. The dynamic
range of photoreceptor current is compressed with a logarithmic response in low light arx:l
square root response in bright light. The range compress circuit represents 5-6 orders of
magnitude of light intensity with 3 orders of magnitude of output current dynamic range.
Subsequently, a passive resistive network is used to realize a discrete implementation of a
Laplacian edge detector. Similar to the rods and cones system in primate retinas, the
response time, hence the maximum detectable target speed, is ambient intensity dependent
(160 (12.5) Ils in 2.5 (250) IlW/cm2). However, this does prevent the system from
handling fast targets even in dim ambient lighting.

VLSI Model of Primate Visual Smooth Pursuit

~

g

u
>

709

20

20

15

15

10

10

5

~

5

0

]

-5

>"" -5

- 10

?

? 10

Target

- -Eye: GH=I 99
- E ye GH=IOO
__ . Eye: GH=O_IO

-15

0

? 15
-20

-20
100

50

0

150

500

600

Updates

(a)

700
800
Updates

900

1000

(b)

Figure 3: (a) The On-Set of Smooth Pursuit for Various GH Values.
(b) Steady-State Smooth Pursuit.

3.2

MOTION MEASUREMENT

This computational layer measures retinal motion. The motion detection technique
implemented here differs from those believed to exist in areas V 1 and MT of the primate
visual cortex. Alternatively, it resembles the fly's and rabbit's retinal motion detection
system (Reichardt, 1961; Barlow and Levick, 1965; Delbruck, 1993). This is not
coincidental, since efficient motion detection at the focal plane must be performed in a
small areas and using simple computational elements in both systems.
The motion detection scheme is a combination of local correlation for direction
determination, and pixel transfer time measurement for speed. In this framework, motion
is defined as the disappearance of an object, represented as the zero-crossings of its edges,
at a pixel , followed by its re-appearance at a neighboring pixel. The (dis)appearance of
the zero-crossing is determined using the (negative) positive temporal derivative at the
pixel. Hence, motion is detected by AND gating the positive derivative of the zerocrossing of the edge at one pixel with the negative derivative at a neighboring pixel. The
direction of motion is given by the neighboring pixel from which the edge disappeared.
Provided that motion has been detected at a pixel, the transfer time of the edge over the
pixel's finite geometry is inversely proportional to its speed.
Equation (4) gives the mathematical representation of the motion detection process for an
object moving in +x direction. In the equation. f,(l.'k ,y.t) is the temporal response of
pixel k as the zero crossing of an edge of an object passes over its 2a aperture. Equation
(4) gives the direction of motion, while equation (5) gives the speed. The schematic of

motion _ x = [

f f,( l: k, y, t) > 0] [ f f t(l.' k + J, y, t) < 0] =0

motion+x=[~f,(l.'k-J,y,t)<O][~f/l.'k , y,t?O]

= 8[t
Motion.' t m =

Speed + x

=

t

-

(b)

(4)

2a(k-n)-a
v
]8[x - 2ak]
x

2a(k -n) -a
vx

J
- t

( a)

vx

2a

Disappear .' t d

2a(k -n) +a

= --~--?
vx

(5)

d
m
the VLSI circuit of the motion detection model is shown in figure 4(a). Figure 4(b)
shows reciprocal of the measured motion pulse-width for 1 D motion. The on-chip speed,
et, is the projected target speed. The measured pulse-widths span 3-4 orders magnitude,

710

R. ETIENNE-CUMMINGS, J. VAN DER SPIEGEL, P. MUELLER

One-Over Pulse-Width vs On-Chip Speed

?

O.R

~

0.4

""

~ -0.0 +--------::II~-----__+
M
~ -0.4

---e-- \IPW_Lefi

-0 .8

- - . - - IIPW_ Rlght

- 1.2 +-'----'--''-+--'--'--'--t---'--''--'-+-'--'--'-t-'--'--'-t---'--'--'-+
-40
00
4.0
8.0
12.0
-12.0
-R.O
On-Chip Speed rcml~J

Right

Left

(b)

(a)

Figure 4: (a) Schematic of the Motion Detection Circuit.
Measured Output of the Motion Detection Circuit.

(b)

depending on the ambient lighting, and show less than 15% variation between chips,
pixels, and directions (Etienne-Cummings, 1993).

3.3

THE SMOOTH PURSUIT CONTROL SYSTEM

The one dimensional smooth pursuit system is implemented using a 9 x I array of
motion detectors. Figure 5 shows the organization of the smooth pursuit chip. In this
system, only diverging motion is computed to reduce the size of each pixel. The outputs
of the motion detectors are grouped into one global motion signal per direction. This
grouping is performed with a simple, but delayed, OR, which prevents pulses from
neighboring motion cells from overlapping. The motion pulse trains for each direction
are XOR gated, which allows a single integrator to be used for both directions, thus
limiting mis-match_ The final value of the integrator is inversely proportional to the
target's speed. The OR gates conserve the direction of motion. The reciprocal of the
integrator voltage is next computed using the linear mode operation of a MOS transistor
(Etienne-Cummings, 1993). The unipolar integrated pulse allows a single inversion
circuit to be used for both directions of motion, again limiting mis-match. The output of
the ""one-over"" circuit is amplified, and the polarity of the measured speed is restored.
This analog voltage is proportional to retinal speed.
The measured retinal speed is subsequently ailed to the stored velocity. Figure 6 shows
the schematic for the retinal velocity accumulation (positive feedback) and storage (analog
Wave Forms

Motion Pulse Integration
and ""One-Over""
V = GIRetinal Velocityl

Polarity
Restoration

Retinal Velocity
Accumulation
and Sample/Hold

Figure 5: Architecture of the VLSI Smooth Pursuit System. Sketches
of the wave forms for a fast leftward followed by a slow rightward
retinal motion are shown.

711

VLSI Model of Primate Visual Smooth Pursuit

memory). The output of the XOR gate in figure 5 is used by the sample-and-hold circuit
to control sampling switches S I and S2. During accumulation, the old stored velocity
value, which is the current eye velocity, is isolated from the summed value. At the
falling edge of the XOR output, the stored value on C2 is replaced by the new value on
Cl. This stored value is amplified using an off chip motor driver circuit, and used to
move the chip. The gain of the motor driver can be finely controlled for optimal
operation.

Motor

Retinal
Velocity

System

Accumulatiun

Target
Velocity

Two Phase Sample/Hold

Figure 6: Schematic Retinal Velocity Error Accumulation, Storage and
Motor Driver Systems.
Figure 7(a) shows a plot of one-over the measured integrated voltage as a function of on
chip target speed. Due to noise in the integrator circuit, the dynamic range of the motion
detection system is reduced to 2 orders of magnitude. However, the matching between left
and right motion is unaffected by the integrator. The MaS ""one-over"" circuit, used to
compute the analog reciprocal of the integrated voltage, exhibits only 0.06% deviation
from a fitted line (Etienne-Cummings, 1993b). Figure 7(b) shows the measured
increments in stored target velocity as a function of retinal (on-chip) speed. This is a test
of all the circuit components of the tracking system. Linearity between retinal velocity
increments and target velocity is observed, however matching between opposite motion
has degraded. This is caused by the polarity restoration circuit since it is the only
location where different circuits are used for opposite motion. On average, positive
increments are a factor of 1.2 times larger than negative increments. The error bars shows
the variation in velocity increments for different motion cells and different Chips. The
deviation is less than 15 %. The analog memory has a leakage of 10 mV/min and an
asymmetric swing of 2 to -1 V, caused by the buffers. The dynamic range of the
complete smooth pursuit system is measured to be 1.5 orders magnitude. The maximum
speed of the system is adjustable by varying the integrator charging time. The maximum
speed is ambient intensity dependent and ranges from 93 cmls to 7 cm/s on-chip speed in
Velocity Error Increment vs On-Chip Speed

Integrated Pulse vs On-Chip Speed
1.4
24

~

16

~

8

~

0

il
?
oS

.'
._

1.2

~

l'! 1.0

""e~
u

-t--------"",/II!...------+

-8

.s

O.R

g 0 .6

LLl

.::;.

g 04

:: -16
-e--lnlPuI~_l..xft

-24

_ _? _

JntPlllo;e_Rl~hl

-32 -t-'---'---'-'--+-'--~~-t--'""-'-~_t_--""--''---'---""-t
10.0
-100
-5.0
0.0
5.0
On-Chip Speed lemlsl

(a)

OJ

>

- - - - . Nc~_ Jn c rt~nl

02

__ ? _ _Po,,_Incremclll

0.0
0

4
6
On-Chip Speed lem/s)

(b)

Figure 7. (a) Measured integrated motion pulse voltage. (b) Measured
output for the complete smooth pursuit system.

10

R. ETIENNE-CUMMINGS, J. VAN DER SPIEGEL, P. MUELLER

712

bright (250 JlW/cm 2) and dim (2.5 JlW/cm 2) lighting, respectively. However, for any
maximum speed chosen, the minimum speed is a factor of 0.03 slower. The minimum
speed is limited by the discharge time of the temporal differentiators in the motion
detection circuit to 0.004 cmls on chip. The contrast sensitivity of this system proved to
be the stumbling block, and it can not track objects in normal indoor lighting. However,
all circuits components tested successfully when a light source is used as the target.
Additional measured data can be found in (Etienne-Cummings, 1995). Further work will
improve the contrast sensitivity, combat noise and also consider two dimensional
implementations with target acquisition (saccades) capabilities.

4

CONCLUSION

A model for biological and silicon smooth pursuit has been presented. It combines the
negative feed back and positive feedback models of Robinson and Wyatt and Pola. The
smooth pursuit system is stable if the gain product of the retinal velocity detection
system and the eye movement system is less than 2. VLSI implementation of this
system has been performed and tested. The performance of the system suggests that wide
range (92.9 - 0.004 cmls retinal speed) target tracking is possible with a single chip focal
plane system. To improve this chip's performance, care must be taken to limit noise,
improve matching and increase contrast sensitivity. Future design should also include a
saccadic component to re-capture escaped targets, similar to biological systems.

References
C. Bandera, ""Foveal Machine Vision Systems"", Ph.D. Thesis, SUNY Buffalo, New
York, ]990
H. Barlow and W. Levick, 'The Mechanism for Directional Selective Units in Rabbit' s
Retina"", Journal of Physiology, Vol. 178, pp. 477-504, ]965
T. Delbruck, ""Silicon Retina with Correlation-Based, Velocity-Tuned Pixels "", IEEE
Transactions on Neural Networks, Vol. 4:3, pp. 529-41, 1993

M. Eckert and G. Buchsbaum, ""Effect of Tracking Strategies on the Velocity Structure of
Two-Dimensional Image Sequences"", J. Opt. Soc. Am., Vol. AIO:7, pp. 1582-85, 1993
R. Etienne-Cummings et at., ""A New Temporal Domain Optical Flow Measurement
Technique for Focal Plane VLSI Implementation"", Proceedings of CAMP 93, M.
Bayoumi, L. Davis and K. Valavanis (Eds.), pp. 24]-25] , 1993
R. Etienne-Cummings, R. Hathaway and J. Van der Spiegel, ""An Accurate and Simple
CMOS 'One-Over' Circuit"", Electronic Letters, Vol. 29-18, pp. ]618-]620, 1993b
R. Etienne-Cummings et aI., ""Real-Time Visual Target Tracking: Two Implementations
of Velocity Based Smooth Pursuit"", Visual Information Processing IV, SPIE Vol. 2488,
Orlando, 17-18 April 1995

W. Reichardt, ""Autocorrelation, A Principle for the Evaluation of Sensory Information by
the Central Nervous System"", Sensory Communication, Wiley, New York, 1961
D. Robinson, ""The Mechanism of Human Smooth Pursuit Eye Movement"", Journal of
Physiology ( London) Vol. 180, pp. 569-591 , 1965
M. Steinbach, ""Pursuing the Perceptual Rather than the Retinal Stimuli"", Vision
Research, Vol. 16, pp. 1371-1376,1976
H. Wyatt and J. Pola, ""The Role of Perceived Motion in Smooth Pursuit Eye
Movements"", Vision Research, Vol. 19, pp. 613-618, 1979

"
1033,1995,Gradient and Hamiltonian Dynamics Applied to Learning in Neural Networks,,1033-gradient-and-hamiltonian-dynamics-applied-to-learning-in-neural-networks.pdf,Abstract Missing,"Gradient and Hamiltonian Dynamics
Applied to Learning in Neural Networks
James W. Howse

Chaouki T. Abdallah

Gregory L. Heileman

Department of Electrical and Computer Engineering
University of New Mexico
Albuquerque, NM 87131

Abstract
The process of machine learning can be considered in two stages: model
selection and parameter estimation. In this paper a technique is presented
for constructing dynamical systems with desired qualitative properties. The
approach is based on the fact that an n-dimensional nonlinear dynamical
system can be decomposed into one gradient and (n - 1) Hamiltonian systems. Thus, the model selection stage consists of choosing the gradient and
Hamiltonian portions appropriately so that a certain behavior is obtainable.
To estimate the parameters, a stably convergent learning rule is presented.
This algorithm has been proven to converge to the desired system trajectory
for all initial conditions and system inputs. This technique can be used to
design neural network models which are guaranteed to solve the trajectory
learning problem.

1

Introduction

A fundamental problem in mathematical systems theory is the identification of dynamical systems. System identification is a dynamic analogue of the functional approximation problem. A set of input-output pairs {u(t), y(t)} is given over some time
interval t E [7i, 1j]. The problem is to find a model which for the given input sequence
returns an approximation of the given output sequence. Broadly speaking, solving an
identification problem involves two steps. The first is choosing a class of identification models which are capable of emulating the behavior of the actual system. The
second is selecting a method to determine which member of this class of models best
emulates the actual system. In this paper we present a class of nonlinear models and
a learning algorithm for these models which are guaranteed to learn the trajectories
of an example system. Algorithms to learn given trajectories of a continuous time
system have been proposed in [6], [8], and [7] to name only a few. To our knowledge,
no one has ever proven that the error between the learned and desired trajectories
vanishes for any of these algorithms. In our trajectory learning system this error is
guaranteed to vanish. Our models extend the work in [1] by showing that Cohen's
systems are one instance of the class of models generated by decomposing the dynamics into a component normal to some surface and a set of components tangent to the
same surface. Conceptually this formalism can be used to design dynamical systems
with a variety of desired qualitative properties. Furthermore, we propose a provably
convergent learning algorithm which allows the parameters of Cohen's models to be
learned from examples rather than being programmed in advance. The algorithm is

275

Gradient and Hamiltonian Dynamics Applied to Learning in Neural Networks

convergent in the sense that the error between the model trajectories and the desired trajectories is guaranteed to vanish. This learning procedure is related to one
discussed in [5] for use in linear system identification.

2

Constructing the Model

First some terminology will be defined. For a system of n first order ordinary differential equations, the phase space of the system is the n-dimensional space of all state
components. A solution trajectory is a curve in phase space described by the differential equations for one specific starting point. At every point on a trajectory there
exists a tangent vector. The space of all such tangent vectors for all possible solution
trajectories constitutes the vector field for this system of differential equations.
The trajectory learning models in this paper are systems of first order ordinary differential equations. The form of these equations will be obtained by considering the
system dynamics as motion relative to some surface. At each point in the state space
an arbitrary system trajectory will be decomposed into a component normal to this
surface and a set of components tangent to this surface. This approach was suggested
to us by the results in [4], where it is shown that an arbitrary n-dimensional vector
field can be decomposed locally into the sum of one gradient vector field and (n - 1)
Hamiltonian vector fields. The concept of a potential function will be used to define these surfaces. A potential function V(:z:) is any scalar valued function of the
system states :z: = [Xl, X2, ??? , Xn.] t which is at least twice continuously differentiable
(Le. V(:z:) E or : r ~ 2). The operation [.]t denotes the transpose of the vector. If
there are n components in the system state, the function V{:z:), when plotted with
respect all of the state components, defines a surface in an (n + 1)-dimensional space.
There are two curves passing through every point on this potential surface which are
of interest in this discussion, they are illustrated in Figure 1(a). The dashed curve is
(z - zo)t \7 ... v (z)l ...o = 0

(a)

(b)

V(z) = K-

Figure 1: (a) The potential function V(z) = X~ (Xl _1)2 +x~ plotted versus its two dependent variables Xl and X2. The dashed curve is called a level surface and is given
by V(z) = 0.5. The solid curve follows the path of steepest descent through Zo.
(b) The partitioning of a 3-dimensional vector field at the point Zo into a 1dimensional portion which is normal to the surface V(z) = K- and a 2-dimensional
portion which is tangent to V(z) = K-. The vector -\7 ... V(z) 1""'0 is the normal vector to the surface V(z) = K- at the point Zo. The plane (z - zo)t \7 ... V (z) 1""'0 = 0
contains all of the vectors which are tangent to V(z) = K- at Zo. Two linearly
independent vectors are needed to form a basis for this tangent space, the pair
Q2(z) \7 ... V (z)l ... o and Q3(Z) \7 ... V (z)l ... o that are shown are just one possibility.
referred to as a level surface, it is a surface along which V(:z:) = K for some constant
K. Note that in general this level surface is an n-dimensional object. The solid curve

276

J. W. HOWSE, C. T. ABDALLAH, G. L. HEILEMAN

moves downhill along V (X) following the path of steepest descent through the point
Xo. The vector which is tangent to this curve at Xo is normal to the level surface
at Xo. The system dynamics will be designed as motion relative to the level surfaces
of V(x). The results in [4] require n different local potential functions to achieve
arbitrary dynamics. However, the results in [1] suggest that a considerable number
of dynamical systems can be achieved using only a single global potential function.
A system which is capable of traversing any downhill path along a given potential
surface V(x), can be constructed by decomposing each element of the vector field
into a vector normal to the level surface of V(x) which passes through each point
and a set of vectors tangent to the level surface of V(x) which passes through the
same point. So the potential function V(x) is used to partition the n-dimensional
phase space into two subspaces. The first contains a vector field normal to some
level surface V(x) = }( for }( E IR, while the second subspace holds a vector field
tangent to V(x) = IC. The subspace containing all possible normal vectors to the
n-dimensional level surface at a given point, has dimension one. This is equivalent
to the statement that every point on a smooth surface has a unique normal vector.
Similarly, the subspace containing all possible tangent vectors to the level surface at
a given point has dimension (n - 1). An example of this partition in the case of a
3-dimensional system is shown in Figure 1(b). Since the space of all tangent vectors
at each point on a level surface is (n - I)-dimensional, (n - 1) linearly independent
vectors are required to form a basis for this space.
Mathematically, there is a straightforward way to construct dynamical systems which
either move downhill along V(x) or remain at a constant height on V(x). In this
paper, dynamical systems which always move downhill along some potential surface
are called gradient-like systems. These systems are defined by differential equations
of the form
x = -P(x) VII:V(x),
(1)
where P(x) is a matrix function which is symmetric (Le. pt = P) and positive
:z~]f. These systems
definite at every point x, and where VIII V(x) =
are similar to the gradient flows discussed in [2]. The trajectories of the system
formed by Equation (1) always move downhill along the potential surface defined by
V(x). This can be shown by taking the time derivative of V(x) which is V(x) =
-[VII: V (x)]t P(x) [VII: V(x)] :5 O. Because P(x) is positive definite, V(x) can only be
zero where V II: V (x) = 0, elsewhere V(x) is negative. This means that the trajectories
of Equation (1) always move toward a level surface of V(x) formed by ""slicing"" V(x)
at a lower height, as pointed out in [2]. It is also easy to design systems which remain
at a constant height on V(x). Such systems will be denoted Hamiltonian-like systems.
They are specified by the equation
x = Q(x) VII: V(x),
(2)
where Q(x) is a matrix function which is skew-symmetric (Le. Qt = -Q) at every
point x. These systems are similar to the Hamiltonian systems defined in [2]. The
elements of the vector field defined by Equation (2) are always tangent to some level
surface of V (x). Hence the trajectories ofthis system remain at a constant height on
the potential surface given by V(x). Again this is indicated by the time derivative
of V(x), which in this case is V(x) = [VII: V(x)]f Q(x)[VII: V(x)] = o. This indicates
that the trajectories of Equation (2) always remain on the level surface on which the
system starts. So a model which can follow an arbitrary downhill path along the
potential surface V(x) can be designed by combining the dynamics of Equations (1)
and (2) . The dynamics in the subspace normal to the level surfaces of V(x) can be

[g;: , g;: ,... ,

Gradient and Hamiltonian Dynamics Applied to Learning in Neural Networks

277

defined using one equation of the form in Equation (1). Similarly the dynamics in the
subspace tangent to the level surfaces of Vex) can be defined using (n - 1) equations
of the form in Equation (2). Hence the total dynamics for the model are
n

z= -P(x)VIDV(x) + LQi(X)VIDV(x).

(3)

i=2

For this model the number and location of equilibria is determined by the function
Vex), while the manner in which the equilibria are approached is determined by the
matrices P(x) and Qi(x).
If the potential function Vex) is bounded below (i.e. Vex) > Bl V x E IRn , where
Bl is a constant), eventually increasing (i.e. limlllDlI-+oo Vex) ~ 00) , and has only
a finite number of isolated local maxima and minima (i.e. in some neighborhood
of every point where V III V (x) = 0 there are no other points where the gradient
vanishes), then the system in Equation (3) satisfies the conditions of Theorem 10
in [1]. Therefore the system will converge to one of the points where V ID Vex) = 0,
called the critical points of Vex), for all initial conditions. Note that this system
is capable of all downhill trajectories along the potential surface only if the (n - 1)
vectors Qi(X) V ID Vex) V i = 2, ... , n are linearly independent at every point x. It
is shown in [1] that the potential function

V(z) = C (

1:., (-y) d-y +

t, [~

(XI - I:.,(xd)'

+~

J:'

1:., h )II:.: (-y)]' d-y

1

(4)

satisfies these three criteria. In this equation ?.i(Xt} Vi = 1, ... , n are interpolation
polynomials, C is a real positive constant, Xi Vi = 1, ... , n are real constants chosen
so that the integrals are positive valued, and ?.Hxt} ==

f:-.

3

The Learning Rule

In Equation (3) the number and location of equilibria can be controlled using the
potential function Vex), while the manner in which the equilibria are approached can
be controlled with the matrices P(x) and Qi(X). If it is assumed that the locations
of the equilibria are known, then a potential function which has local minima and
maxima at these points can be constructed using Equation (4). The problem of
trajectory learning is thereby reduced to the problem of parameterizing the matrices
P(x) and Qi(x) and finding the parameter values which cause this model to best
emulate the actual system. If the elements P(x) and Qi(x) are correctly chosen,
then a learning rule can be designed which makes the model dynamics converge to
that of the actual system. Assume that the dynamics given by Equation (3) are a
parameterized model of the actual dynamics. Using this model and samples of the
actual system states, an estimator for states of the actual system can be designed. The
behavior of the model is altered by changing its parameters, so a parameter estimator
must also be constructed. The following theorem provides a form for both the state
and parameter estimators which guarantees convergence to a set of parameters for
which the error between the estimated and target trajectories vanishes.
Theorem 3.1. Given the model system
k

Z = LAili(x) +Bg(u)

(5)

i=l

where Ai E IRnxn and BE IRnxm are unknown, and li(') and g(.) are known smooth
functions such that the system has bounded solutions for bounded inputs u(t). Choose

J. W. HOWSE, C. T. ABDALLAH, G. L. HEILEMAN

278

a state estimator of the form
k

~ = 'R. B (x - x) +

L Ai fi(x) + iJ g(u)

(6)

i=1

where'R. B is an (n x n) matrix of real constants whose eigenvalues must all be in the
left half plane, and Ai and iJ are the estimates of the actual parameters. Choose
parameter estimators of the form
~
t
Ai = -'R.p (x - x) [fi(x)] V i = 1, ... , k
(7)
= -'R.p (x - x) [g(u)]t

B

where 'R. p is an (n x n) matrix of real constants which is symmetric and positive
definite, and (x - x) [.]t denotes an outer product. For these choices of state and
parameter estimators limt~oo(x(t) -x(t? = 0 for all initial conditions. Furthermore,
this remains true if any of the elements of Ai or iJ are set to 0, or if any of these
matrices are restricted to being symmetric or skew-symmetric.
The proof of this theorem appears in [3]. Note that convergence of the parameter
estimates to the actual parameter values is not guaranteed by this theorem. The
model dynamics in Equation (3) can be cast in the form of Equation (5) by choosing
each element of P(x) and Qi(X) to have the form
I-I

n

n

I-I

= LL~rBjkt?k(Xj)

and
QrB = LLArBjk ek(Xj),
(8)
j=1 k=O
j=1 k=O
where {t?o(Xj), t?1 (Xj), ... ,t?I-1 (Xj)} and {eo(Xj), el (Xj), ... ,el-l (Xj)} are a set of 1
orthogonal polynomials which depend on the state Xj' There is a set of such polynomials for every state Xj, j = 1,2, ... , n. The constants ~rBjk and ArBjk determine
the contribution of the kth polynomial which depends on the jth state to the value
of Prs and Qrs respectively. In this case the dynamics in Equation (3) become
PrB

:i:

=

t. ~ {

S;. [11.(x;) V. V (z)j

+

t,

A;;. [e;.(x;)

v. V(z)j } + T g(u(t))

(9)

where 8 jk is the (n x n) matrix of all values ~rsjk which have the same value of j and
k. Likewise A ijk is the (n x n) matrix of all values Arsjk, having the same value of
j and k, which are associated with the ith matrix Qi(X). This system has m inputs,
which may explicitly depend on time, that are represented by the m-element vector
function u(t). The m-element vector function g(.) is a smooth, possibly nonlinear,
transformation of the input function. The matrix Y is an (n x m) parameter matrix
which determines how much of input S E {I, ... , m} effects state r E {I, ... , n}.
Appropriate state and parameter estimators can be designed based on Equations (6)
and (7) respectively.

4

Simulation Results

Now an example is presented in which the parameters of the model in Equation (9)
are trained, using the learning rule in Equations (6) and (7), on one input signal and
then are tested on a different input signal. The actual system has three equilibrium
points, two stable points located at (1,3) and (3,5), and a saddle point located at
(2 - ~,4 + ~). In this example the dynamics of both the actual system and the
model are given by

(~1) =
Z2

Z~

Z~

O

(1'1 + 1'2
+:3
2)
0 1'4 + 1'5 Z1 + 1'6 Z2

(:~)
+ (0 - {1'7 + 1'8 Z1 + 1'9 Z2}) (:~ ) + (1'10) u(t)
8Y
'P7 + 'P8 ZI + 1'9 Z2
8Y
0

8Z2

0

8Z2

(10)

Gradient and Hamiltonian Dynamics Applied to Learning in Neural Networks

279

where V(x) is defined in Equation (4) and u(t) is a time varying input. For the actual
system the parameter values were 'PI = 'P4 = -4, 'P2 = 'Ps = -2, 'P3 = 'P6 = -1,
'P7 = 1, 'Ps = 3, 'P9 = 5, and 'PIO = 1. In the model the 10 elements 'Pi are
treated as the unknown parameters which must be learned. Note that the first matrix
function is positive definite if the parameters 'PI-'P6 are all negative valued. The
second matrix function is skew-symmetric for all values of 'P7-'P9. The two input
signals used for training and testing were Ul = 10000 (sin! 1000t + sin ~ 1000t) and
U2 = 5000 sin 1000 t. The phase space responses of the actual system to the inputs UI
and U2 are shown by the solid curves in Figures 3(b) and 3(a) respectively. Notice that
both of these inputs produce a periodic attractor in the phase space of Equation (10).
In order to evaluate the effectiveness of the learning algorithm the Euclidean distance
between the actual and learned state and parameter values was computed and plotted
versus time. The results are shown in Figure 2. Figure 2(a) shows these statistics when
{1I~zll, II~'PII}

{1I~zll, II~'PII}

17.5
15
15
12.5
12.5
10

7.5

i

----

,., ~--.----... ... .......

- --

2.5

150
200
250
300 t
50
100
150
200
250
300 t
(a)
(b)
Figure 2: (a) The state and parameter errors for training using input signal Ut. The solid
curve is the Euclidean distance between the state estimates and the actual states
as a function of time. The dashed curve shows the distance between the estimated
and actual parameter values versus time.
(b) The state and parameter errors for training using input signal U2.
50

100

training with input UI, while Figure 2(b) shows the same statistics for input U2. The
solid curves are the Euclidean distance between the learned and actual system states,
and the dashed curves are the distance between the learned and actual parameter
values. These statistics have two noteworthy features. First, the error between the
learned and desired states quickly converges to very small values, regardless of how
well the actual parameters are learned. This result was guaranteed by Theorem 3.1.
Second, the final error between the learned and desired parameters is much lower when
the system is trained with input UI. Intuitively this is because input Ul excites more
frequency modes of the system than input U2. Recall that in a nonlinear system the
frequency modes excited by a given input do not depend solely on the input because
the system can generate frequencies not present in the input. The quality of the
learned parameters can be qualitatively judged by comparing the phase plots using
the learned and actual parameters for each input, as shown in Figure 3. In Figure 3(a)
the system was trained using input Ul and tested with input U2, while in Figure 3(b)
the situation was reversed. The solid curves are the system response using the actual
parameter values, and the dashed curves are the response for the learned parameters.
The Euclidean distance between the target and test trajectories in Figure 3(a) is in
the range (0,0.64) with a mean distance of 0.21 and a standard deviation of 0.14. The
distance between the the target and test trajectories in Figure 3(b) is in the range
(0,4.53) with a mean distance of 0.98 and a standard deviation of 1.35. Qualitatively,
both sets of learned parameters give an accurate response for non-training inputs.

280

1. W. HOWSE, C. T. ABDALLAH, G. L. HEILEMAN

5
I

o

{i

-------r-- -- ----- --- -- I

-5

-10

-15

-l

-1

1

-2

-1

4

Xl

(a)
(b)
Figure 3: (a) A phase plot of the system response when trained with input UI and tested
with input U2. The solid line is the response to the test input using the actual
parameters. The dotted line is the system response using the learned parameters.
(b) A phase plot of the system response when trained with input U2 and tested
with input UI.

Note that even when the error between the learned and actual parameters is large,
the periodic attractor resulting from the learned parameters appears to have the same
""shape"" as that for the actual parameters.

5

Conclusion

We have presented a conceptual framework for designing dynamical systems with
specific qualitative properties by decomposing the dynamics into a component normal
to some surface and a set of components tangent to the same surface. We have
presented a specific instance of this class of systems which converges to one of a finite
number of equilibrium points. By parameterizing these systems, the manner in which
these equilibrium points are approached can be fitted to an arbitrary data set. We
present a learning algorithm to estimate these parameters which is guaranteed to
converge to a set of parameter values for which the error between the learned and
desired trajectories vanishes.

Acknowledgments
This research was supported by a grant from Boeing Computer Services under Contract
W-300445. The authors would like to thank Vangelis Coutsias, Tom Caudell, and Bill
Home for stimulating discussions and insightful suggestions.

References
[1] M.A. Cohen. The construction of arbitrary stable dynamics in nonlinear neural networks.
Neural Networks, 5(1):83-103, 1992.
[2] M.W. Hirsch and S. Smale. Differential equations, dynamical systems, and linear algebra,
volume 60 of Pure and Applied Mathematics. Academic Press, Inc., San Diego, CA, 1974.
[3] J.W. Howse, C.T. Abdallah, and G.L. Heileman. A gradient-hamiltonian decomposition
for designing and learning dynamical systems. Submitted to Neural Computation, 1995.
[4] R.V. Mendes and J .T. Duarte. Decomposition of vector fields and mixed dynamics.
Journal of Mathematical Physics, 22(7):1420-1422, 1981.
[5] K.S. Narendra and A.M. Annaswamy. Stable adaptitJe systems. Prentice-Hall, Inc., Englewood Cliffs, NJ, 1989.
[6] B.A. Pearlmutter. Learning state space trajectories in recurrent neural networks. Neural
Computation, 1(2):263-269, 1989.
[7] D. Saad. Training recurrent neural networks via trajectory modification. Complex Systems, 6(2) :213-236, 1992.
[8] M.-A. Sato. A real time learning algorithm for recurrent analog neural networks. Biological Cybernetics, 62(2):237-241, 1990.

"
1034,1995,Is Learning The n-th Thing Any Easier Than Learning The First?,,1034-is-learning-the-n-th-thing-any-easier-than-learning-the-first.pdf,Abstract Missing,"Is Learning The n-th Thing Any Easier Than
Learning The First?

Sebastian Thrun I
Computer Science Department
Carnegie Mellon University
Pittsburgh, PA 15213-3891
World Wide Web: http://www.cs.cmu.edul'''thrun

Abstract
This paper investigates learning in a lifelong context. Lifelong learning
addresses situations in which a learner faces a whole stream of learning tasks. Such scenarios provide the opportunity to transfer knowledge
across multiple learning tasks, in order to generalize more accurately from
less training data. In this paper, several different approaches to lifelong
learning are described, and applied in an object recognition domain. It
is shown that across the board, lifelong learning approaches generalize
consistently more accurately from less training data, by their ability to
transfer knowledge across learning tasks.

1 Introduction
Supervised learning is concerned with approximating an unknown function based on examples. Virtually all current approaches to supervised learning assume that one is given a set
of input-output examples, denoted by X, which characterize an unknown function, denoted
by f. The target function f is drawn from a class of functions, F, and the learner is given a
space of hypotheses, denoted by H, and an order (preference/prior) with which it considers
them during learning. For example, H might be the space of functions represented by an
artificial neural network with different weight vectors.
While this formulation establishes a rigid framework for research in machine learning, it
dismisses important aspects that are essential for human learning. Psychological studies
have shown that humans often employ more than just the training data for generalization.
They are often able to generalize correctly even from a single training example [2, 10]. One
of the key aspects of the learning problem faced by humans, which differs from the vast
majority of problems studied in the field of neural network learning, is the fact that humans
encounter a whole stream of learning problems over their entire lifetime. When faced with
a new thing to learn, humans can usually exploit an enormous amount of training data and
I also

affiliated with: Institut fur Informatik III, Universitat Bonn, Romerstr. 164, Germany

Is Learning the n-th Thing Any Easier Than Learning the First?

641

experiences that stem from other, related learning tasks. For example, when learning to drive
a car, years of learning experience with basic motor skills, typical traffic patterns, logical
reasoning, language and much more precede and influence this learning task. The transfer of
knowledge across learning tasks seems to play an essential role for generalizing accurately,
particularly when training data is scarce.
A framework for the study of the transfer of knowledge is the lifelong learning framework.
In this framework, it is assumed that a learner faces a whole collection of learning problems
over its entire lifetime. Such a scenario opens the opportunity for synergy. When facing its
n-th learning task, a learner can re-use knowledge gathered in its previous n - 1 learning
tasks to boost the generalization accuracy.
In this paper we will be interested in the most simple version of the lifelong learning problem,
in which the learner faces a family of concept learning tasks. More specifically, the functions
to be learned over the lifetime of the learner, denoted by 11 , 12 , 13, .. . E F , are all of the type
I : I --+ {O, I} and sampled from F. Each function I E {II , h ,13, . . .} is an indicator
function that defines a particular concept: a pattern x E I is member of this concept if
and only if I(x) = 1. When learning the n-th indicator function, In , the training set X
contains examples of the type (x , In(x)) (which may be distorted by noise). In addition to
the training set, the learner is also given n - 1 sets of examples of other concept functions,
denoted by Xk (k
1, .. . , n - I). Each Xk contains training examples that characterize
Ik. Since this additional data is desired to support learning In, Xk is called a support set
for the training set X .

=

An example of the above is the recognition of faces [5, 7]. When learning to recognize the
n-th person, say IBob, the learner is given a set of positive and negative example of face
images of this person. In lifelong learning, it may also exploit training information stemming
from other persons, such as I E {/Rieh, IMike , IDave , ... }. The support sets usually cannot be
used directly as training patterns when learning a new concept, since they describe different
concepts (hence have different class labels). However, certain features (like the shape of the
eyes) are more important than others (like the facial expression, or the location of the face
within the image). Once the invariances of the domain are learned, they can be transferred
to new learning tasks (new people) and hence improve generalization.
To illustrate the potential importance of related learning tasks in lifelong learning, this
paper does not present just one particular approach to the transfer of knowledge. Instead,
it describes several, all of which extend conventional memory-based or neural network
algorithms. These approaches are compared with more traditional learning algorithms, i.e.,
those that do not transfer knowledge. The goal of this research is to demonstrate that,
independent of a particular learning approach, more complex functions can be learned from
less training data iflearning is embedded into a lifelong context.

2 Memory-Based Learning Approaches
Memory-based algorithms memorize all training examples explicitly and interpolate them
at query-time. We will first sketch two simple, well-known approaches to memory-based
learning, then propose extensions that take the support sets into account.

2.1

Nearest Neighbor and Shepard's Method

Probably the most widely used memory-based learning algorithm is

J{ -nearest

neighbor

(KNN) [15]. Suppose x is a query pattern, for which we would like to know the output y .

KNN searches the set of training examples X for those J{ examples (Xi, Yi ) E X whose
input patterns Xi are nearest to X (according to some distance metric, e.g., the Euclidian
distance). It then returns the mean output value 2:= Yi of these nearest neighbors.

k

Another commonly used method, which is due to Shepard [13], averages the output values

s. THRUN

642

of all training examples but weights each example according to the inverse distance to the
query

:~~~t x.

(

L

(x""y.)EX

)

Ilx -

~: II + E ?

(

L

(x. ,y.)EX

Ilx -

I)

Xi

-I

(1)

II + E

Here E > 0 is a small constant that prevents division by zero. Plain memory-based learning
uses exclusively the training set X for learning. There is no obvious way to incorporate the
support sets, since they carry the wrong class labels.
2.2

Learning A New Representation

The first modification of memory-based learning proposed in this paper employs the support
sets to learn a new representation of the data. More specifically, the support sets are employed
to learn a function, denoted by 9 : I --+ I', which maps input patterns in I to a new space,
I' . This new space I' forms the input space for a memory-based algorithm.
Obviously, the key property of a good data representations is that multiple examples of a
single concept should have a similar representation, whereas the representation of an example
and a counterexample of a concept should be more different. This property can directly be
transformed into an energy function for g:

~ (X,y~EXk (X""y~EXk Ilg(x)-g(x')11

n-I

E:=

(

(X""y~EXk Ilg( x )-g(x')11

)

(2)

Adjusting 9 to minimize E forces the distance between pairs of examples of the same
concept to be small, and the distance between an example and a counterexample of a concept
to be large. In our implementation, 9 is realized by a neural network and trained using the
Back-Propagation algorithm [12].
Notice that the new representation, g, is obtained through the support sets. Assuming that
the learned representation is appropriate for new learning tasks, standard memory-based
learning can be applied using this new representation when learning the n-th concept.
2.3

Learning A Distance Function

An alternative way for exploiting support sets to improve memory-based learning is to learn
a distance function [3, 9]. This approach learns a function d : I x I --+ [0, I] which accepts
two input patterns, say x and x' , and outputs whether x and x' are members of the same
concept, regardless what the concept is. Training examples for d are

(( x , x'),I)
((x, x'), 0)

ify=y'=l

if(y=IAy'=O)or(y=OAy'=I).
They are derived from pairs of examples (x , y) , (x', y') E Xk taken from a single support
set X k (k = 1, . .. , n - I). In our implementation, d is an artificial neural network trained
with Back-Propagation. Notice that the training examples for d lack information concerning
the concept for which they were originally derived. Hence, all support sets can be used to
train d. After training, d can be interpreted as the probability that two patterns x, x' E I are
examples of the same concept.
Once trained, d can be used as a generalized distance function for a memory-based approach.
Suppose one is given a training set X and a query point x E I. Then, for each positive
example (x' , y' = I) EX , d( x , x') can be interpreted as the probability that x is a member
of the target concept. Votes from multiple positive examples (XI, I) , (X2' I), ... E X are
combined using Bayes' rule, yielding

Prob(fn(x)=I)

.-

1-

(I

+

II
(x' ,y'=I)EXk

I:(~(::~,))-I

(3)

Is Learning the n-th Thing Any Easier Than Learning the First?

643

Notice that d is not a distance metric. It generalizes the notion of a distance metric, because
the triangle inequality needs not hold, and because an example of the target concept x' can
provide evidence that x is not a member of that concept (if d(x, x') < 0.5).

3 Neural Network Approaches
To make our comparison more complete, we will now briefly describe approaches that rely
exclusively on artificial neural networks for learning In.

3.1

Back-Propagation

Standard Back-Propagation can be used to learn the indicator function In, using X as training
set. This approach does not employ the support sets, hence is unable to transfer knowledge
across learning tasks.

3.2 Learning With Hints
Learning with hints [1, 4, 6, 16] constructs a neural network with n output units, one for
each function Ik (k = 1,2, .. . , n). This network is then trained to simultaneously minimize
the error on both the support sets {Xk} and the training set X. By doing so, the internal
representation of this network is not only determined by X but also shaped through the
support sets {X k }. If similar internal representations are required for al1 functions Ik
(k
1,2, .. . , n), the support sets provide additional training examples for the internal
representation.

=

3.3 Explanation-Based Neural Network Learning
The last method described here uses the explanation-based neural network learning algorithm (EBNN), which was original1y proposed in the context of reinforcement learning
[8, 17]. EBNN trains an artificial neural network, denoted by h : I ----+ [0, 1], just like
Back-Propagation. However, in addition to the target values given by the training set X,
EBNN estimates the slopes (tangents) of the target function In for each example in X. More
specifically, training examples in EBNN are of the sort (x, In (x), \7 xln(x)), which are fit
using the Tangent-Prop algorithm [14]. The input x and target value In(x) are taken from
the trai ning set X. The third term, the slope \7 xln ( X ), is estimated using the learned distance
function d described above. Suppose (x', y'
1) E X is a (positive) training example.
Then, the function d x ' : I ----+ [0, 1] with d x ' (z) := d(z , x') maps a single input pattern to
[0, 1], and is an approximation to In. Since d( z, x') is represented by a neural network and
neural networks are differentiable, the gradient 8dx ' (z) /8z is an estimate of the slope of In
at z. Setting z := x yields the desired estimate of \7 xln (x) . As stated above, both the target
value In (x) and the slope vector \7 xIn (x) are fit using the Tangent-Prop algorithm for each
training example x EX .

=

The slope \7 xln provides additional information about the target function In. Since d is
learned using the support sets, EBNN approach transfers knowledge from the support sets
to the new learning task. EBNN relies on the assumption that d is accurate enough to yield
helpful sensitivity information. However, since EBNN fits both training patterns (values)
and slopes, misleading slopes can be overridden by training examples. See [17] for a more
detailed description of EBNN and further references.

4 Experimental Results
All approaches were tested using a database of color camera images of different objects
(see Fig. 3.3). Each of the object in the database has a distinct color or size. The n-th

644
l1

S. THRUN

....

I't'

'I

'I

?

:.

<

,

~~~

>
-~"""""":::.~

--....

'""

.
~

~

-,:~~,}

I

1:1 ,I

......

'

~..

.

,
?

~

?.~ <.~
~

<.-

~~-

...

:t
-_,1-

""
~

~,-l/> ;' ;'j III
'1 ~' ''',ll

t!

~[~

,,-

,

~_

~

~ ~_l_~
II

-

-...
__

E~

_e?m;,

'~~
;1 ~

t

~,~,AA(

.d!t~)ltI!{iH-""""

,

,

,
~

c-

_.

ML~._. . ,

:R;1-;

I

'''!!!i!~,

='

;~~~

""

,

""""111':'i, It r
f4~

,

Figure 1: The support sets were compiled out of a hundred
images of a bottle, a
hat, a hammer, a coke
can, and a book. The
n-th learning tasks
involves distinguishing the shoe from the
sunglasses. Images
were subsampled to
a 100x 100 pixel matrix (each pixel has a
color, saturation, and
a brightness value),
shown on the right
side.

learning task was the recognition of one of these objects, namely the shoe. The previous
n - 1 learning tasks correspond to the recognition of five other objects, namely the bottle,
the hat, the hammer, the coke can, and the book. To ensure that the latter images could
not be used simply as additional training data for In, the only counterexamples of the shoe
was the seventh object, the sunglasses. Hence, the training set for In contained images of
the shoe and the sunglasses, and the support sets contained images of the other five objects.
The object recognition domain is a good testbed for the transfer of knowledge in lifelong
learning. This is because finding a good approximation to In involves recognizing the target
object invariant of rotation, translation, scaling in size, change of lighting and so on. Since
these invariances are common to all object recognition tasks, images showing other objects
can provide additional information and boost the generalization accuracy.
Transfer of knowledge is most important when training data is scarce. Hence, in an initial
experiment we tested all methods using a single image of the shoe and the sunglasses only.
Those methods that are able to transfer knowledge were also provided 100 images of each
of the other five objects. The results are intriguing. The generalization accuracies

KNN

Shepard

60.4%
?8.3%

60.4%
?8.3%

repro g+Shep.
74.4%
?18.5%

distanced
75.2%
?18.9%

Back-Prop

hints

EBNN

59.7%
?9.0%

62.1%
?10.2%

74.8%
?11.1%

illustrate that all approaches that transfer knowledge (printed in bold font) generalize significantly better than those that do not. With the exception of the hint learning technique,
the approaches can be grouped into two categories: Those which generalize approximately
60% of the testing set correctly, and those which achieve approximately 75% generalization accuracy. The former group contains the standard supervised learning algorithms, and
the latter contains the ""new"" algorithms proposed here, which are capable of transferring
knOWledge. The differences within each group are statistically not significant, while the
differences between them are (at the 95% level). Notice that random guessing classifies 50%
of the testing examples correctly.
These results suggest that the generalization accuracy merely depends on the particular
choice of the learning algorithm (memory-based vs. neural networks). Instead, the main
factor determining the generalization accuracy is the fact whether or not knowledge is
transferred from past learning tasks.

Is Learning the n-th Thing Any Easier Than Learning the First?
95%

645

95%

distance function d
85%
~

~
-

80%

hepard 's method with representation g

15%
70%

, ,,'~.

.</'~
70%

Shepard's method

65%

65%

60%

60% ;;./

55%

55%

50%~2--~~----~10~~1~2~1~4--1~6--1~.--~20
training example.

/f

if

.

Back-Propagauon

~%~2--~~----~1~O~1~2--1~4--1~6--~1B--~20
training exampletl

Figure 2: Generalization accuracy as a function of training examples, measured on an
independent test set and averaged over 100 experiments. 95%-confidence bars are also
displayed.
What happens as more training data arrives? Fig. 2 shows generalization curves with
increasing numbers of training examples for some of these methods. As the number of
training examples increases, prior knowledge becomes less important. After presenting 20
training examples, the results

KNN
81.0%
?3.4%

Shepard
70.5%
?4.9%

repro g+Shep.
81.7%
?2.7%

distance d
87.3%
?O_9%

Back-Prop
88.4%
?2.5%

hints
n_avail.

EBNN

90.8%
?2.7%

illustrate that some of the standard methods (especially Back-Propagation) generalize about
as accurately as those methods that exploit support sets. Here the differences in the underlying
learning mechanisms becomes more dominant. However, when comparing lifelong learning
methods with their corresponding standard approaches, the latter ones are stiIl inferior: BackPropagation (88.4%) is outperformed by EBNN (90.8%), and Shepard's method (70.5%)
generalizes less accurately when the representation is learned (81.7%) or when the distance
function is learned (87.3%). All these differences are significant at the 95% confidence level.

5

Discussion

The experimental results reported in this paper provide evidence that learning becomes easier
when embedded in a lifelong learning context. By transferring knowledge across related
learning tasks, a learner can become ""more experienced"" and generalize better. To test
this conjecture in a more systematic way, a variety of learning approaches were evaluated
and compared with methods that are unable to transfer knowledge. It is consistently found
that lifelong learning algorithms generalize significantly more accurately, particularly when
training data is scarce.
Notice that these results are well in tune with other results obtained by the author. One of
the approaches here, EBNN, has extensively been studied in the context of robot perception
[11], reinforcement learning for robot control, and chess [17]. In all these domains, it has
consistently been found to generalize better from less training data by transferring knowledge
from previous learning tasks. The results are also consistent with observations made about
human learning [2, 10], namely that previously learned knowledge plays an important role
in generalization, particularly when training data is scarce. [18] extends these techniques to
situations where most support sets are not related.w
However, lifelong learning rests on the assumption that more than a single task is to be
learned, and that learning tasks are appropriately related. Lifelong learning algorithms
are particularly well-suited in domains where the costs of collecting training data is the
dominating factor in learning, since these costs can be amortized over several learning tasks.
Such domains include, for example, autonomous service robots which are to learn and
improve over their entire lifetime. They include personal software assistants which have

646

S. THRUN

to perform various tasks for various users. Pattern recognition, speech recognition, time
series prediction, and database mining might be other, potential application domains for the
techniques presented here.

References
[1] Y. S. Abu-Mostafa. Learning from hints in neural networks. Journal of Complexity, 6: 192-198,
1990.
[2] W-K. Ahn and W F. Brewer. Psychological studies of explanation-based learning. In
G. Dejong, editor, Investigating Explanation-Based Learning . Kluwer Academic Publishers,
BostonlDordrechtILondon, 1993.
[3] c. A. Atkeson. Using locally weighted regression for robot learning. In Proceedings of the 1991
1EEE International Conference on Robotics and Automation, pages 958-962, Sacramento, CA,
April 1991.
[4] J. Baxter. Learning internal representations. In Proceedings of the Conference on Computation
Learning Theory, 1995.
[5] D. Beymer and T. Poggio. Face recognition from one model view. In Proceedings of the
International Conference on Computer Vision, 1995.
[6] R. Caruana. MuItitask learning: A knowledge-based of source of inductive bias. In P. E. Utgoff,
editor, Proceedings of the Tenth International Conference on Machine Learning, pages 41-48,
San Mateo, CA, 1993. Morgan Kaufmann.
[7] M. Lando and S. Edelman. Generalizing from a single view in face recognition. Technical Report
CS-TR 95-02, Department of Applied Mathematics and Computer Science, The Weizmann
Institute of Science, Rehovot 76100, Israel, January 1995.
[8] T. M. Mitchell and S. Thrun. Explanation-based neural network learning for robot control. In
S. J. Hanson, J. Cowan, and C. L. Giles, editors, Advances in Neural Information Processing
Systems 5, pages 287-294, San Mateo, CA, 1993. Morgan Kaufmann.
[9] A. W Moore, D. 1. Hill, and M. P. Johnson. An Empirical Investigation of Brute Force to choose
Features, Smoothers and Function Approximators. In S. Hanson, S. Judd, and T. Petsche, editors,
Computational Learning Theory and Natural Learning Systems, Volume 3. MIT Press, 1992.
[10] Y. Moses, S. Ullman, and S. Edelman. Generalization across changes in illumination and viewing
position in upright and inverted faces. Technical Report CS-TR 93-14, Department of Applied
Mathematics and Computer Science, The Weizmann Institute of Science, Rehovot 76100, Israel,
1993.
[11] J. O'Sullivan, T. M. Mitchell, and S. Thrun. Explanation-based neural network learning from
mobile robot perception. In K. Ikeuchi and M. Veloso, editors, Symbolic Visual Learning. Oxford
University Press, 1995.
[12] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal representations by error
propagation. In D. E. Rumelhart and 1. L. McClelland, editors, Parallel Distributed Processing.
Vol. I + II. MIT Press, 1986.
[13] D. Shepard. A two-dimensional interpolation function for irregularly spaced data. In 23rd
National Conference ACM, pages 517-523, 1968.
[14] P. Simard, B. Victorri, Y. LeCun, and J. Denker. Tangent prop - a formalism for specifying
selected invariances in an adaptive network. In 1. E. Moody, S. J. Hanson, and R. P. Lippmann,
editors, Advances in Neural Information Processing Systems 4, pages 895-903, San Mateo, CA,
1992. Morgan Kaufmann.
[15] c. Stanfill and D. Waltz. Towards memory-based reasoning. Communications of the ACM,
29(12): 1213-1228, December 1986.
[16] S. C. Suddarth and A. Holden. Symbolic neural systems and the use of hints for developing
complex systems. International Journal of Machine Studies, 35, 1991.
[17] S. Thrun. Explanation-Based Neural Network Learning: A Lifelong Learning Approach. Kluwer
Academic Publishers, Boston, MA, 1996. to appear.
[18] S. Thrun and J. O'Sullivan. Clustering learning tasks and the selective cross-task transfer
of knowledge. Technical Report CMU-CS-95-209, Carnegie Mellon University, School of
Computer Science, Pittsburgh, PA 15213, November 1995.

"
1035,1995,A Dynamical Model of Context Dependencies for the Vestibulo-Ocular Reflex,,1035-a-dynamical-model-of-context-dependencies-for-the-vestibulo-ocular-reflex.pdf,Abstract Missing,"A Dynamical Model of Context Dependencies for the
Vestibulo-Ocular Reflex
Terrence J. Sejnowskit

Olivier J.M.D. Coenen*

Computational Neurobiology Laboratory
Howard Hughes Medical Institute
The Salk Institute for Biological Studies
10010 North Torrey Pines Road
La Jolla, CA 92037, U.S.A.
Departments oftBiology and *tPhysics
University of California, San Diego
La Jolla, CA 92093, U.S.A

{olivier,terry}@salk.edu

Abstract
The vestibulo-ocular reflex (VOR) stabilizes images on the retina during rapid
head motions. The gain of the VOR (the ratio of eye to head rotation velocity)
is typically around -1 when the eyes are focused on a distant target. However, to
stabilize images accurately, the VOR gain must vary with context (eye position,
eye vergence and head translation). We first describe a kinematic model of the
VOR which relies solely on sensory information available from the semicircular
canals (head rotation), the otoliths (head translation), and neural correlates of eye
position and vergence angle. We then propose a dynamical model and compare it
to the eye velocity responses measured in monkeys. The dynamical model reproduces the observed amplitude and time course of the modulation of the VOR and
suggests one way to combine the required neural signals within the cerebellum and
the brain stem. It also makes predictions for the responses of neurons to multiple
inputs (head rotation and translation, eye position, etc.) in the oculomotor system.

1 Introduction
The VOR stabilizes images on the retina during rapid head motions: Rotations and translations of
the head in three dimensions must be compensated by appropriate rotations of the eye. Because the
head's rotation axis is not the same as the eye's rotation axis, the calculations for proper image stabilization of an object must take into account diverse variables such as object distance from each eye,

O. J. M. D. COENEN, T. J. SEJNOWSKI

90

gaze direction, and head translation (Viire et al., 1986). The stabilization is achieved by integrating
infonnation from different sources: head rotations from the semicircular canals of the inner ear, head
translations from the otolith organs, eye positions, viewing distance, as well as other context infonnation, such as posture (head tilts) or activity (walking, running) (Snyder and King, 1992; Shelhamer
et al.,1992; Grossman et al., 1989). In this paper we concentrate on the context modulation of the
VOR which can be described by the kinematics of the reflex, i.e. eye position, eye vergence and
head translation.

2

The Vestibulo-Ocular Reflex: Kinematic Model
Definition of Vectors

Target Object

Coordinate System

Gaze Vector

Gaze Angle
Interocular
Distance
Eye position
Vector

Rotation Axis

Semicircular
Canals and
Otoliths
Head
Top View

?

~_--+_

Origin of coordinate
syste,,-, (arbitrary)

Figure 1: Diagram showing the definition of the vectors used in the equation of the kinematic model of the
vestibulo-ocular reflex.

The ideal VOR response is a compensatory eye movement which keeps the image fixed on the retina
for any head rotations and translations. We therefore derived an equation for the eye rotation velocity
by requiring that a target remains stationary on the retina. The velocity of the resulting compensatory
eye rotation can be written as (see fig. 1):

w= -Oe + 1:1

x [Dej x

Oe - To;]

(1)

where Oe is the head rotation velocity sensed by the semicircular canals, TOj is the head translation
velocity sensed by the otoliths, Dej == (e - OJ), eis a constant vector specifying the location of an
eye in the head, OJ is the position of either the left or right otolith, fJ and Igl are the unit vector and
amplitude of the gaze vector: fJ gives the eye position (orientation of the eye relative to the head),
and Igl gives the distance from the eye to the object, and the symbol x indicates the cross-product
between two vectors. wand Oe are rotation vectors which describe the instantaneous angUlar velocity
of the eye and head, respectively. A rotation vector lies along the instantaneous axis of rotation;
its magnitude indicates the speed of rotation around the axis, and its direction is given by the righthand screw rule. A motion of the head combining rotation (0) and translation (T) is sensed as the
combination of a rotation velocity Oe measured by the semicircular canals and a translation velocity
To sensed by the otoliths. The rotation vectors are equal (0 = Oe), and the translation velocity vector
as measured by the otoliths is given by: TOj = OOj x 0 + T, where OOj == (a - OJ), and a is the
position vector of the axis of rotation.

91

A Dynarnical Model of Context Dependencies for the Vestibula-Ocular Reflex

The special case where the gaze is horizontal and the rotation vector is vertical (horizontal head rotation) has been studied extensively in the literature. We used this special case in the sirnulations.
In that case rnay be sirnplify by writing its equation with dot products. Since 9 and
are then
perpendicular (9 . fie = 0). the first term of the following expression in brackets is zero:

w

slc

(2)

The sernicircular canals decornpose and report acceleration and velocity of head rotation fi by its
cornponents along the three canals on each side of the head fie : horizontal. anterior and posterior.
The two otolith organs on each side report the dynamical inertial forces generated during linear rnotion (translation) in two perpendicular plane. one vertical and the other horizontal relative to the head.
Here we assurne that a translation velocity signal (To) derived frorn or reported by the otolith afferents is available. The otoliths encode as well the head orientation relative to the gravity vector force.
but was not included in this study.
To cornplete the correspondence between the equation and a neural correlate. we need to determine
The eye position 9 is assurned to be given by the output of the
a physiological source for 9 and
velocity-to-position transformation or so-called ""neural integrator"" which provides eye position information and which is necessary for the activation of the rnotoneuron to sustain the eye in a fixed
position. The integrator for horizontal eye position appears to be located in the nucleus prepositus
hypoglossi in the pons. and the vertical integrator in the rnidbrain interstitial nucleus of Cajal. (Crawford. Cadera and Vilis. 1991; Cannon and Robinson. 1987). We assurne that the eye position is given
as the coordinates of the unit vector 9 along the ~ and 1; of fig. 1. The eye position depends on the
eye velocity according to
= 9 x w. For the special case w(t) = w(t)z. i.e. for horizontal head
rotation. the eye position coordinates are given by:

I!I.

'*

91 (t) =

91 (0) + f~ iJ2( r )w( r) dr

92(t) =

92(0) - f~ 91(r)w(r)dr

(3)

This is a set of two negatively coupled integrators. The ""neural integrator"" therefore does not integrate the eye velocity directly but a product of eye position and eye velocity. The distance frorn eye
to target
can be written using the gaze angles in the horizontal plane of the head:

I!I

1

(4)

1

(5)

Right eye:

19RT

Left eye:

19LT

where ?()R - () L) is the vergence angle. and I is the interocular distance; the angles are rneasured frorn
a straight ahead gaze. and take on negative values when the eyes are turned towards the right. Within
the oculornotor systern. the vergence angle and speed are encoded by the rnesencephalic reticular
formation neurons (Judge and Curnrning. 1986; Mays. 1984). The nucleus reticularis tegrnenti pontis
with reciprocal connections to the flocculus. oculornotor vermis. paravermis of the cerebellurn also
contains neurons which activity varies linearly with vergence angle (Gamlin and Clarke. 1995).
We conclude that it is possible to perform the cornputations needed to obtain an ideal VOR with signals known to be available physiologically.

O. J. M. D. COENEN, T. J. SEJNOWSKI

92
Dynamical Model Overview

Nod_
PftpoIItao

IIyposIoooI

Figure 2: Anatomical connections considered in the dynamical model. Only the left side is shown, the right
side is identical and connected to the left side only for the calculation of vergence angle. The nucleus prepositus
hypoglossi and the nucleus reticularis tegmenti pontis are meant to be representative of a class of nuclei in the
brain stem carrying eye position or vergence signal. All connections are known to exist except the connection
between the prepositus nucleus to the reticularis nucleus which has not been verified. Details of the cerebellum
are in fig. 3 and of the vestibular nucleus in fig. 4.

3 Dynamical Model
Snyder & King (1992) studied the effect of viewing distance and location of the axis of rotation on
the VOR in monkeys; their main results are reproduced in fig. 5. In an attempt to reproduce their
data and to understand how the signals that we have described in section 2 may be combined in time,
we constructed a dynamical model based on the kinematic model. Its basic anatomical structure is
shown in fig. 2. Details of the model are shown in fig. 3, and fig . 4 where all constants are written
using a millisecond time scale. The results are presented in fig. 5. The dynamical variables represent
the change of average firing rate from resting level of activity. The firing rate of the afferents has a
tonic component proportional to the velocity and a phasic component proportional to the acceleration
of movement. Physiologically, the afferents have a wide range of phasic and tonic amplitudes. This
is reflected by a wide selection of parameters in the numerators in the boxes of fig. 3 and fig. 4. The
Laplace transform of the integration operator in equation (3) of the eye position coordinates is ~.
Following Robinson (1981), we modeled the neural integrator with a gain and a time constant of
20 seconds. We therefore replaced the pure integrator ~ with 20~~~~1 in the calculations of eye
position. The term 1 in fig. 3 is calculated by using equations (4) and (5), and by using the integrator
9

20~o:!~1 on the eye velocity motor command to find the angles (h and (JR.

The dynamical model is based on the assumption that the cerebellum is required for context modulation, and that because of its architecture, the cerebellum is more likely to implement complex functions of multiple signals than other relevant nuclei. The major contributions of vergence and eye
position modulation on the VOR are therefore mediated by the cerebellum. Smaller and more transient contributions from eye position are assumed to be mediated through the vestibular nucleus as
shown in fig. 4. The motivation for combining eye position as in fig . 4 are, first, the evidence for eye
response oscillations; second, the theoretical consideration that linear movement information (To) is
useless without eye position information for proper VOR.
The parameters in the dynamical model were adjusted by hand after observing the behavior of the different components of the model and noting how these combine to produce the oscillations observed

93

A Dynamical Model of Context Dependencies for the Vestibulo-Ocular Reflex
Vestibular
Semicirtular

Cerebellum

c..l

O-

- - - - t 401+1 r-----?--f--..j

x

300+1

OIolith
0Igan

VHlibabr
Nuc1tul

Figure 3: Contribution of the cerebellum to the dynamical model. Filtered velocity inputs from the canals and
otoliths are combined with eye position according to equation (2). These calculations could be performed either
outside the cerebellum in one or multiple brain stem nuclei (as shown) or possibly inside the cerebellum. The
only output is to the vestibular nucleus. The Laplace notation is used in each boxes to represent a leaky integrator
with a time constant. input derivative and input gain. The term oe are the coordinates of the vector oe shown
in fig. 1. The x indicates a multiplication. The term! multiplies each inputs individually. The open arrows
indicate inhibitory (negative) connections.
Cere... lIum

VHlibalu

Semicimtlu

c.w

O--'----t~l---+--?----t~~

X
Figure 4: Contribution of the vestibular nucleus to the dynamical model. Three pathways in the vestibular nucleus process the canal and otolith inputs to drive the eye. The first pathway is modulated by the output of the
cerebellum through a FIN (Flocculus Target Neuron). The second and third pathways report transient information from the inputs which are combined with eye position in a manner identical to fig. 3. The location of these
calculations is hypothetical.

in the data. Even though the number of parameters in the model is not small. it was not possible to
fit any single response in fig. 5 without affecting most of the other eye responses. This puts severe
limits on the set of parameters allowed in the model.
The dynamical model suggests that the oscillations present in the data reflect: 1) important acceleration components in the neural signals. both rotational and linear, 2) different time delays between the
canal and otolith signal processing. and 3) antagonistic or synergistic action of the canal and otolith
signals with different axes of rotation, as described by the two terms in the bracket of equation (2).

4 Discussion
By fitting the dynamical model to the data, we tested the hypothesis that the VOR has a response
close to ideal taking into account the time constraints imposed by the sensory inputs and the neural
networks performing the computations. The vector computations that we used in the model may not

94

O. J. M. D. COENEN, T. J. SEJNOWSKI
Dynamical Model Responses vs Experimental Data
80

80
LOMtIOftof
.... 01 rotMIon

-,a.-om

.-

T..........~

60

40
20

~
w

-20

-20

-400~----~5~0------~
10
=0
~
Time (m.)

-40oL-----~
5~
0 ----~1~
0~
0-?

Time (m.)

Figure 5: Comparison between the dynamical model and monkey data. The dotted lines show the effect of
viewing distance and location of the axis of rotation on the VOR as recorded by Snyder & King (1992) from
monkeys in the dark. The average eye velocity response (of left and right eye) to a sudden change in head velocity is shown for different target distances (left) and rotational axes (right). On the left, the location of the axis
of rotation was in the midsagittal plane 12.5 cm behind the eyes (-12.5 cm), and the target distance was varied
between 220 cm and 9 cm. On the right, the target di stance was kept constant at 9 cm in front of the eye, and the
location of the axis of rotation was varied from 14 cm behind t04cm in front of the eyes (-14cm to 4cm) in the
midsagittal plane. The solid lines show the model responses. The model replicates many characteristics of the
data. On the left the model captures the eye velocity fluctuations between 20-50 ms, followed by a decrease and
an increase which are both modulated with target distance (50-80 ms). The later phase of the response (80-100
ms) is almost exact for 220 cm, and one peak is seen at the appropriate location for the other distances. On the
right the closest fits were obtained for the 4 cm and 0 cm locations. The mean values are in good agreement and
the waveforms are close, but could be shifted in time for the other locations of the axis of rotations. Finally, the
latest peak (..... lOOms) in the data appears in the model for -14 cm and 9 cm location.

be the representation used in the oculomotor system. Mathematically, the vector representation is
only one way to describe the computations involved. Other representations exist such as the quaternion representation which has been studied in the context of the saccadic system (Tweed and Vilis,
1987; see also Handzel and Flash, 1996 for a very general representation). Detailed comparisons
between the model and recordings from neurons will be require to settle this issue.
Direct comparison between Purkinje cell recordings (L.H. Snyder & W.M. King, unpublished data)
and predictions of the model could be used to determine more precisely the different inputs to some
Purkinje cells. The model can therefore be an important tool to gain insights difficult to obtain directly with experiments.
The question of how the central nervous system learns the transformations that we described still
remains. The cerebellum may be one site of learning for these transformations, and its output may
modulate the VOR in real time depending on the context. This view is compatible with the results
of Angelaki and Hess (1995) which indicate that the cerebellum is required to correctly perform an
otolith transformation. It is also consistent with adaptation results in the VOR. To test this hypothesis,
we have been working on a model of the cerebellum which learns to anticipate sensory inputs and
feedbacks, and use these signals to modulate the VOR. The learning in the cerebellum and vestibular
nuclei is mediated by the climbing fibers which report a reinforcement signal of the prediction error
(Coenen and Sejnowski. in preparation).

A Dynamical Model of Context Dependencies for the Vestibulo-Ocular Reflex

95

5 Conclusion
Most research on the VOR has assumed forward gaze focussed at infinity. The kinematics of offcenter gaze and fixation at finite distance necessitates nonlinear corrections that require the integration of a variety of sensory inputs. The dynamical model studied here is a working hypothesis for
how these corrections could be computed and is generally consistent with what is known about the
cerebellum and brain stem nuclei. We are, however, far from knowing the mechanisms underlying
these computations, or how they are learned through experience.

6 Acknowledgments
The first author was supported by a McDonnell-Pew Graduate Fellowship during this research. We
would like to thank Paul Viola for helpful discussions.
References
Angelaki, D. E. and Hess, B. J. (1995). Inertial representation of angular motion in the vestibular system of rhesus monkeyus. II. Otolith-controlled transformation that depends on an intact cerebellar nodulus. Journal
of Neurophysiology, 73(5): 1729-1751.
Cannon, S. C. and Robinson, D. A. (1987). Loss of the neural integrator of the oculomotor system from brain
stem lesions in monkey. Journal of Neurophysiology, 57(5):1383-1409.
Crawford, J. D., Cadera, W., and Vilis, T. (1991). Generation of torsional and vertical eye position signals by
the interstitial nucleus of Cajal. Science, 252:1551-1553.
Gamlin, P. D. R. and Clarke, R. J. (1995). Single-unit activity in the primate nucleus reticularis tegmenti pontis
related to vergence and ocular accomodation. Journal of Neurophysiology, 73(5):2115-2119.
Grossman, G. E., Leigh, R. J., Bruce, E. N., Huebner, W. P.,and Lanska, D.J. (1989). Performanceofthe human
vestibu1oocu1ar reflex during locomotion. Journal of Neurophysiology, 62(1 ):264-272.
Handzel, A. A. and Flash, T. (1996). The geometry of eye rotations and listing's law. In Touretzky, D., Mozer,
M., and Hasselmo, M., editors, Advances in Neural Information Processing Systems 8, Cambridge, MA.
MIT Press.
Judge, S. J. and Cumming, B. G. (1986). Neurons in the monkey midbrain with activity related to vergence eye
movement and accomodation. Journal of Neurophysiology, 55:915-930.
Mays, L. E. (1984). Neural control of vergence eye movements: Convergence and divergence neurons in midbrain. Journal of Neurophysiology, 51:1091-1108.
Robinson, D. A. (1981). The use of control systems analysis in the neurophysiology of eye movements. Ann.
Rev. Neurosci., 4:463-503.
Shelhamer, M., Robinson, D. A., and Tan, H. S. (1992). Context-specific adaptation of the gain of the vestibuloocular reflex in humans. Journal of Vestibular Research, 2:89-96.
Snyder, L. H. and King, W. M. (1992). Effect of viewing distance and location ofthe axis of head rotation on the
monkey's vestibuloocular reflex I. eye movement response. Journal of Neurophysiology, 67(4):861-874.
Tweed, D. and Vilis, T. (1987). Implications of rotational kinematics for the oculomotor system in three dimensions. Journal of Neurophysiology, 58(4):832-849.
Viire, E., Tweed, D., Milner, K., and Vilis, T. (1986). A reexamination of the gain ofthe vestibuloocular reflex.
Journal of Neurophysiology, 56(2):439-450.

"
1036,1995,Improved Gaussian Mixture Density Estimates Using Bayesian Penalty Terms and Network Averaging,,1036-improved-gaussian-mixture-density-estimates-using-bayesian-penalty-terms-and-network-averaging.pdf,Abstract Missing,"Improved Gaussian Mixture Density
Estimates Using Bayesian Penalty Terms
and Network Averaging

Dirk Ormoneit
Institut fur Informatik (H2)
Technische Universitat Munchen
80290 Munchen, Germany
ormoneit@inJormatik.tu-muenchen.de

Volker Tresp
Siemens AG
Central Research
81730 Munchen, Germany
Volker. Tresp@zJe.siemens.de

Abstract

We compare two regularization methods which can be used to improve the generalization capabilities of Gaussian mixture density
estimates. The first method uses a Bayesian prior on the parameter space. We derive EM (Expectation Maximization) update rules
which maximize the a posterior parameter probability. In the second approach we apply ensemble averaging to density estimation.
This includes Breiman's ""bagging"" , which recently has been found
to produce impressive results for classification networks.

1

Introduction

Gaussian mixture models have recently attracted wide attention in the neural network community. Important examples of their application include the training of
radial basis function classifiers, learning from patterns with missing features, and
active learning. The appeal of Gaussian mixtures is based to a high degree on the
applicability of the EM (Expectation Maximization) learning algorithm, which may
be implemented as a fast neural network learning rule ([Now91], [Orm93]). Severe
problems arise, however, due to singularities and local maxima in the log-likelihood
function. Particularly in high-dimensional spaces these problems frequently cause
the computed density estimates to possess only relatively limited generalization capabilities in terms of predicting the densities of new data points. As shown in this
paper, considerably better generalization can be achieved using regularization.

543

Improved Gaussian Mixture Density Estimates Using Bayesian Penalty Terms

We will compare two regularization methods. The first one uses a Bayesian prior
on the parameters. By using conjugate priors we can derive EM learning rules
for finding the MAP (maximum a posteriori probability) parameter estimate. The
second approach consists of averaging the outputs of ensembles of Gaussian mixture
density estimators trained on identical or resampled data sets. The latter is a form
of ""bagging"" which was introduced by Breiman ([Bre94]) and which has recently
been found to produce impressive results for classification networks. By using the
regularized density estimators in a Bayes classifier ([THA93], [HT94], [KL95]) , we
demonstrate that both methods lead to density estimates which are superior to the
unregularized Gaussian mixture estimate.

2

Gaussian Mixtures and the EM Algorithm

Consider the lroblem of estimating the probability density of a continuous random
vector x E 'R based on a set x* = {x k 11 S k S m} of iid. realizations of x. As a density model we choose the class of Gaussian mixtures p(xle) = L:7=1 Kip(xli, pi, E i ),
where the restrictions Ki ~ 0 and L:7=1 Kj = 1 apply. e denotes the parameter
vector (Ki' Iti, E i )i=1. The p(xli, Pi, E i ) are multivariate normal densities:
p( xli , Pi , Ei) = (271"")- 41Ei 1- 1 / 2 exp [-1/2(x - Pi)tEi 1 (x - Iti)] .
The Gaussian mixture model is well suited to approximate a wide class of continuous
probability densities. Based on the model and given the data x*, we may formulate
the log-likelihood as

lee)

= log [rr mk=l p(xkle)] = "",m
log ""'~ Kip(xkli, Pi, Ei) .
.L..."".k=1
.L..."".J=l

e

Maximum likelihood parameter estimates may efficiently be computed with the
EM (Expectation Maximization) algorithm ([DLR77]) . It consists of the iterative
application of the following two steps:
1. In the E-step, based on the current parameter estimates, the posterior
probability that unit i is responsible for the generation of pattern xk is

estimated as
(1)

2. In the M-step, we obtain new parameter estimates (denoted by the prime):
,

K ?
J

= -m1 L mk=1 h?k
J

~m

,

(2)

=

Pi

wk-l

~m

hki X k
hi

wl=l

~.' _ L:~1 hf(x k - pD(x k - pDt
L.J J

-

m

I

(3)

i

(4)

L:l=l hi
Note that K~ is a scalar , whereas p~ denotes a d-dimensional vector and E/
is a d x d matrix.
It is well known that training neural networks as predictors using the maximum
likelihood parameter estimate leads to overfitting. The problem of overfitting is
even more severe in density estimation due to singularities in the log-likelihood
function. Obviously, the model likelihood becomes infinite in a trivial way if we
concentrate all the probability mass on one or several samples of the training set.

544

D. ORMONEIT, V. TRESP

In a Gaussian mixture this is just the case if the center of a unit coincides with
one of the data points and E approaches the zero matrix. Figure 1 compares the
true and the estimated probability density in a toy problem. As may be seen,
the contraction of the Gaussians results in (possibly infinitely) high peaks in the
Gaussian mixture density estimate. A simple way to achieve numerical stability
is to artificially enforce a lower bound on the diagonal elements of E. This is a
very rude way of regularization, however, and usually results in low generalization
capabilities. The problem becomes even more severe in high-dimensional spaces.
To yield reasonable approximations, we will apply two methods of regularization,
which will be discussed in the following two sections.

Figure 1: True density (left) and unregularized density estimation (right).

3

Bayesian Regularization

In this section we propose a Bayesian prior distribution on the Gaussian mixture
parameters, which leads to a numerically stable version of the EM algorithm. We
first select a family of prior distributions on the parameters which is conjugate*.
Selecting a conjugate prior has a number of advantages. In particular, we obtain
analytic solutions for the posterior density and the predictive density. In our case,
the posterior density is a complex mixture of densities t . It is possible, however, to
derive EM-update rules to obtain the MAP parameter estimates.
A conjugate prior of a single multivariate normal density is a product of a normal
density N(JLilft,1]-lE i ) and a Wishart density Wi(E;lla,,8) ([Bun94]). A proper
conjugate prior for the the mixture weightings '"" = (""'1, ... , ""'n) is a Dirichlet density
D(""'hV. Consequently, the prior of the overall Gaussian mixture is the product
D("",lr)
N(JLilil, 71- 1Ei)Wi(E;1I a , ,8). Our goal is to find the MAP parameter
estimate, that is parameters which assume the maximum of the log-posterior

il7=1

Ip(S)

2:=~=1 log 2:=;=1 ""'iP(X k Ii, JLi, Ei ) + log D(""'lr)

+ 2:=;=1 [logN(JLilft, 71- 1Ei) + log Wi(E;lla, ,8)].
As in the unregularized case, we may use the EM-algorithm to find a local maximum
? A family F of probability distributions on 0 is said to be conjugate if, for every 1r E F,
the posterior 1r(0Ix) also belongs to F ([Rob94]).
tThe posterior distribution can be written as a sum of nm simple terms.
tThose densities are defined as follows (b and c are normalizing constants):

bII n

D(1I:17)

.=1

~.=l

(21r)-i 11,-IE;I-l/2 exp [-~(Il' -

N(Il.lp,1,-IE.)
W i(Ei l la,,8)

11:7,-1, with 11:, ~ 0 and "",n

=

cIEillo-Cd+l)/2 exp [-tr(,8Ei 1 )]

11:.

=1

Mt Ei 1 (1l' - M]
?

545

Improved Gaussian Mixture Density Estimates Using Bayesian Penalty Terms

of Ip(8). The E-step is identical to (1). The M-step becomes
,
L..""k-l hki + ri - 1
(5)
,L..""k=l hki x k + '1J1.
""'i
J1.i
hi
m + L..""i=l ri - n
L..,,1=1 i + 11

=

""m

E~ =

""m

""n

2:;-1 hf(x k -

A

= ""m

+ 11(J1.i 2:~1 h~ + 20: - d

J1.D(xk - J1.D t

I

jJ.)(J1.i - jJ.)t

(6)

+ 2f3

(7)

As typical for conjugate priors, prior knowledge corresponds to a set of artificial
training data which is also reflected in the EM-update equations. In our experiments, we focus on a prior on the variances which is implemented by f3 =F 0, where
o denotes the d x d zero matrix. All other parameters we set to ""neutral"" values:

ri=l'v'i : l::;i::;n,

0:= (d+I)/2,

11=0,

f3=iJl d

ld is the d x d unity matrix. The choice of 0: introdu~es a bias which favors large
variances?. The effect of various values of the scalar f3 on the density estimate is
illustrated in figure 2. Note that if iJ is chosen too small, overfitting still occurs. If
it is chosen to large , on the other hand, the model is too constraint to recognize the
underlying structure.

Figure 2: Regularized density estimates (left:

iJ =

0.05, right: 'iJ = 0.1).

Typically, the optimal value for iJ is not known a priori. The simplest procedure
consists of using that iJ which leads to the best performance on a validation set,
analogous to the determination of the optimal weight decay parameter in neural
network training. Alternatively, iJ might be determined according to appropriate
Bayesian methods ([Mac9I]). Either way, only few additional computations are
required for this method if compared with standard EM.

4

Averaging Gaussian Mixtures

In this section we discuss the averaging of several Gaussian mixtures to yield improved probability density estimation. The averaging over neural network ensembles
has been applied previously to regression and classification tasks ([PC93]) .
There are several different variants on the simple averaging idea. First, one may
train all networks on the complete set of training data. The only source of disagreement between the individual predictions consists in different local solutions
found by the likelihood maximization procedure due to different starting points.
Disagreement is essential to yield an improvement by averaging, however, so that
this proceeding only seems advantageous in cases where the relation between training data and weights is extremely non-deterministic in the sense that in training,
?If A is distributed according to Wi(AIO', (3), then E[A- 1 ] = (0' - (d + 1)/2)-1 {3. In our
case A is B;-I, so that E[Bi] -+ 00 ? {3 for 0' -+ (d + 1)/2.

546

D. ORMONEIT, V. TRESP

different solutions are found from different random starting points. A straightforward way to increase the disagreement is to train each network on a resampled
version of the original data set. If we resample the data without replacement, the
size of each training set is reduced, in our experiments to 70% of the original. The
averaging of neural network predictions based on resampling with replacement has
recently been proposed under the notation ""bagging"" by Breiman ([Bre94]), who
has achieved dramatic.ally improved results in several classification tasks. He also
notes, however, that an actual improvement of the prediction can only result if the
estimation procedure is relatively unstable. As discussed, this is particularly the
case for Gaussian mixture training. We therefore expect bagging to be well suited
for our task.

5

Experiments and Results

To assess the practical advantage resulting from regularization, we used the density
estimates to construct classifiers and compared the resulting prediction accuracies
using a toy problem and a real-world problem. The reason is that the generalization error of density estimates in terms of the likelihood based on the test data
is rather unintuitive whereas performance on a classification problem provides a
good impression of the degree of improvement. Assume we have a set of N labeled
data z* = {(xk, lk)lk = 1, ... , N}, where lk E Y = {I, ... , C} denotes the class label
of each input xk . A classifier of new inputs x is yielded by choosing the class I
with the maximum posterior class-probability p(llx). The posterior probabilities
may be derived from the class-conditional data likelihood p(xll) via Bayes theorem:
p(llx) = p(xll)p(l)/p(x) ex p(xll)p(l) . The resulting partitions ofthe input space are
optimal for the true p(llx). A viable way to approximate the posterior p(llx) is to
estimate p(xll) and p(l) from the sample data.
5.1

Toy Problem

In the toy classification problem the task is to discriminate the two classes of circulatory arranged data shown in figure 3. We generated 200 data points for each class
and subdivided them into two sets of 100 data points. The first was used for training, the second to test the generalization performance. As a network architecture
we chose a Gaussian mixture with 20 units. Table 1 summarizes the results, beginning with the unregularized Gaussian mixture which is followed by the averaging
and the Bayesian penalty approaches. The three rows for averaging correspond to
the results yielded without applying resampling (local max.), with resampling with-

Figure 3: Toy Classification Task.

547

Improved Gaussian Mixture Density Estimates Using Bayesian Penalty Terms

out replacement (70% subsets), and with resampling with replacement (bagging).
The performances on training and test set are measured in terms of the model loglikelihood. Larger values indicate a better performance. We report separate results
for dass A and B, since the densities of both were estimated separately. The final
column shows the prediction accuracy in terms of the percentage of correctly classified data in the test set. We report the average results from 20 experiments. The
numbers in brackets denote the standard deviations u of the results. Multiplying u
with T19;95%/v'20 = 0.4680 yields 95% confidence intervals. The best result in each
category is underlined.
Algorithm

Log- Likelihood

unreg.
Averaging:
local max.
70% subset
bagging
Penalty:
[3 = 0.01
[3 = 0.02
[3 = 0.05
[3 = 0.1

Accuracy

A
-120.8 (13.3)

-120.4 (10.8)

Test
A
B
-224.9 (32.6) -241.9 (34.1)

-115.6 (6.0)
-106.8 (5.8)
-83.8 (4.9)

-112.6 (6.6)
-105.1 (6.7)
-83.1 (7.1)

-200.9 (13.9)
-188.8 (9.5)
-194.2 (7.3)

-209.1 (16.3)
-196.4 (11.3)
-200.1 (11.3)

81.8% (3.1)
83.2% (2.9)
82.6% (3.4)

-149.3
-156.0
-173.9
-183.0

-146.5 (5.9)
-153.0 (4.8)
-167.0 (15.8)
-181.9 (21.1)

-186.2
-177.1
-182.0
-184.6

-182.9 (11.6)
-174.9 (7.0)
-173.9 (14.3)
-182.5 (21.1)

83.1%
84.4%
81.5%
78.5%

Training

(18.5)
(16.5)
(24.3)
(21.9)

B

(13.9)
(11.8)
(20.1)
(21.0)

I
80.6'70 (2.8)

(2.9)
(6.3)
(5.9)
(5.1)

Table 1: Performances in the toy classification problem .
As expected, all regularization methods outperform the maximum likelihood approach in terms of correct classification. The performance of the Bayesian regularization is hereby very sensitive to the appropriate choice of the regularization
parameter (3. Optimality of (3 with respect to the density prediction and oytimality
with respect to prediction accuracy on the test set roughly coincide (for (3 = 0.02).
A veraging is inferior to the Bayesian approach if an optimal {3 is chosen.
5.2

BUPA Liver Disorder Classification

As a second task we applied our methods to a real-world decision problem from
the medical environment. The problem is to detect liver disorders which might
arise from excessive alcohol consumption. Available information consists of five
blood tests as well as a measure of the patients' daily alcohol consumption. We
subdivided the 345 available samples into a training set of 200 and a test set of 145
samples. Due to the relatively few data we did not try to determine the optimal
regularization parameter using a validation process and will report results on the
test set for different parameter values.
Algorithm
unregularized
Bayesian penalty ({3 = 0.05)
Bayesian penalty ?(3 = 0.10)
Bayesian penal ty (3 = 0.20
averaging local maxima
averaging (70 % subset)
averaging (bagging)

Accuracy
64.8 %
65.5 %
66.9 %
61.4 %
65 .5 0
72.4 %
71.0 %

Table 2: Performances in the liver disorder classification problem.

548

D. ORMONEIT. V. TRESP

The results of our experiments are shown in table 2. Again, both regularization
methods led to an improvement in prediction accuracy. In contrast to the toy problem, the averaged predictor was superior to the Bayesian approach here. Note that
the resampling led to an improvement of more than five percent points compared
to unresampled averaging.

6

Conclusion

We proposed a Bayesian and an averaging approach to regularize Gaussian mixture
density estimates. In comparison with the maximum likelihood solution both approaches led to considerably improved results as demonstrated using a toy problem
and a real-world classification task. Interestingly, none of the methods outperformed
the other in both tasks. This might be explained with the fact that Gaussian mixture density estimates are particularly unstable in high-dimensional spaces with
relatively few data. The benefit of averaging might thus be greater in this case.
A veraging proved to be particularly effective if applied in connection with resampIing of the training data, which agrees with results in regression and classification
tasks. If compared to Bayesian regularization, averaging is computationally expensive. On the other hand, Baysian approaches typically require the determination of
hyper parameters (in our case 13), which is not the case for averaging approaches.

References
[Bre94]

L. Breiman. Bagging predictors. Technical report , UC Berkeley, 1994.

[Bun94]

W . Buntine. Operations for learning with graphical models. Journal of Artificial
Intelligence Research, 2:159-225, 1994.

[DLR77] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from
incomplete data via the EM algorithm. J. Royal Statistical Society B, 1977.
[HT94]

T. Hastie and R. Tibshirani. Discriminant analysis by gaussian mixtures. Technical report , AT&T Bell Labs and University of Toronto, 1994.

[KL95]

N. Kambhatla and T. K. Leen. Classifying with gaussian mixtures and clusters.
In Advances in Neural Information Processing Systems 7. Morgan Kaufman,
1995.

[Mac91]

D. MacKay. Bayesian Modelling and Neural Networks. PhD thesis, California
Institute of Technology, Pasadena, 1991.

[Now91] S. J. Nowlan. Soft Competitive Adaption: Neural Network Learning Algorithms
based on Fitting Statistical Mixtures. PhD thesis, School of Computer Science,
Carnegie Mellon University, Pittsburgh, 1991.
[Orm93] D. Ormoneit. Estimation of probability densities using neural networks. Master's
thesis, Technische Universitiit Munchen, 1993.
[PC93]

M. P. Perrone and L. N. Cooper. When networks disagree: Ensemble methods for
hybrid Neural networks. In Neural Networks for Speech and Image Processing.
Chapman Hall, 1993.

[Rob94]

C. P. Robert. The Bayesian Choice. Springer-Verlag, 1994.

[THA93] V. Tresp, J. Hollatz, and S. Ahmad. Network structuring and training using
rule-based knowledge. In Advances in Neural Information Processing Systems 5.
Morgan Kaufman, 1993.

"
1037,1995,Quadratic-Type Lyapunov Functions for Competitive Neural Networks with Different Time-Scales,,1037-quadratic-type-lyapunov-functions-for-competitive-neural-networks-with-different-time-scales.pdf,Abstract Missing,"Quadratic-Type Lyapunov Functions for
Competitive Neural Networks with
Different Time-Scales
Anke Meyer-Base
Institute of Technical Informatics
Technical University of Darmstadt
Darmstadt, Germany 64283

Abstract
The dynamics of complex neural networks modelling the selforganization process in cortical maps must include the aspects of
long and short-term memory. The behaviour of the network is such
characterized by an equation of neural activity as a fast phenomenon and an equation of synaptic modification as a slow part of the
neural system. We present a quadratic-type Lyapunov function for
the flow of a competitive neural system with fast and slow dynamic
variables. We also show the consequences of the stability analysis
on the neural net parameters.

1

INTRODUCTION

This paper investigates a special class of laterally inhibited neural networks. In
particular, we have examined the dynamics of a restricted class of laterally inhibited
neural networks from a rigorous analytic standpoint.
The network models for retinotopic and somatotopic cortical maps are usually composed of several layers of neurons from sensory receptors to cortical units, with
feedforward excitations between the layers and lateral (or recurrent) connection
within the layer. Standard techniques include (1) Hebbian rule and its variations
for modifying synaptic efficacies, (2) lateral inhibition for establishing topographical
organization of the cortex, and (3) adiabatic approximation in decoupling the dynamics of relaxation (which is on the fast time scale) and the dynamics of learning
(which is on the slow time scale) of the network . However, in most cases, only computer simulation results were obtained and therefore provided limited mathematical
understanding of the self-organizating neural response fields.
The networks under study model the dynamics of both the neural activity levels,

A. MEYER-BASE

338

the short-term memory (STM), and the dynamics of synaptic modifications, the
long-term memory (LTM). The actual network models under consideration may be
considered extensions of Grossberg's shunting network [Gr076] or Amari's model
for primitive neuronal competition [Ama82]. These earlier networks are considered
pools of mutually inhibitory neurons with fixed synaptic connections. Our results
extended these earlier studies to systems where the synapses can be modified by
external stimuli. The dynamics of competitive systems may be extremely complex,
exhibiting convergence to point attractors and periodic attractors. For networks
which model only the dynamic of the neural activity levels Cohen and Grossberg
[CG83] found a Lyapunov function as a necessary condition for the convergence
behavior to point attractors.
In this paper we apply the results of the theory of Lyapunov functions for singularly
perturbed systems on large-scale neural networks, which have two types of state
variables (LTM and STM) describing the slow and the fast dynamics of the system.
So we can find a Lyapunov function for the neural system with different time-scales
and give a design concept of storing desired pattern as stable equilibrium points.

2

THE CLASS OF NEURAL NETWORKS WITH
DIFFERENT TIME-SCALES

This section defines the network of differential equations characterizing laterally
inhibited neural networks. We consider a laterally inhibited network with a deterministic signal Hebbian learning law [Heb49] and is similar to the spatiotemporal
system of Amari [Ama83] .
The general neural network equations describe the temporal evolution of the STM
(activity modification) and LTM states (synaptic modification). For the jth neuron
of aN-neuron network these equations are:
N

Xj

= -ajxj + L

D i j!(Xi )

+ BjSj

(1)

i=l

(2)

where Xj is the current activity level, aj is the time constant of the neuron , Bj is
the contribution of the external stimulus term, !(Xi) is the neuron's output , D ij is
the .lateral inhibition term and Yi is the external stimulus. The dynamic variable
Sj represents the synaptic modification state and lyl21 is defined as lyl2 = yTy.
We will assume that the input stimuli are normalized vectors of unit magnitude
lyl2 = 1. These systems will be subject to our analysis considerations regarding the
stability of their equilibrium points.

3

ASYMPTOTIC STABILITY OF NEURAL
NETWORKS WITH DIFFERENT TIME-SCALES

We show in this section that it is possible to determine the asymptotic stability of
this class of neural networks interpreting them as nonlinear singularly perturbed
systems. While singular perturbation theory, a traditional tool of fluid dynamics
and nonlinear mechanics, embraces a wide variety of dynamic phenomena possesing
slow and fast modes, we show that singular perturbations are present in many

339

Quadratic-type Lyapunov Functions for Competitive Neural Networks

neurodynamical problems. In this sense we apply in this paper the results of this
valuable analysis tool on the dynamics of laterally inhibited networks.
In [SK84] is shown that a quadratic-type Lyapunov function for a singularly perturbed system is obtained as a weighted sum of quadratic-type Lyapunov functions
of two lower order systems: the so-called reduced and the boundary-layer systems.
Assuming that each of the two systems is asymptotically stable and has a Lyapunov
function, conditions are derived to guarantee that, for a sufficiently small perturbation parameter, asymptotic stability of the singularly perturbed system can be
established by means of a Lyapunov function which is composed as a weighted sum
of the Lyapunov functions of the reduced and boundary-layer systems.
Adopting the notations from [SK84] we will consider the singularly perturbed system 2

x = f(x, y)

x E Bx C R n

(3)
(4)

We assume that, in Bx and By, the origin (x = y = 0) is the unique equilibrium point
and (3) and (4) has a unique solution. A reduced system is defined by setting c = in (3)
and (4) to obtain

?

x = f(x,y)

(5)

O=g(x,y,O)

(6)

Assuming that in Bx and By, (6) has a unique root y = h(x), the reduced system is
rewritten as

x = f(x, h(x)) = fr(x)

(7)

A boundary-layer system is defined as

ay
aT

(8)

= g(X,y(T),O)

where T = tic is a stretching time scale. In (8) the vector x E R n is treated as a fixed
unknown parameter that takes values in Bx. The aim is to establish the stability properties
of the singularly perturbed system (3) and (4), for small c, from those of the reduced system
(7) and the boundary-layer system (8). The Lyapunov functions for system 7 and 8 are of
quadratic-type. In [SK84] it is shown that under mild assumptions, for sufficiently small
c, any weighted sum of the Lyapunov functions of the reduced and boundary-layer system
is a quadratic-type Lyapunov function for the singularly perturbed system (3) and (4).
The necessary assumptions are stated now [SK84]:
1. The reduced system (7) has a Lyapunov function V : R n

-+

R+ such that for all

xE Bx
(9)
where t/I(x) is a scalar-valued function of x that vanishes at x = 0 and is different
from zero for all other x E Bx. This condition guarantees that x = 0 is an
asymptotically stable equilibrium point of the reduced system (7).
2The symbol Bx indicates a closed sphere centered at x = OJ By is defined in the same
way.

A. MEYER-BASE

340

2. The boundary-layer system (8) has a Lyapunov function W(x, y) : R n x R m
R+ such that for all x E Bx and y E By

('\7yW(X,y)fg(X,y , O)::;-0:2??(y-h(x))

0:2>0

->

(10)

where ?>(y - h(x)) is a scalar-valued function (y - h(x)) E R m that vanishes
at y = h(x) and is different from zero for all other x E Bx and y E By. This
condition guarantees that y = h(x) is an asymptotically stable equilibrium point
of the boundary-layer system (8).

3. The following three inequalities hold ""Ix E Bx and Vy E By:

a.)
('\7 ,..W(x, y)ff(x, y) ::; C1?>2(y - h(x)) + C21/J(X)?>(Y - h(x))

(11)

b.)
('\7,.. V(x)f[f(x, y) - f(x, h(x))] ::; /311/J(X)?>(y - h(x))

(12)

c.)

<

('\7yW(x,y)f[g(x,y,()-g(x,y,O)]

(K1?>2(y - h(x))

+

(K21/J(X)?>(Y - h(x)) (13)

The constants C1, C2, /31 , K1 and K2 are nonnegative. The inequalities above determine the
permissible interaction between the slow and fast variables. They are basically smoothness
requirements of f and g.

After these introductory remarks the stability criterion is now stated:
Theorem: Suppose that conditions 1-3 hold; let d be a positive number such that
0< d < 1, and let c*(d) be the positive number given by

(14)

where Ih = f{2 + G2, 'Y = f{l + Gl , then for all c < c*(d), the origin (x
is an asymptotically stable equilibrium point of (3) and (.0 and
v(x, y) = (1 - d)V(x)

+ dW(x, y)

= y = 0)
(15)

is a Lyapunov function of (3) and (4).

t

If we put c =
as a global neural time constant in equation (1) then we have
to determine two Lyapunov functions: one for the boundary-layer system and the
other for the reduced-order system.
In [CG83] is mentioned a global Lyapunov function for a competitive neural network
with only an activation dynamics.

(16)

under the constraints: mij = mji, ai(xi)

2: 0,

fj(xj)

2: O.

This Lyapunov-function can be t?aken as one for the boundary-layer system (STMequation) , if the LTM contribution Si is considered as a fixed unknown parameter:

Quadratic-type Lyapunov Functions for Competitive Neural Networks
N

W(x, S) = L
j=l

r

10

i

(Xi

N

aj((j)!;((j)d(j-L BjSj

0

10

j=l

341

1 N
f;((j)d(j-2 L Dij!i(Xj)!k(Xk)
j=l

0

(17)

For the reduced-order system (LTM- equation) we can take as a Lyapunov-function:
N

V(S)

= ~STS = L S;

(18)

i=l

The Lyapunov-function for the coupled STM and LTM dynamics is the sum of the
two Lyapunov-function:

vex, S)

4

= (1 -

d)V(S)

+ dW(x, S)

(19)

DESIGN OF STABLE COMPETITIVE NEURAL
NETWORKS

Competitive neural networks with learning rules have moving equilibria during the
learning process. The concept of asymptotic stability derived from matrix perturbation theory can capture this phenomenon.
We design in this section a competitive neural network that is able to store a desired
pattern as a stable equilibrium.
The theoretical implications are illustrated in an example of a two neuron network .
Example: Let N = 2, ai = A, B j = B, Dii = a > 0, Dij = -(3
nonlinearity be a linear function f(xj) = Xj in equations (1) and (2).

<

0 and the

We get for the boundary-layer system:
N

Xj

= -Axj + L

Dijf(xd + BSj

(20)

i=l

and for the reduced-order system:

.

B
lA-a

C
A-a

S? = S ? [ - - -1] - - J

(21)

Then we get for the Lyapunov-functions:
(22)
and
(23)

A. MEYER-BASE

342

-0.2
.

\
\

-0.4
[JJ

OJ
.IJ

lIS

.IJ
[JJ

-0.6

/

~
U)

\J

-0.8

-1

-1.2

~

o

__

~

__

~

1

2

____ __ __- L__
3
4
5
time in msec
~

~

~~

6

__

~

__- L__

7

~~~

8

9

10

Figure 1: Time histories of the neural network with the origin as an equilibrium
point: STM states.
For the nonnegative constants we get: al = 1 - A~a' a2
with B < 0 , and C2 = i3l = i32 = 1 and I<l = I<2 = O.

= (A -

a)2,

Cl

= 'Y = - B,

We get some interesting implications from the above results as: A-a> B , A-a> 0
and B < o.
The above impications can be interpreted as follows: To achieve a stable equilibrium
point (0,0) we should have a negative contribution of the external stimulus term
and the sum of the excitatory and inhibitory contribution of the neurons should
be less than the time constant of a neuron. An evolution of the trajectories of the
STM and LTM states for a two neuron system is shown in figure 1 and 2. The
STM states exhibit first an oscillation from the expected equilibrium point, while
the LTM states reach monotonically the equilibrium point. We can see from the
pictures that the equilibrium point (0,0) is reached after 5 msec by the STM- and
LTM-states.

= 55+ll.of
.
.d(1-d)
From the above formula we can see that f*(d) has a maximum at d = d* = 0.5.
Choosing B = -5, A

5

= 1 and a = 0.5 we obtain for

f*(d) : f*(d)

CONCLUSIONS

We presented in this paper a quadratic-type Lyapunov function for analyzing the
stability of equilibrium points of competitive neural networks with fast and slow
dynamics. This global stability analysis method is interpreting neural networks
as nonlinear singularly perturbed systems. The equilibrium point is constrained
to a neighborhood of (0,0). This technique supposes a monotonically increasing
non-linearity and a symmetric lateral inhibition matrix. The learning rule is a
deterministic Hebbian. This method gives an upper bound on the perturbation

343

Quadratic-type Lyapunov Functions for Competitive Neural Networks
0.6

~--~--~----~--~--~----~--'---~--~r---.

0.5

0.4
III

<II

.j.J

III

.j.J

III

0.3

~
~
0.2

0.1

o L-__
1
o

~~~~~~~

2

3

____~__~__~__- L_ _~

4

5

6

7

8

9

10

time in msec

Figure 2: Time histories of the neural network with the origin as an equilibrium
point: LTM states.
parameter and such an estimation of a maximal positive neural time-constant. The
practical implication ofthe theoretical problem is the design of a competitive neural
network that is able to store a desired pattern as a stable equilibrium.

References
[Ama82] S. Amari. Competitive and cooperative aspects in dynamics of neural excitation and self-organization. Competition and cooperation in neural networks, 20:1-28, 7 1982.
[Ama83] S. Amari. Field theory of self-organizing neural nets. IEEE Transactions
on systems, machines and communication, SMC-13:741-748, 7 1983.
A. M. Cohen und S. Grossberg. Absolute Stability of Global Pattern Formation and Parallel Memory Storage by Competitive Neural Networks.
IEEE Transactions on Systems, Man and Cybernetics, SMC-13:815-826,
9 1983.
[Gro76] S. Grossberg. Adaptive Pattern Classification and Universal Recording.
Biological Cybernetics, 23:121-134, 1 1976.

[CG83]

[Heb49] D. O. Hebb. The Organization of Behavior. J. Wiley Verlag, 1949.
[SK84] Ali Saberi und Hassan Khalil. Quadratic-Type Lyapunov Functions for
Singularly Perturbed Systems. IEEE Transactions on A utomatic Control,
pp. 542-550, June 1984.

"
1038,1995,Stable LInear Approximations to Dynamic Programming for Stochastic Control Problems with Local Transitions,,1038-stable-linear-approximations-to-dynamic-programming-for-stochastic-control-problems-with-local-transitions.pdf,Abstract Missing,"Stable Linear Approximations to
Dynamic Programming for Stochastic
Control Problems with Local Transitions

Benjamin Van Roy and John N. Tsitsiklis
Laboratory for Information and Decision Systems
Massachusetts Institute of Technology
Cambridge, MA 02139
e-mail: bvr@mit.edu, jnt@mit.edu

Abstract
We consider the solution to large stochastic control problems by
means of methods that rely on compact representations and a variant of the value iteration algorithm to compute approximate costto-go functions. While such methods are known to be unstable in
general, we identify a new class of problems for which convergence,
as well as graceful error bounds, are guaranteed. This class involves linear parameterizations of the cost-to- go function together
with an assumption that the dynamic programming operator is a
contraction with respect to the Euclidean norm when applied to
functions in the parameterized class. We provide a special case
where this assumption is satisfied, which relies on the locality of
transitions in a state space. Other cases will be discussed in a full
length version of this paper.

1

INTRODUCTION

Neural networks are well established in the domains of pattern recognition and
function approximation, where their properties and training algorithms have been
well studied. Recently, however, there have been some successful applications of
neural networks in a totally different context - that of sequential decision making
under uncertainty (stochastic control).
Stochastic control problems have been studied extensively in the operations research
and control theory literature for a long time, using the methodology of dynamic
programming [Bertsekas, 1995]. In dynamic programming, the most important
object is the cost-to-go (or value) junction, which evaluates the expected future

1046

B. V. ROY, 1. N. TSITSIKLIS

cost to be incurred, as a function of the current state of a system. Such functions
can be used to guide control decisions.
Dynamic programming provides a variety of methods for computing cost-to- go
functions. Unfortunately, dynamic programming is computationally intractable in
the context of many stochastic control problems that arise in practice. This is
because a cost-to-go value is computed and stored for each state, and due to the
curse of dimensionality, the number of states grows exponentially with the number
of variables involved.
Due to the limited applicability of dynamic programming, practitioners often rely
on ad hoc heuristic strategies when dealing with stochastic control problems. Several recent success stories - most notably, the celebrated Backgammon player of
Tesauro (1992) - suggest that neural networks can help in overcoming this limitation. In these applications, neural networks are used as compact representations
that approximate cost- to-go functions using far fewer parameters than states. This
approach offers the possibility of a systematic and practical methodology for addressing complex stochastic control problems.
Despite the success of neural networks in dynamic programming, the algorithms
used to tune parameters are poorly understood. Even when used to tune the parameters of linear approximators, algorithms employed in practice can be unstable
[Boyan and Moore, 1995; Gordon, 1995; Tsitsiklis and Van Roy, 1994].
Some recent research has focused on establishing classes of algorithms and compact
representation that guarantee stability and graceful error bounds. Tsitsiklis and Van
Roy (1994) prove results involving algorithms that employ feature extraction and interpolative architectures. Gordon (1995) proves similar results concerning a closely
related class of compact representations called averagers. However, there remains
a huge gap between these simple approximation schemes that guarantee reasonable
behavior and the complex neural network architectures employed in practice.
In this paper, we motivate an algorithm for tuning the parameters of linear compact representations, prove its convergence when used in conjunction with a class
of approximation architectures, and establish error bounds. Such architectures are
not captured by previous results. However, the results in this paper rely on additional assumptions. In particular, we restrict attention to Markov decision problems
for which the dynamic programming operator is a contraction with respect to the
Euclidean norm when applied to functions in the parameterized class. Though
this assumption on the combination of compact representation and Markov decision problem appears restrictive, it is actually satisfied by several cases of practical
interest. In this paper, we discuss one special case which employs affine approximations over a state space, and relies on the locality of transitions. Other cases will
be discussed in a full length version of this paper.

2

MARKOV DECISION PROBLEMS

We consider infinite horizon, discounted Markov decision problems defined on a
finite state space S = {I, .. . , n} [Bertsekas, 1995]. For every state i E S, there is
a finite set U(i) of possible control actions, and for each pair i,j E S of states and
control action u E U (i) there is a probability Pij (u) of a transition from state i to
state j given that action u is applied. Furthermore, for every state i and control
action u E U (i), there is a random variable Ciu which represents the one-stage cost
if action u is applied at state i.
Let

f3

E [0,1) be a discount factor. Since the state spaces we consider in this paper

Stable Linear Approximations Programming for Stochastic Control Problems

1047

are finite, we choose to think of cost-to-go functions mapping states to cost- to-go
values in terms of cost-to-go vectors whose components are the cost-to-go values
of various states. The optimal cost-to-go vector V* E !R n is the unique solution to
Bellman's equation:

Vi*= min. (E[CiU]+.BLPij(U)Vj*),
uEU(t)

ViES.

(1)

jES

If the optimal cost-to-go vector is known, optimal decisions can be made at any
state i as follows:

u*=arg min. (E[CiU]+.BLPij(U)l--j*),
uEU(t)

ViES.

jES

There are several algorithms for computing V* but we only discuss the value iteration algorithm which forms the basis of the approximation algorithm to be considered later on. We start with some notation. We define the dynamic programming
operator as the mapping T : !R n r-t !R n with components Ti : !R n r-t !R defined by

Ti(V) = min. (E[CiU]+.BLPij(U)Vj ),
uEU(t)

ViES.

(2)

jES

It is well known and easy to prove that T is a maximum norm contraction. In
particular ,

IIT(V) - T(V')lloo :s;

.BIIV -

V'lIoo,

The value iteration algorithm is described by
V(t + 1) = T(V(t)),
where V (0) is an arbitrary vector in !R n used to initialize the algorithm. It is easy
to see that the sequence {V(t)} converges to V*, since T is a contraction.

3

APPROXIMATIONS TO DYNAMIC PROGRAMMING

Classical dynamic programming algorithms such as value iteration require that we
maintain and update a vector V of dimension n. This is essentially impossible when
n is extremely large, as is the norm in practical applications. We set out to overcome
this limitation by using compact representations to approximate cost-to-go vectors.
In this section, we develop a formal framework for compact representations, describe
an algorithm for tuning the parameters of linear compact representations, and prove
a theorem concerning the convergence properties of this algorithm.

3.1

COMPACT REPRESENTATIONS

A compact representation (or approximation architecture) can be thought of as a
scheme for recording a high-dimensional cost-to-go vector V E !R n using a lowerdimensional parameter vector wE !Rm (m ?n). Such a scheme can be described by
a mapping V : !Rm r-t !R n which to any given parameter vector w E !R m associates
a cost-to-go vector V(w). In particular, each component Vi (w) of the mapping is
the ith component of a cost-to-go vector represented by the parameter vector w.
Note that, although we may wish to represent an arbitrary vector V E !R n , such a
scheme allows for exact representation only of those vectors V which happen to lie
in the range of V.
In this paper, we are concerned exclusively with linear compact representations of
the form V(w) = Mw, where M E !Rnxm is a fixed matrix representing our choice
of approximation architecture. In particular, we have Vi(w) = Miw, where Mi (a
row vector) is the ith row of the matrix M.

1048

3.2

B. V. ROY, J. N. TSITSIKLIS

A STOCHASTIC APPROXIMATION SCHEME

Once an appropriate compact representation is chosen, the next step is to generate
a parameter vector w such that V{w) approximates V*. One possible objective is
to minimize squared error of the form IIMw - V*II~. If we were given a fixed set
of N samples {( iI, ~:), (i2' Vi;), ... , (i N, ~:)} of an optimal cost-to-go vector V*, it
seems natural to choose a parameter vector w that minimizE's E7=1 (Mij w - ~;)2.
On the other hand, if we can actively sample as many data pairs as we want, one
at a time, we might consider an iterative algorithm which generates a sequence of
parameter vectors {w(t)} that converges to the desired parameter vector. One such
algorithm works as follows: choose an initial guess w(O), then for each t E {O, 1, ... }
sample a state i{t) from a uniform distribution over the state space and apply the
iteration
(3)
where {a(t)} is a sequence of diminishing step sizes and the superscript T denotes
a transpose. Such an approximation scheme conforms to the spirit of traditional
function approximation - the algorithm is the common stochastic gradient descent
method. However, as discussed in the introduction, we do not have access to such
samples of the optimal cost-to-go vector. We therefore need more sophisticated
methods for tuning parameters.
One possibility involves the use of an algorithm similar to that of Equation 3,
replacing samples of ~(t) with TiCt) (V(t)). This might be justified by the fact that
T(V) can be viewed as an improved approximation to V*, relative to V. The
modified algorithm takes on the form
(4)

Intuitively, at each time t this algorithm treats T(Mw(t)) as a ""target"" and takes
a steepest descent step as if the goal were to find a w that would minimize IIMwT(Mw(t))II~. Such an algorithm is closely related to the TD(O) algorithm of Sutton
(1988). Unfortunately, as pointed out in Tsitsiklis and Van Roy (1994), such a
scheme can produce a diverging sequence {w(t)} of weight vectors even when there
exists a parameter vector w* that makes the approximation error V* - Mw* zero at
every state. However, as we will show in the remainder of this paper, under certain
assumptions, such an algorithm converges.

3.3

MAIN CONVERGENCE RESULT

Our first assumption concerning the step size sequence {a(t)} is standard to stochastic approximation and is required for the upcoming theorem.
Assumption 1 Each step size a(t) is chosen prior to the generation of i(t), and
the sequence satisfies E~o a(t) = 00 and E~o a 2 (t) < 00.
Our second assumption requires that T : lR n t-+ lR n be a contraction with respect
to the Euclidean norm, at least when it operates on value functions that can be
represented in the form Mw, for some w. This assumption is not always satisfied,
but it appears to hold in some situations of interest, one of which is to be discussed
in Section 4.

{3' E [0, 1) such that
::; {3'IIMw - Mw'112,

Assumption 2 There exists some

IIT(Mw) - T(Mw')112

Vw,w' E lRm.

Stable Linear Approximations to Programming for Stochastic Control Problems

1049

The following theorem characterizes the stability and error bounds associated with
the algorithm when the Markov decision problem satisfies the necessary criteria.
Theorem 1 Let Assumptions 1 and 2 hold, and assume that M has full column

rank. Let I1 = M(MT M)-l MT denote the projection matrix onto the subspace
X = {Mwlw E ~m}. Then,
(a) With probability 1, the sequence w(t) converges to w*, the unique vector that
solves:
Mw* = I1T(Mw*).
(b) Let V* be the optimal cost-to-go vector. The following error bound holds:
IIMw* - V*1I2

3.4

~ (1 ;!~ynllI1V* - V*lloo.

OVERVIEW OF PROOF

Due to space limitations, we only provide an overview of the proof of Theorem 1.
Let s : ~m

f-7 ~m

be defined by

s(w)

=E

[( Miw - Ti(Mw(t)))MT] ,

where the expectation is taken over i uniformly distributed among {I, .. . , n}.
Hence,
E[w(t + l)lw(t), a(t)] = w(t) - a(t)s(w(t)),
where the expectation is taken over i(t). We can rewrite s as

s(w) =

~(MTMW -

MTT(MW)) ,

and it can be thought of as a vector field over ~m. If the sequence {w(t)} converges
to some w, then s (w) must be zero, and we have

MTMw
Mw

MTT(Mw)
I1T(Mw).

=

Note that

III1T(Mw) -

I1T(Mw')lb

~

{j'IIMw - Mw'112,

Vw,w' E

~m,

due to Assumption 2 and the fact that projection is a nonexpansion of the Euclidean
norm. It follows that I1Te) has a unique fixed point w* E ~m, and this point
uniquely satisfies
Mw* = I1T(Mw*).
We can further establish the desired error bound:

IIMw* -

V*112 <

IIMw* - I1T(I1V*) 112 + III1T(I1V*) - I1V*112 + III1V*
< {j'IIMw* - V*112 + IIT(I1V*) - V*112 + III1V* - V*1I2
< t3'IIMw* - V*112 + (1 + mv'nII I1 V* - V*lloo,

- V*112

and it follows that

Consider the potential function U(w)
(\1U(w))T s(w) 2 ,U(w), for some,

=
>

~llw

-

w*II~.

We will establish that
0, and we are therefore dealing with a

B. V. ROY, J. N. TSITSIKLIS

1050

""pseudogradient algorithm"" whose convergence follows from standard results on
stochastic approximation [Polyak and Tsypkin, 1972J. This is done as follows:

(\7U(w)f s(w)

~ (w -

w*) T MT (Mw - T(Mw))

~ (w -

w*) T MT(Mw - IIT(Mw) - (J - II)T(MW))

~(MW-Mw*)T(MW-IIT(MW)),

=

where the last equality follows because MTrr = MT. Using the contraction assumption on T and the nonexpansion property of projection mappings, we have

IlIIT(Mw) - Mw*112
::;

IIIIT(Mw) - rrT(Mw*)112
,6'IIMw - Mw*1I2'

and applying the Cauchy-Schwartz inequality, we obtain

(\7U(W))T s(w)

1
n

> -(IIMw - Mw*ll~ -IIMw - Mw*1121IMw* - IIT(Mw)112)
> !:.(l - ,6')IIMw - Mw*II~?
n

Since M has full column rank, it follows that (\7U(W))T s(w) ~ 1'U(w), for some
fixed l' > 0, and the proof is complete.

4

EXAMPLE: LOCAL TRANSITIONS ON GRIDS

Theorem 1 leads us to the next question: are there some interesting cases for which
Assumption 2 is satisfied? We describe a particular example here that relies on
properties of Markov decision problems that naturally arise in some practical situations.
When we encounter real Markov decision problems we often interpret the states
in some meaningful way, associating more information with a state than an index
value. For example, in the context of a queuing network, where each state is one
possible queue configuration, we might think of the state as a vector in which each
component records the current length of a particular queue in the network. Hence,
if there are d queues and each queue can hold up to k customers, our state space is
(Le., the set of vectors with integer components each in the range
a finite grid
{O, ... ,k-l}).

zt

Consider a state space where each state i E {I, ... , n} is associated to a point
xi E
(n = k d ), as in the queuing example. We might expect that individual
transitions between states in such a state space are local. That is, if we are at
a state xi the next visited state x j is probably close to xi in terms of Euclidean
distance. For instance, we would not expect the configuration of a queuing network
to change drastically in a second. This is because one customer is served at a time
so a queue that is full can not suddenly become empty.

zt

zt

grows exponentially
Note that the number of states in a state space of the form
with d. Consequently, classical dynamic programming algorithms such as value
iteration quickly become impractical. To efficiently generate an approximation to
the cost-to-go vector, we might consider tuning the parameters w E Rd and a E R
of an affine approximation ~(w, a) = w T xi + a using the algorithm presented in
the previous section. It is possible to show that, under the following assumption

Stable Linear Approximations to Programming for Stochastic Control Problems

1051

concerning the state space topology and locality of transitions, Assumption 2 holds
with f3' = .;f32
the algorithm.

+ k~3' and

thus Theorem 1 characterizes convergence properties of

Assumption 3 The Markov decision problem has state space S = {1, ... , k d }, and
each state i is uniquely associated with a vector xi E
with k ~ 6(1 - (32)-1 + 3.
A ny pair xi, x j E
of consecutively visited states either are identical or have
exactly one unequal component, which differs by one.

zt

zt

While this assumption may seem restrictive, it is only one example. There are many
more candidate examples, involving other approximation architectures and particular classes of Markov decision problems, which are currently under investigation.

5

CONCLUSIONS

We have proven a new theorem that establishes convergence properties of an algorithm for generating linear approximations to cost-to-go functions for dynamic
programming. This theorem applies whenever the dynamic programming operator
for a Markov decision problem is a contraction with respect to the Euclidean norm
when applied to vectors in the parameterized class. In this paper, we have described
one example in which such a condition holds. More examples of practical interest
will be discussed in a forthcoming full length version of this paper.
Acknowledgments

This research was supported by the NSF under grant ECS 9216531, by EPRI under
contract 8030-10, and by the ARO.
References

Bertsekas, D. P. (1995) Dynamic Programming and Optimal Control. Athena Scientific, Belmont, MA.
Boyan, J. A. & Moore, A. W. (1995) Generalization in Reinforcement Learning:
Safely Approximating the Value Function. In J. D. Cowan, G. Tesauro, and D.
Touretzky, editors, Advances in Neural Information Processing Systems 7. Morgan
Kaufmann.
Gordon, G. J. (1995) Stable Function Approximation in Dynamic Programming.
Technical Report: CMU-CS-95-103, Carnegie Mellon University.
Polyak, B. T. & Tsypkin, Y. Z., (1972) Pseudogradient Adaptation and Training
Algorithms. A vtomatika i Telemekhanika, 3:45-68.
Sutton, R. S. (1988) Learning to Predict by the Method of Temporal Differences.
Machine Learning, 3:9-44.
Tesauro, G. (1992) Practical Issues in Temporal Difference Learning.
Learning, 8:257-277.

Machine

Tsitsiklis, J. & Van Roy, B. (1994) Feature-Based Methods for Large Scale Dynamic
Programming. Technical Report: LIDS-P-2277, Laboratory for Information and
Decision Systems, Massachusetts Institute of Technology. Also to appear in Machine
Learning.

"
1039,1995,Context-Dependent Classes in a Hybrid Recurrent Network-HMM Speech Recognition System,,1039-context-dependent-classes-in-a-hybrid-recurrent-network-hmm-speech-recognition-system.pdf,Abstract Missing,"Context-Dependent Classes in a Hybrid
Recurrent Network-HMM Speech
Recognition System

Dan Kershaw
Tony Robinson
Mike Hochberg ?
Cambridge University Engineering Department,
Trumpington Street, Cambridge CB2 1PZ, England.
Tel: [+44]1223332800, Fax: [+44]1223332662.
Email: djk.ajr@eng.cam.ac.uk

Abstract
A method for incorporating context-dependent phone classes in
a connectionist-HMM hybrid speech recognition system is introduced . A modular approach is adopted, where single-layer networks
discriminate between different context classes given the phone class
and the acoustic data. The context networks are combined with a
context-independent (CI) network to generate context-dependent
(CD) phone probability estimates. Experiments show an average
reduction in word error rate of 16% and 13% from the CI system
on ARPA 5,000 word and SQALE 20,000 word tasks respectively.
Due to improved modelling, the decoding speed of the CD system
is more than twice as fast as the CI system.

INTRODUCTION
The ABBOT hybrid connectionist-HMM system performed competitively with many
conventional hidden Markov model (HMM) systems in the 1994 ARPA evaluations
of speech recognition systems (Hochberg, Cook, Renals, Robinson & Schechtman
1995). This hybrid framework is attractive because it is compact, having far fewer
parameters than conventional HMM systems, whilst also providing the discriminative powers of a connectionist architecture.
It is well established that particular phones vary acoustically when they occur in
different phonetic contexts. For example a vowel may become nasalized when following a nasal sound. The short-term contextual influence of co-articulation is
?Mike Hochberg is now at Nuance Communications, 333 Ravenswood Avenue, Building
110, Menlo Park, CA 94025, USA. Tel: [+1] 415 6148260.

Context-dependent Classes in a Speech Recognition System

751

handled in HMMs by creating a model for all sufficiently differing phonetic contexts with enough acoustic evidence. This modelling of phones in their particular
phonetic contexts produces sharper probability density functions . This approach
vastly improves HMM recognition accuracy over equivalent context-independent
systems (Lee 1989). Although the recurrent neural network (RNN) model acoustic
context internally (within the state vector) , it does not model phonetic context.
This paper presents an approach to improving the ABBOT system through phonetic
context-dependent modelling.
In Cohen, Franco, Morgan , Rumelhart & Abrash (1992) separate sets of contextdependent output layers are used to model context effects in different states ofHMM
phone models. A set of networks discriminate between phones in 8 different broadclass left and right contexts. Training time is reduced by initialising from a CI multilayer perceptron (MLP) and only changing the hidden-to-output weights during
context-dependent training. This system performs well on the DARPA Resource
Management Task. The work presented in Zhoa, Schwartz , Sroka & Makhoul (1995)
followed along similar work to Cohen et al. (1992) . A context-dependent mixture
of experts (ME) system (Jordan & Jacobs 1994) based on the structure of the
context-independent ME was built. For each state, the whole training data was
divided into 46 parts according to its left or right context. Then, a separate ME
model was built for each context.
Another approach to phonetic context-dependent modelling with MLPs was proposed by Bourlard & Morgan (1993) . It was based on factoring the conditional
probability of a phone-in-context given the data in terms of the phone given the
data , and its context given the data and the phone. The approach taken in this
paper is a mixture of the above work. However, this work augments a recurrent network (rather than an MLP) and concentrates on building a more compact system,
which is more suited to our requirements. As a result, the context training scheme is
fast and is implemented on a workstation (rather than a parallel processing machine
as is used for training the RNN) .

OVERVIEW OF THE ABBOT HYBRID SYSTEM
The basic framework of the ABBOT system is similar to the one described in Bourlard
& Morgan (1994) except that a recurrent network is used as the acoustic model
for the within the HMM framework. A more detailed description of the recurrent
network for phone probability estimation is given in Robinson (1994). At each 16ms
time frame , the acoustic vector u(t) is mapped to an output vector y(t), which
represents an estimate of the posterior probability of each of the phone classes

Yi(t) ~ Pr(qi(t)lui H ) ,
(1)
where qi(t) is phone class i at time t , and ul = {u(l) , .. . , u(t)} is the input from
time 1 to t . Left (past) acoustic context is modelled internally by a 256 dimensional
state vector x(t) , which can be envisaged as ""storing"" the information that has
been presented at the input. Right (future) acoustic context is given by delaying
the posterior probability estimation until four frames of input have been seen by the
network . The network is trained using a modified version of error back-propagation
through time (Robinson 1994) .
Decoding with the hybrid connectionist-HMM approach is equivalent to conventional HMM decoding, with the difference being that the RNN models the state
observations. Like typical HMM systems, the recognition process is expressed as
finding the maximum a posteriori state sequence for the utterance . The decoding
criterion specified above requires the computation of the likelihood of the acoustic

752

D. KERSHAW, T. ROBINSON, M. HOCHBERG

data given a phone (state) sequence,

( (t)1 .(t)) = Pr(qi(t)lu(t))p(u(t))
q,
Pr(qi)'

(2)

p u

where p(u(t)) is the same for all phones, and hence drops out of the decoding
process. Hence, the network outputs are mapped to scaled likelihoods by,

Yi(t)
p(U(t)lqi(t)) ::: Pr(qd '

(3)

where the priors Pr(qi) are estimated from the training data. Decoding uses the
NOWAY decoder (Renals & Hochberg 1995) to compute the utterance model that is
most likely to have generated the observed speech signal.

CONTEXT-DEPENDENT PROBABILITY ESTIMATION
The approach taken by this work is to augment the CI RNN, in a similar vein
to Bourlard & Morgan (1993). The context-dependent likelihood, p(UtIC t , Qd,
can be factored as,
p

(u

t

IC Q) = Pr(Ct!Ut, Qt)p(Ut/Qt)
t, t
Pr(Ct!Qd'

(4)

where C is a set of context classes and Q is a set of context-independent phones or
monophones. Substituting for the context independent probability density function,
p(U t IQt), using (2), this becomes
p

(u

t

IC Q) = Pr(C t IUt, Qd Pr(Qt!U t ) (U)
t, t
Pr(CtIQt) Pr(Qt)
Pt?

(5)

The term p(U t} is constant for all frames, so this drops out of the decoding process
and is ignored for all further purposes. This format is extremely appealing since
Pr(C t IQt) and Pr(Qt) are estimated from the training data and the CI RNN estimates Pr(QtIUt). All that is then needed is an estimate of Pr(CtIU t , Qt). The
approach taken in this paper uses a set of context experts or modules for each monophone class to augment the existing CI RNN.

TRAINING ON THE STATE VECTOR
An estimate of Pr(Ct!U t , Qt) can be obtained by training a recurrent network to
discriminate between contexts Cj(t) for phone class qi(t), such that

(6)
where Yjli (t) is an estimate of the posterior probability of context class j given
phone class i. However, training recurrent neural networks in this format would
be expensive and difficult. For a recurrent format, the network must contain no
discontinuities in the frame-by-frame acoustic input vectors. This implies all recurrent networks for all the phone classes i must be ""shown"" all the data. Instead, the
assumption is made that since the state vector x = f(u), then

x(t

+ 4)

is a good representation for

uiH .

Hence, a single-layer perceptron is trained on the state vectors corresponding to
each monophone, qi, to classify the different phonetic context classes. Finally,

Context-dependent Classes in a Speech Recognition System

753

the likelihood estimates for the phonetic context class j for phone class i used
in decoding are given by,
Pr(qi(t)lui+4) Pr(cj (t)lx(t

+ 4) , qi(t))

Pr(cj(t)lqi(t)) Pr(qi(t))
Yi (t)Yjli (t)
Pr( Cj Iqi) Pr( qd .

(7)

Embedded training is used to estimate the parameters of the CD networks and
the training data is aligned using a Viterbi segmentation. Each context network
is trained on a non-overlapping subset of the state vectors generated from all the
Viterbi aligned training data. The context networks were trained using the RProp
training procedure (Robinson 1994).

=>
=> ~
=> -i
oo

:I

I

~

i:I

C.
CD

:I

""'tJ

CD

~.

o
""""I

""'tJ

01ime 0
O~elayO

'---------fO ~

=>

0 11 - - - - '

a
C""
D)

g:

'~
yj1i(t)
~

_______ oJ'

,,

,',.-.._,1

Figure 1: The Phonetic Context-Dependent RNN Modular System.
The frame-by-frame phonetic context posterior probabilities are required as input to
the NOWAY decoder, i.e. all the outputs from the context modules on the right hand
side of Figure 1. These posterior probabilities are calculated from the numerator
of (7). The CI RNN stage operates in its normal fashion, generating frame-by-frame
monophone posterior probabilities. At the same time the CD modules take the state
vector generated by the RNN as input, in order to classify into a context class. The

754

D. KERSHAW, T. ROBINSON, M. HOCHBERG

RNN posterior probability outputs are multiplied by the module outputs to form
context-dependent posterior probability estimates.

RELATIONSHIP WITH MIXTURE OF EXPERTS
This architecture has similarities with mixture of experts (Jordan & Jacobs 1994).
During training, rather than making a ""soft"" split of the data as in the mixture of
experts case, the Viterbi segmentation selects one expert at every exemplar. This
means only one expert is responsible for each example in the data. This assumes that
the Viterbi segmentation is a good approximation to tjle segmentation/selection
process. Hence, each expert is trained on a small subset of the training data,
avoiding the computationally expensive requirement for each expert to ""see"" all
the data. During decoding, the RNN is treated as a gating network, smoothing the
predictions of the experts, in an analogous manner to a standard mixture of experts
gating network . For further description of the system see Kershaw, Hochberg &
Robinson (1995) .

CLUSTERING CONTEXT CLASSES
One of the problems faced by having a context-dependent system is to decide which
context classes are to be included in the CD system. A method for overcoming
this problem is a decision-tree based approach to cluster the context classes. This
guarantees a full coverage of all phones in any context with the context classes
being chosen using the acoustic evidence available. The tree clustering framework
also allows for the building of a small number of context-dependent phones, keeping
the new context-dependent connectionist system architecture compact. The tree
building algorithm was based on Young, Odell & Woodland (1994), and further
details can be found in Kershaw et al. (1995). Once the trees were built, they were
used to relabel the training data and the pronunciation lexicon.

EVALUATION OF THE CONTEXT SYSTEM
The context-independent networks were trained on the ARPA Wall Street Journal S184 Corpus. The phonetic context-dependent classes were clustered on the
acoustic data according to the decision tree algorithm. Running the data through a
recurrent network in a feed-forward fashion to obtain three million frames with 256
dimensional state vectors took approximately 8 hours on an HP735 workstation.
Training all the context-dependent networks on all the training data takes between
4- 6 hours (in total) on an HP735 workstation. The context-dependent modules
were cross-validated on a development set at the word level.
Results for two context-dependent systems, compared with the context-independent
baseline are shown in Table 1, where the 1993 spoke 5 test is used for cross-validation
and development purposes.
The context-dependent systems were also applied to larger tasks such as the recent
1995 SQALE (a European multi-language speech recognition evaluation) 20,000
word development and evaluation sets. The American English context-dependent
system (CD527) was extended to include a set of modules trained backwards in
time (which were log-merged with the forward context), to augment a four way logmerged context-independent system (Hochberg, Cook, Renals & Robinson 1994).

755

Context-dependent Classes in a Speech Recognition System

Table 1: Comparison Of The CI System With The CD205 And CD527 Systems,
For 5000 Word , Bigram Language Model Tasks.
1993
Test Sets
Spoke 5
Spoke 6
Eval.

CI System
WER
16.0
14.6
15.7

CD205 System
WER I Red!!. WER
14.0
12.7
12.2
16.3
14.3
8.4

CD527 System
WER I Red!!. WER
13.6
14.9
11.7
19.8
13.7
12.6

Table 2: Comparison Of The Merged CI Systems With The CD527US And
CD465UK Systems, For 20 ,000 Word Tasks. All Tests Use A Trigram Language
Model. The CD527US And CD465UK Evaluation Results Have Been Officially
Adjudicated .
1995 Test Sets

US English dev _test
US English evLtest
UK English dev _test
UK English evLtest

CI System
WER
12.8
14.5
15.6
16.4

CD System
WER
11.3
12.9 T
12.7
13.8 T

Red!!.
WER
12.2
9.8
18.9
15.7

Table 3: Comparison Of Average Utterance Decode Speed Of The CI Systems With
The CD527US And CD465UK Systems On An HP735, For 20,000 Word Tasks. All
Tests Use A Trigram Language Model, And The Same Pruning Levels.
Tests
American English
British English

CI
Utterance Av .
Decode Speed (s)
67
131

CD
Utterance Av .
Decode Speed (s)
31
48

Speedup
2.16
2.73

Table 4: The Number Of Parameters Used For The CI Systems As Compared With
The CD527US And CD465UK Systems.
System
American English
British English

# CI
Parameters
341,000
331 ,000

#CD
Parameters
612,000
570,000

'fo Increase In
Parameters
79.0
72.2

A similar system was built for British English (CD465). Table 2 shows the improvement gained by using context models. The daggers indicate the official entries for
the 1995 SQALE evaluation. These figures represent the lowest reported word error
rate for both the US and UK English tasks.
As a result of improved phonetic modelling and class discrimination the search
space was reduced. This meant that decoding speed was over twice as fast as the
context-dependent system, Table 3, even though there were roughly ten times as
many context-dependent phones compared to the monophones.
The increase in the number of parameters due to the introduction of the context
models for the SQALE evaluation system are shown in Table 4. Although this
seems a large increase in the number of system parameters, it is still an order of
magnitude less than any equivalent HMM system built for this task.

756

D. KERSHAW, T. ROBINSON, M. HOCHBERG

CONCLUSIONS
This paper has discussed a successful way of integrating phonetic context-dependent
classes into the current ABBOT hybrid system. The architecture followed a modular
approach which could be used to augment any current RNN-HMM hybrid system.
Fast training of the context-dependent modules was achieved. Training on all of the
SI84 corpus took between 4 and 6 hours. Utterance decoding was performed using
the standard NOWAY decoder. The word error was significantly reduced, whilst the
decoding speed of the context system was over twice as fast as the baseline system
(for 20,000 word tasks).

References
Bourlard, H. & Morgan, N. (1993), 'Continuous Speech Recognition by Connectionist Statistical Methods', IEEE Transactions on Neural Networks 4(6), 893- 909.
Bourlard, H. & Morgan, N. (1994), Connectionist Speech Recognition: A Hybrid
Approach, Kluwer Acedemic Publishers.
Cohen, M., Franco, H., Morgan, N., Rumelhart, D. & Abrash, V. (1992), ContextDependent Multiple Distribution Phonetic Modeling with MLPs, in 'NIPS 5'.
Hochberg, M., Cook, G., Renals, S. & Robinson, A. (1994), Connectionist Model
Combination for Large Vocabulary Speech Recognition, in 'Neural Networks
for Signal Processing', Vol. IV, pp. 269-278.
Hochberg, M., Cook, G., Renals, S., Robinson, A. & Schechtman, R. (1995), The
1994 ABBOT Hybrid Connectionist-HMM Large-Vocabulary Recognition System, in 'Spoken Language Systems Technology Workshop', ARPA, pp. 170-6.
Jordan, M. & Jacobs, R. (1994), 'Hierarchical Mixtures of Experts and the EM
Algorithm', Neural Computation 6, 181-214.
Kershaw, D., Hochberg, M. & Robinson, A. (1995), Incorporating ContextDependent Classes in a Hybrid Recurrent Network-HMM Speech Recognition
System, F-INFENG TR217, Cambridge University Engineering Department.
Lee, K.-F. (1989), Automatic Speech Recognition; The Development of the SPHINX
System, Kluwer Acedemic Publishers.
Renals, S. & Hochberg, M. (1995), Efficient Search Using Posterior Phone Probability Estimates, in 'ICASSP', Vol. 1, pp. 596-9.
Robinson, A. (1994), 'An Application of Recurrent Nets to Phone Probability Estimation.', IEEE Transactions on Neural Networks 5(2),298-305.
Young, S., Odell, J. & Woodland, P. (1994), 'Tree-Based State Tying for High Accuracy Acoustic Modelling', Spoken Language Systems Technology Workshop.
Zhoa, Y., Schwartz, R., Sroka, J . & Makhoul, J. (1995), Hierarchical Mixtures of
Experts Methodology Applied to Continuous Speech Recognition, in 'NIPS 7'.

"
104,1988,Digital Realisation of Self-Organising Maps,,104-digital-realisation-of-self-organising-maps.pdf,Abstract Missing,"728

DIGITAL REALISATION OF SELF-ORGANISING MAPS
Nigel M. Allinson

M~rtin J. Johnson
Department of Electronics
University of York
York
Y015DD
England

Kevin J. Moon

ABSTRACT
A digital realisation of two-dimensional self-organising feature
maps is presented.
The method is based on subspace
Weight vector
classification using an n-tuple technique.
approximation and orthogonal projections to produce a winnertakes-all network are also discussed. Over one million effective
binary weights can be applied in 25ms using a conventional
microcomputer. Details of a number of image recognition tasks,
including character recognition and object centring, are
described.

INTRODUCTION
Background

The overall aim of our work is to develop fast and flexible systems for image
recognition, usually for commercial inspection tasks. There is an urgent need for
automatic learning systems in such applications, since at present most systems
employ heuristic classification techniques. This approach requires an extensive
development effort for each new application, which exaggerates implementation
costs; and for many tasks, there are no clearly defined features which can be
employed for classification. Enquiring of a human expert will often only produce
""good"" and ""bad"" examples of each class and not the underlying strategies which
he may employ. Our approach is to model in a quite abstract way the perceptual
networks found in the mammalian brain for vision. A back-propagation network
could be employed to generalise about the input pattern space, and it would find
some useful representations. However, there are many difficulties with this
approach, since the network structure assumes nothing about the input space and
it can be difficult to bound complicated feature clusters using hyperplanes. The
mammalian brain is a layered structure, and so another model may be proposed
which involves the application of many two-dimensional feature maps. Each map
takes information from the output of the preceding one and performs some type of
clustering analysis in order to reduce the dimensionality of the input information.
For successful recognition, similar patterns must be topologically close so that

Digi tal Realisation of Self-Organising Maps

novel patterns are in the same general area of the feature map as the class they
are most like. There is therefore a need for both global and local ordering
processes within the feature map. The process of global ordering in a topological
map is termed, by Kohonen (1984), as self-organisation.
It Is important to realize that all feedforward networks perform only one function,
namely the labelling of areas in a pattern space. This paper concentrates on a
technique for realising large, fast, two-dimensional feature maps using a purely
digital implementation.

Figure 1. Unbounded Feature Map of Local Edges
Self Organisation
Global ordering needs to adapt the entire neural map, but local ordering needs
only local information. Once the optimum global organisation has been found,
then only more localised ordering can improve the topological organisation. This
process is the basis of the Kohonen clustering algorithm, where the specified area

729

730

Johnson, Allinson and Moon

of adaption decreases with time to give an increasing local ordering. It has been
shown that this approach gives optimal ordering at global and local levels (Oja,
1983). It may be considered as a dimensionality reduction algorithm, and can be
used as a vector quantiser.
Although Kohonen's self-organising feature maps have been successfully applied
to speech recognition (Kohonen, 1988; Tattersall et aI., 1988), there has been little
Investigation in their application for image recognition. Such feature maps can be
used to extract various image primitives, such as textures, localised edges and
terminations, at various scales of representations (Johnson and Allinson, 1988).
As a simple example, a test image of concentric circles is employed to construct a
small feature map of localised edges (Figure 1). The distance measure used is the
normalised dot product since in general magnitude information is unimportant.
Under these conditions, each neuron output can be considered a similarity
measure of the directions between the input pattern and the synaptic weight
vector. This map shows that similar edges have been grouped together and that
inverses are as far from each other as possible.

DIGITAL IMPLEMENTATION
Sub-Space Classification
Although a conventional serial computer is normally thought of as only performing
one operation at a time, there is a task which it can successfully perform involving
parallel computation. The action of addressing memory can be thought of as a
hi&JhlY parallel process, since it involves the comparison of a word, W, with a set ~
2 others where N is the number of bits in W. It is, in effect, performing 2
parallel computations - each being a single match. This can be exploited to speed
up the simulation of a network by using a conversion between conventional
pattern space labelling and binary addressing.
Figure 2 shows how the labelling of two-dimensional pattern space is equivalent to
the partitioning of the same space by the decision regions of a multiple layer
perceptron. If each quantised part of the space is labelled with a number for each
class then all that is necessary is for the pattern to be used as an address to give
the stored label (i.e. the response) for each class. These labels may form a cluster
of any shape and so multiple layers are not required to combine regions.
The apparent flaw in the above suggestion is that for anything other than a trivial
problem, the labelling of every part of pattern space is impractical. For example a
32 x 32 input vector would require a memory of 2 1024 words per unit! What is
needed is a coding system which uses some basic assumptions about patterns in
order to reduce the memory requirements. One assumption which can be made
is that patterns will cI uster together into various classes. As early as 1959, a
method known as the n-tuple technique was used for pattern recognition (Bledsoe
and Browning, 1959). This technique takes a number of subspaces of the pattern

Digital Realisation of Self-Organising Maps

PERCEPTRON
c1/c2

x2

x1

~=C1

=c2

I' I,' ,I,' ~I,J::-I' 1.-1-1?' 1' 1? I?'
I.;:I~'I~I-::I ' I,
I? ~~t:'

l

I? ~ ~~

~~~

....t It ..
,- II ?

010 I. ' .' I,

? ? ~'I
f

.

.ltl

~.

I- I.

??

,

I-

I_

.?
~~

I .? ~
I""~ ~~~ ~~~~
~
i' I, I.'

I? ?
? I?

x2

~

""I.'

~

~

~~

'.
II ? .'

I~. ,
1:\ ,

~~

1--1' ~~
, 1,' 1' ~
I'
?
I,' , I""
1411. Itili ?
1.'1,
i"" ""

1'1 ?

I~r.-r~

,

'

~

~ .'
~

.

LABELING
The labeling of a quantized
subspace is equivalent to
the partitioning of pattern
space by the multi-layer
perceptron.

I,'
I?

i? - It It 11111.
?1? 1?1?1-

? = Class 1 0

= Class 2

Figure 2. Comparison of Perceptron and Sub-Space Classification
space and uses the sum of the resultant labels as the overall response. This gives
a set of much smaller memories and inherent in the coding method is that similar
patterns will have identical labels.
For example, assume a 16 bit pattern - 0101101001010100. Taking a four-bit
sample from this, say bits 0-3, giving 0100. This can be used to address a 16 word
memory to produce a single bit. If this bit is set to 1, then it is in effect labelling all
patterns with 0100 as their first four bits; that is 4096 patterns of the form
xxxxxxxxxxxx0100. Taking a second sample, namely bits 4-7 (0101). This labels
xxxxxxxx0101xxxx patterns, but when added to the first sample there will be 256
patterns labelled twice (namely, xxxxxxxx01010100) and 7936 (Le. 8192-256)
labelled once.
The third four-bit sample produces 16 patterns (namely,

731

732

Johnson, Allinson and Moon

xxx(101001010100) labelled three times. The fourth sample produces only one
pattem 0101101001010100, which has been labelled four times. If an input pattern
is applied which differs from this by one bit, then this will now be labelled three
times by the samples; if it differs by two bits, it will either be labelled two or three
times depending on whether the changes were in the same four-bit sample or not.
Thus a distance measure is implicit in the coding method and reflects the
assumed clustering of patterns. Applying this approach to the earlier problem of a
32 x 32 binary input vector and taking 128 eight-bit samples results in a distance
measure between 0 and 128 and uses 32K bits of memory per unit.
Weight Vector Approximation
It is possible to make an estimate of the approximate weight vector for a particular
sample from the bit table. For simplicity, consider a binary image from which t
samples are taken to form a word, w, where
t-1
w = xo + 2x 1 + .... + 2 ~-1
This word can be used to address a vector W. Every bit in W[b] which is 1 either
increases the weight vector probability where the respective bit in the address is
set, or decreases if it is clear. Hence, if BIT [w,i] is the ith bit of wand A[i] is the
contents of the memory {O, 1} then,

2 t -1

E

W[b] =
i= 0

A[i] (2 BIT(b,i) -1)

This represents an approximate measure of the weight element. Table 1
demonstrates the principle for a four-bit sample memory. Given randomly
distributed inputs this binary vector is equivalent to the weight vector [2, 4, 0, -2].
If there is a large number of set bits in the memory for a particular unit then that
will always give a high response - that is, it will become saturated. However, if
there are too few bits set, this unit will not rfiSpond strongly to a general set of
patterns. The number of bits must, therefore, be fixed at the start of training,
distributed randomly within the memory and only redistribution of these bits
allowed. Set bits could be taken from any other sample, but some samples will be
more important than others. The proportion of 1's in an image should not be used
as a measure, otherwise large uniform regions will be more significant than the
pattern detail. This is a form of magnitude independent operation similar to the
use of the normalised dot product applied in the analogue approach and so bits
may only be moved from addresses with the same number of set bits as the
current address.

Digital Realisation of Self-Organising Maps

TABLE 1. Weight Vector Approximation
Address
X3 x2 x,

Xo

A

Weight change
W3 W2 W, W0

Address
x3 x2 x,

Xo

A

Weight change
W3 W2 W, Wo

+

-

0

0

0

0

0

1

0

0

0

1

0

0

0

1

0

1

0

0

1

0

0

0

1

0

0

1

0

1

0

0

a

0

1

1

1

+ +

1

0

1

1

0

0

1

0

0

1

-

1

1

0

0

1

+

+

-

0

1

0

1

0

1

1

0

1

1

+ +

-

0

1

1

0

1

1

1

1

0

1

+

+ + -

0

1

1

1

0

1

1

1

1

1

+

+

+

2

4

0-2

+

+

+

-

Equivalent weight vector

+

+

Orthogonal Projections
In order to speed up the simulation further, instead of representing each unit by a
single bit in memory, each unit can be represented by a combination of bits.
Hence many calculations can be effectively computed in parallel. The number of
units which require a 1 for a particular sample will always be relatively small, and
hence these can be coded. The coding method employed is to split the binary
word, W, into x and y fields. These projection fields address a two dimensional
map and so provide a fast technique of approximating the true content of the
memory. The x bits are summed separately to the y bits, and together they give a
good estimate of the unit co-ordinates with the most bits set in x and in y. This
map becomes, in effect, a winner-takes-all network. The reducing neighbourhood
of adaption employed in the Kohonen algorithm can also be readily incorporated
by applying an overall mask to this map during the training phase.
Though only this output map is required during normal application of the system
to image recognition tasks, it is possible to reconstruct the distribution of the twodimensional weight vectors. Figure 3, using the technique illustrated in Table 1,
shows this weight vector map for the concentric circle test image applied

733

734

Johnson, Allinson and Moon

Figure 3. Reconstructed Feature Map of Local Edges
previously in the conventional analogue approach. This is a small digitised map
containing 32 x 32 elements each with 16 x 16 input units and can be applied,
using a general purpose desktop microcomputer running at 4 mips, in a few
milliseconds.

APPLICATION EXAMPLES
Character Recognition
Though a long term objective remains the development of general purpose
computer vision systems, with many layers of interacting feature maps together
with suitable pre- and post-processing, many commercial tasks require decisions
based on a constricted range of objects - that is their perceptual set is severely
limited. However, ease of training and speed of application are paramount. An
example of such an application involves the recognition of characters.
Figures 4 and 5 show an input pattern of hand-drawn A's and B's. The network,
using the above digital technique, was given no information concerning the input
image and the input window of 32 x 32 pixels was placed randomly on the image.
,The network took less than one minute to adapt and can be applied in 25 ms. This
network is a 32 x 32 feature map of 32 x 32 elements, thus giving over one million
effective weights. The output map forms two distinct clusters, one for A's in the
top right corner of the map (Figure 4), and one for B's in the bottom left corner
(Figure 5). If further characters are introduced in the input image then the output
map will, during the training phase, self-organise to incorporate them.

Digital Realisation of Self-Organising Maps

Figure 4. Trained Network Response for 'A' in Input Window

Figure 5. Trained Network Response for 'B' in Input Window

735

736

Johnson, Allinson and Moon

Corrupted Images
Once the maximum response from the map is known, then the parts of the input
window which caused it can be reconstructed to provide a form of ideal input
pattern. The reconstructed input pattern is shown in the figures beneath the input
image. This reconstruction can be employed to recognise occuluded patterns or
to eliminate noise in subsequent input images.

Figure 6. Trained Network Response for Corrupted 'A' in Input Window.
Reconstructed Input Pattern Shown Below Test Image
Figure 6 shows the response of the network, trained on the input image of Figures
4 and 5, to a corrupted image of A's and B's. It has still managed to recognise the
input character as an A, but the reconstructed version shows that the extra noise
has been eliminated.
Object Centring
The centering of an object within the input window permits the application of
conformant mapping strategies, such as polar exponential grids, to be applied
which yields scale and rotation invariant recognition. The same network as
employed in the previous example was used, but a target position for the
maximum network response was specified and the network was adapted half-way
between this and the actual maximum response location.

Digital Realisation of Self-Organising Maps

Figure 7. Trained Network Response for Off-Centred Character. Input Window is
Low-Pass Filtered as shown.
Figure 7 shows such a network. When the response is in the centre of the output
map then an input object (character) is centred in the recognition window. In the
example shown, there is an off-centred response of the trained network for an offcentred character. This deviation is used to change the position of the input
window. Once centering has been achieved, object recognition can occur.

CONCLUSIONS
The application of unsupervised feature maps for image recognition has been
demonstrated. The digital realisation technique permits the application of large
maps. which can be applied in real time using conventional microcomputers. The
use of orthogonal projections to give a winner-take-all network reduces memorY
requirements by approximately 3D-fold and gives a computational cost of O(n 1/2),
where n is the number of elements in the map. The general approach can be
applied in any form of feedforward neural network.
Acknowledgements
This work has been supported by the Innovation and Research Priming Fund of
the University of York.

737

738

Johnson, Allinson and Moon

References
W. W. Bledsoe and I. Browning. Pattern Recognition and Reading by Machine.
Proc. East. Joint Compo Conf., 225-232 (1959).
M. J. Johnson and N. M. Allinson. An Advanced Neural Network for Visual Pattern
Recognition. Proc. UKIT 88, Swansea, 296-299 (1988).
T. Kohonen. Self Organization and Associative Memory. Springer-Vertag, Bertin
(1984).
T. Kohonen. The 'Neural' Phonetic Typewriter. Computer21,11-22 (1988).
E. Oja.

Subspace Methods of Pattern Recognition.
Letchworth (1983).

Research Studies Press,

G. D. Tattersall, P. W. Linford and R. Linggard. Neural Arrays for Speech
Recognition. Br. Telecom Techno/. J. Q. 140-163 (1988).

"
1040,1995,Empirical Entropy Manipulation for Real-World Problems,,1040-empirical-entropy-manipulation-for-real-world-problems.pdf,Abstract Missing,"Empirical Entropy Manipulation for
Real-World Problems
Paul Viola: Nicol N. Schraudolph, Terrence J. Sejnowski
Computational Neurobiology Laboratory
The Salk Institute for Biological Studies
10010 North Torrey Pines Road
La Jolla, CA 92037-1099
viola@salk.edu

Abstract
No finite sample is sufficient to determine the density, and therefore
the entropy, of a signal directly. Some assumption about either the
functional form of the density or about its smoothness is necessary.
Both amount to a prior over the space of possible density functions.
By far the most common approach is to assume that the density
has a parametric form.
By contrast we derive a differential learning rule called EMMA
that optimizes entropy by way of kernel density estimation. Entropy and its derivative can then be calculated by sampling from
this density estimate. The resulting parameter update rule is surprisingly simple and efficient.
We will show how EMMA can be used to detect and correct corruption in magnetic resonance images (MRI). This application is
beyond the scope of existing parametric entropy models.

1

Introduction

Information theory is playing an increasing role in unsupervised learning and visual
processing. For example, Linsker has used the concept of information maximization
to produce theories of development in the visual cortex (Linsker, 1988). Becker and
Hinton have used information theory to motivate algorithms for visual processing
(Becker and Hinton, 1992). Bell and Sejnowski have used information maximization
? Author to whom correspondence should be addressed. Current address: M.LT., 545
Technology Square, Cambridge, MA 02139.

P. VIOLA, N. N. SCHRAUDOLPH, T. J. SEJNOWSKI

852

to solve the ""cocktail party"" or signal separation problem (Bell and Sejnowski,
1995). In order to simplify analysis and implementation, each of these techniques
makes specific assumptions about the nature of the signals used, typically that the
signals are drawn from some parametric density. In practice, such assumptions are
very inflexible.
In this paper we will derive a procedure that can effectively estimate and manipulate the entropy of a wide variety of signals using non-parametric densities. Our
technique is distinguished by is simplicity, flexibility and efficiency.
We will begin with a discussion of principal components analysis (PCA) as an example of a simple parametric entropy manipulation technique. After pointing out some
of PCA's limitation, we will then derive a more powerful non-parametric entropy
manipulation procedure. Finally, we will show that the same entropy estimation
procedure can be used to tackle a difficult visual processing problem.
1.1

Parametric Entropy Estimation

Typically parametric entropy estimation is a two step process. We are given a
parametric model for the density of a signal and a sample. First, from the space
of possible density functions the most probable is selected. This often requires a
search through parameter space. Second, the entropy of the most likely density
function is evaluated.
Parametric techniques can work well when the assumed form of the density matches
the actual data. Conversely, when the parametric assumption is violated the resulting algorithms are incorrect. The most common assumption, that the data follow the
Gaussian density, is especially restrictive. An entropy maximization technique that
assumes that data is Gaussian, but operates on data drawn from a non-Gaussian
density, may in fact end up minimizing entropy.
1.2

Example: Principal Components Analysis

There are a number of signal processing and learning problems that can be formulated as entropy maximization problems. One prominent example is principal component analYllill (PCA). Given a random variable X, a vector v can be used to define
a new random variable, Y"" = X . v with variance Var(Y,,) = E[(X . v - E[X . v])2].
The principal component v is the unit vector for which Var(Yv) is maximized.
In practice neither the density of X nor Y"" is known. The projection variance is
computed from a finite sample, A, of points from X,
Var(Y,,) ~ Var(Y,,) == EA[(X . v - EA[X . v])2] ,
(1)
A

where VarA(Y,,) and E A [?] are shorthand for the empirical variance and mean evaluated over A. Oja has derived an elegant on-line rule for learning v when presented
with a sample of X (Oja, 1982).
Under the assumption that X is Gaussian is is easily proven that Yv has maximum
entropy. Moreover, in the absence of noise, Yij, contains maximal information about
X. However, when X is not Gaussian Yij is generally not the most informative
projection.

2

Estimating Entropy with Parzen Densities

We will now derive a general procedure for manipulating and estimating the entropy
of a random variable from a sample. Given a sample of a random variable X, we can

Empirical Entropy Manipulation for Real-world Problems

853

construct another random variable Y = F(X,l1). The entropy, heY), is a function of
v and can be manipulated by changing 11. The following derivation assumes that Y is
a vector random variable. The joint entropy of a two random variables, h(Wl' W2),
can be evaluated by constructing the vector random variable, Y = [Wl' w2 jT and
evaluating heY).
Rather than assume that the density has a parametric form, whose parameters are
selected using maximum likelihood estimation, we will instead use Parzen window
density estimation (Duda and Hart, 1973). In the context of entropy estimation, the
Parzen density estimate has three significant advantages over maximum likelihood
parametric density estimates: (1) it can model the density of any signal provided
the density function is smooth; (2) since the Parzen estimate is computed directly
from the sample, there is no search for parameters; (3) the derivative of the entropy
of the Parzen estimate is simple to compute.
The form of the Parzen estimate constructed from a sample A is

p.(y, A)

= ~A

I: R(y -

YA)

= EA[R(y -

(2)

YA)] ,

YAEA
where the Parzen estimator is constructed with the window function R(?) which
integrates to 1. We will assume that the Parzen window function is a Gaussian
density function. This will simplify some analysis, but it is not necessary. Any
differentiable function could be used. Another good choice is the Cauchy density.
Unfortunately evaluating the entropy integral

hey)

~ -E[log p.(~, A)] =

-

i:

log p.(y, A)dy

is inordinately difficult. This integral can however be approximated as a sample
mean:

(3)
where EB{ ] is the sample mean taken over the sample B. The sample mean
converges toward the true expectation at a rate proportional to 1/ v'N B (N B is
the size of B). To reiterate, two samples can be used to estimate the entropy of a
density: the first is used to estimate the density, the second is used to estimate the
entropyl. We call h? (Y) the EMMA estimate of entropy2.
One way to extremize entropy is to use the derivative of entropy with respect to v.
This may be expressed as

~h(Y) ~ ~h?(Y) =
dl1

dv

__1_ ' "" LYAEA f;gt/J(YB - YA)
N B YBE
L....iB Ly A EA gt/J(YB - YA)

1
= NB

I: I: Wy (YB , YA) dl1d ""21 Dt/J(YB -

(4)

YA),

(5)

YBEB YAEA
_

where WY(Yl' Y2) = L

gt/J(Yl - Y2)
(
) ,
YAEA gt/J Yl - YA

(6)

Dt/J(Y) == yT.,p-ly, and gt/J(Y) is a multi-dimensional Gaussian with covariance .,p.
Wy(Yl' Y2) is an indicator of the degree of match between its arguments, in a ""soft""
lUsing a procedure akin to leave-one-out cross-validation a single sample can be used
for both purposes.
2EMMA is a random but pronounceable subset of the letters in the words ""Empirical
entropy Manipulation and Analysis"".

P. VIOLA, N. N. SCHRAUDOLPH, T. J. SEJNOWSKl

854

sense. It will approach one if Yl is significantly closer to Y2 than any element of A.
To reduce entropy the parameters v are adjusted such that there is a reduction in
the average squared distance between points which Wy indicates are nearby.
2.1

Stochastic Maximization Algorithm

Both the calculation of the EMMA entropy estimate and its derivative involve a
double summation. As a result the cost of evaluation is quadratic in sample size:
O(NANB). While an accurate estimate of empirical entropy could be obtained by
using all of the available data (at great cost), a stochastic estimate of the entropy
can be obtained by using a random subset of the available data (at quadratically
lower cost). This is especially critical in entropy manipulation problems, where the
derivative of entropy is evaluated many hundreds or thousands of times. Without
the quadratic savings that arise from using smaller samples entropy manipulation
would be impossible (see (Viola, 1995) for a discussion of these issues).
2.2

Estimating the Covariance

In addition to the learning rate .A, the covariance matrices of the Parzen window
functions, g,p, are important parameters of EMMA. These parameters may be chosen so that they are optimal in the maximum likelihood sense. For simplicity, we
assume that the covariance matrices are diagonal,.,p
DIAG(O""~,O""~, ... ). Following a derivation almost identical to the one described in Section 2 we can derive an
equation analogous to (4),

=

"" """"
-d
h. (Y) = - 1 ""L...J
L...J WY(YB' YA) ( -1 )
NB

dO""k

b

O""k

YsE YAEa

([y]~
-- O""~

1)

(7)

where [Y]k is the kth component of the vector y. The optimal, or most likely,
.,p minimizes h? (Y). In practice both v and .,p are adjusted simultaneously; for
example, while v is adjusted to maximize h? (YlI ), .,p is adjusted to minimize h? (y,,).

3

Principal Components Analysis and Information

As a demonstration, we can derive a parameter estimation rule akin to principal
components analysis that truly maximizes information. This new EMMA based
component analysis (ECA) manipulates the entropy of the random variable Y"" =
X?v under the constraint that Ivl = 1. For any given value of v the entropy of Y v can
be estimated from two samples of X as: h?(Yv )
-EB[logEA[g,p(xB?v - XA? v)]],
where .,p is the variance of the Parzen window function. Moreover we can estimate
the derivative of entropy:

=

d~ h?(Y = ;

L

lI )

B

B

L Wy(YB, YA) .,p-l(YB - YA)(XB - XA) ,
A

where YA = XA . v and YB = XB . v. The derivative may be decomposed into parts
which can be understood more easily. Ignoring the weighting function Wy.,p-l we
are left with the derivative of some unknown function f(y""):
d
1
dvf(Yv ) = N N L L(YB - YA)(XB - XA)
(8)
B

A

B

A

What then is f(y"")? The derivative of the squared difference between samples is:
d~ (YB - YA)2 = 2(YB - YA)(XB - XA) . So we can see that

f(Y,,) = 2N IN L
B

A

B

L(YB - YA)2
A

Empirical Entropy Manipulation for Real-world Problems

?

3

855

I

.

:

2

ECA-MIN
ECA-MAX
BCM
BINGO
PCA

o
-I
??

-2

t

-3
-4

-2

o

2

4

Figure 1: See text for description.

is one half the expectation of the squared difference between pairs of trials of Yv ?
Recall that PCA searches for the projection, Yv , that has the largest sample variance. Interestingly, f(Yv ) is precisely the sample variance. Without the weighting
term Wll ,p-l, ECA would find exactly the same vector that PCA does: the maximum variance projection vector. However because of Wll , the derivative of ECA
does not act on all points of A and B equally. Pairs of points that are far apart are
forced no further apart. Another way of interpreting ECA is as a type of robust
variance maximization. Points that might best be interpreted as outliers, because
they are very far from the body of other points, playa very small role in the minimization. This robust nature stands in contrast to PCA which is very sensitive to
outliers.
For densities that are Gaussian, the maximum entropy projection is the first principal component. In simulations ECA effectively finds the same projection as PCA,
and it does so with speeds that are comparable to Oja's rule. ECA can be used both
to find the entropy maximizing (ECA-MAX) and minimizing (ECA-MIN) axes. For
more complex densities the PCA axis is very different from the entropy maximizing
axis. To provide some intuition regarding the behavior of ECA we have run ECAMAX, ECA-MIN, Oja's rule, and two related procedures, BCM and BINGO, on
the same density. BCM is a learning rule that was originally proposed to explain
development of receptive fields patterns in visual cortex (Bienenstock, Cooper and
Munro, 1982). More recently it has been argued that the rule finds projections
that are far from Gaussian (Intrator and Cooper, 1992). Under a limited set of
conditions this is equivalent to finding the minimum entropy projection. BINGO
was proposed to find axes along which there is a bimodal distribution (Schraudolph
and Sejnowski, 1993).
Figure 1 displays a 400 point sample and the projection axes discussed above. The
density is a mixture of two clusters. Each cluster has high kurtosis in the horizontal
direction. The oblique axis projects the data so that it is most uniform and hence
has the highest entropy; ECA-MAX finds this axis. Along the vertical axis the
data is clustered and has low entropy; ECA-MIN finds this axis. The vertical axis
also has the highest variance. Contrary to published accounts, the first principal
component can in fact correspond to the minimum entropy projection. BCM, while
it may find minimum entropy projections for some densities, is attracted to the
kurtosis along the horizontal axis. For this distribution BCM neither minimizes nor
maximizes entropy. Finally, BINGO successfully discovers that the vertical axis is
very bimodal.

856

P. VIOLA, N. N. SCHRAUOOLPH, T. J. SEJNOWSKI

\ Corrupted-

1200

:. Corrected .?
....:
'

1000

800
600

400

200

~.1

0

0.1 0.2 0.3 0.4

'.

0.7 0.8 0.9

Figure 2: At left: A slice from an MRI scan of a head. Center: The scan after
correction. Right: The density of pixel values in the MRI scan before and after
correction.

4

Applications

EMMA has proven useful in a number of applications. In object recognition EMMA
has been used align 3D shape models with video images (Viola and Wells III, 1995).
In the area of medical imaging EMMA has been used to register data that arises
from differing medical modalities such as magnetic resonance images, computed
tomography images, and positron emission tomography (Wells, Viola and Kikinis,
1995).
4.1

MRI Processing

In addition, EMMA can be used to process magnetic resonance images (MRI).
An MRI is a 2 or 3 dimensional image that records the density of tissues inside the
body. In the head, as in other parts of the body, there are a number of distinct tissue
classes including: bone, water, white matter, grey matter, and fat. ~n principle the
density of pixel values in an MRI should be clustered, with one cluster for each
tissue class. In reality MRI signals are corrupted by a bias field, a multiplicative
offset that varies slowly in space. The bias field results from unavoidable variations
in magnetic field (see (Wells III et al., 1994) for an overview of this problem).
Because the densities of each tissue type cluster together tightly, an uncorrupted
MRI should have relatively low entropy. Corruption from the bias field perturbs
the MRI image, increasing the values of some pixels and decreasing others. The
bias field acts like noise, adding entropy to the pixel density. We use EMMA to find
a low-frequency correction field that when applied to the image, makes the pixel
density have a lower entropy. The resulting corrected image will have a tighter
clustering than the original density.
Call the uncorrupted scan s(z); it is a function of a spatial random variable z. The
corrupted scan, c( x) s( z) + b( z) is a sum of the true scan and the bias field. There
are physical reasons to believe b( x) is a low order polynomial in the components of
z. EMMA is used to minimize the entropy of the corrected signal, h( c( x) - b( z, v?,
where b( z, v), a third order polynomial with coefficients v, is an estimate for the
bias corruption.

=

Figure 2 shows an MRI scan and a histogram of pixel intensity before and after
correction. The difference between the two scans is quite subtle: the uncorrected
scan is brighter at top right and dimmer at bottom left. This non-homogeneity

Empirical Entropy Manipulation for Real-world Problems

857

makes constructing automatic tissue classifiers difficult. In the histogram of the
original scan white and grey matter tissue classes are confounded into a single peak
ranging from about 0.4 to 0.6. The histogram of the corrected scan shows much
better separation between these two classes. For images like this the correction field
takes between 20 and 200 seconds to compute on a Sparc 10.

5

Conclusion

We have demonstrated a novel entropy manipulation technique working on problems
of significant complexity and practical importance. Because it is based on nonparametric density estimation it is quite flexible, requiring no strong assumptions
about the nature of signals. The technique is widely applicable to problems in
signal processing, vision and unsupervised learning. The resulting algorithms are
computationally efficient.
Acknowledgements
This research was support by the Howard Hughes Medical Institute.

References
Becker, S. and Hinton, G. E. (1992). A self-organizing neural network that discovers
surfaces in random-dot stereograms. Nature, 355:161-163.
Bell, A. J. and Sejnowski, T. J. (1995). An information-maximisation approach to blind
separation. In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, Advance8 in
Neural Information Proce88ing, volume 7, Denver 1994. MIT Press, Cambridge.
Bienenstock, E., Cooper, L., and Munro, P. (1982). Theory for the development of neuron
selectivity: Orientation specificity and binocular interaction in visual cortex. Journal
of Neur08cience, 2.
Duda, R. and Hart, P. (1973). Pattern Cla88ification and Scene AnalY8i8. Wiley, New
York.
Intrator, N. and Cooper, L. N. (1992). Objective function formulation of the bcm theory of visual cortical plasticity: Statistical connections, stability conditions. Neural
Network., 5:3-17.
Linsker, R. (1988). Self-organization in a perceptual network. IEEE Computer, pages
105-117.
Oja, E. (1982). A simplified neuron model as a principal component analyzer. Journal of
Mathematical Biology, 15:267-273.
Schraudolph, N. N. and Sejnowski, T. J. (1993). Unsupervised discrimination of clustered
data via optimization of binary information gain. In Hanson, S. J., Cowan, J. D.,
and Giles, C. L., editors, Advance. in Neural Information Proce88ing, volume 5, pages
499-506, Denver 1992. Morgan Kaufmann, San Mateo.
Viola, P. A. (1995). Alignment by Ma:cimization of Mutual Information. PhD thesis,
Massachusetts Institute of Technology. MIT AI Laboratory TR 1548.
Viola, P. A. and Wells III, W. M. (1995). Alignment by maximization of mutual information. In Fifth Inti. Conf. on Computer Vi8ion, pages 16-23, Cambridge, MA.
IEEE.
Wells, W., Viola, P., and Kikinis, R. (1995). Multi-modal volume registration by maximization of mutual information. In Proceeding. of the Second International Sympo8ium on Medical Robotic. and Computer A88i8ted Surgery, pages 55 - 62. Wiley.
Wells III, W., Grimson, W., Kikinis, R., and Jolesz, F. (1994). Statistical Gain Correction
and Segmentation of MRI Data. In Proceeding. of the Computer Society Conference
on Computer Vi.ion and Pattern Recognition, Seattle, Wash. IEEE, Submitted.

"
1041,1995,The Geometry of Eye Rotations and Listing's Law,,1041-the-geometry-of-eye-rotations-and-listings-law.pdf,Abstract Missing,"The Geometry of Eye Rotations
and Listing's Law

Amir A. Handzel*
Tamar Flash t
Department of Applied Mathematics and Computer Science
Weizmann Institute of Science
Rehovot, 76100 Israel

Abstract
We analyse the geometry of eye rotations, and in particular
saccades, using basic Lie group theory and differential geometry. Various parameterizations of rotations are related through
a unifying mathematical treatment, and transformations between
co-ordinate systems are computed using the Campbell-BakerHausdorff formula. Next, we describe Listing's law by means of
the Lie algebra so(3). This enables us to demonstrate a direct
connection to Donders' law, by showing that eye orientations are
restricted to the quotient space 80(3)/80(2). The latter is equivalent to the sphere S2, which is exactly the space of gaze directions.
Our analysis provides a mathematical framework for studying the
oculomotor system and could also be extended to investigate the
geometry of mUlti-joint arm movements .

1

INTRODUCTION

1.1

SACCADES AND LISTING'S LAW

Saccades are fast eye movements, bringing objects of interest into the center of
the visual field. It is known that eye positions are restricted to a subset of those
which are anatomically possible, both during saccades and fixation (Tweed & Vilis ,
1990). According to Donders' law, the eye's gaze direction determines its orientation
uniquely, and moreover, the orientation does not depend on the history of eye motion
which has led to the given gaze direction . A precise specification of the ""allowed""
subspace of position is given by Listing's law: the observed orientations of the eye
are those which can be reached from the distinguished orientation called primary
*hand@wisdom.weizmann.ac.il
t tamar@wisdom.weizmann.ac.il

118

A. A. HANDZEL, T. FLASH

position through a single rotation about an axis which lies in the plane perpendicular
to the gaze direction at the primary position (Listing's plane). We say then that
the orientation of the eye has zero torsion. Recently, the domain of validity of
Listing's law has been extended to include eye vergence by employing a suitable
mathematical treatment (Van Rijn & Van Den Berg, 1993).

Tweed and Vilis used quaternion calculus to demonstrate, in addition, that in order
to move from one allowed position to another in a single rotation, the rotation axis
itself lies outside Listing's plane (Tweed & Vilis, 1987). Indeed, normal saccades are
performed approximately about a single axis. However, the validity of Listing's law
does not depend on the rotation having a single axis, as was shown in double-step
target displacement experiments (Minken, Van Opstal & Van Gisbergen, 1993):
even when the axis of rotation itself changes during the saccade, Listing's law is
obeyed at each and every point along the trajectory which is traced by the eye.
Previous analyses of eye rotations (and in particular of Listing's law) have been
based on various representations of rotations: quaternions (Westheimer, 1957), rotation vectors (Hepp, 1990), spinors (Hestenes, 1994) and 3 x 3 rotation matrices;
however, they are all related through the same underlying mathematical object the three dimensional (3D) rotation group . In this work we analyse the geometry of
saccades using the Lie algebra of the rotation group and the group structure. Next,
we briefly describe the basic mathematical notions which will be needed later. This
is followed by Section 2 in which we analyse various parameterizations of rotations
from the point of view of group theory; Section 3 contains a detailed mathematical
analysis of Listing's law and its connection to Donders' law based on the group
structure; in Section 4 we briefly discuss the issue of angular velocity vectors or
axes of rotation ending with a short conclusion.

1.2

THE ROTATION GROUP AND ITS LIE ALGEBRA

The group of rotations in three dimensions, G = 80(3), (where '80' stands for
special orthogonal transformations) is used both to describe actual rotations and
to denote eye positions by means of a unique virtual rotation from the primary
position. The identity operation leaves the eye at the primary position, therefore,
we identify this position with the unit element of the group e E 80(3) . A rotation
can be parameterized by a 3D axis and the angle of rotation about it. Each axis
""generates"" a continuous set of rotations through increasing angles . Formally, if n
is a unit axis of rotation, then
EXP(O? n)
(1)
is a continuous one-parameter subgroup (in G) of rotations through angles () in the
plane that is perpendicular to n. Such a subgroup is denoted as 80(2) C 80(3).
We can take an explicit representation of n as a matrix and the exponent can
be calculated as a Taylor series expansion. Let us look, for example, at the one
parameter subgroup of rotations in the y- z plane, i.e. rotations about the x axis
which is represented in this case by the matrix

o
o

(2)

-1

A direct computation of this rotation by an angle () gives

o
cos ()
- sin ()

o
sin () )
cos ()

(3)

119

The Geometry of Eye Rotations and Listing's Law

where I is the identity matrix. Thus, the rotation matrix R( 0) can be constructed
from the axis and angle of rotation . The same rotation, however, could also be
achieved using ALx instead of Lx, where A is any scalar, while rescaling the angle
to 0/ A. The collection of matrices ALx is a one dimensional linear space whose
elements are the generators of rotations in the y-z plane.
The set of all the generators constitutes the Lie algebra of a group. For the full
space of 3D rotations, the Lie algebra is the three dimensional vector space that is
spanned by the standard orthonormal basis comprising the three direction vectors
of the principal axes:

(4)
Every axis n can be expressed as a linear combination of this basis. Elements of
the Lie algebra can also be represented in matrix form and the corresponding basis
for the matrix space is

L.=

0 D
0
0

-1

L,

=(

0
0

-1

0
0
0

n

L,

=(

~1

1
0
0

D;

(5)

hence we have the isomorphism

( -~,

-Oy

Oz
0

-Ox

8, )
Ox

+-------t

0

U: )

(6)

Thanks to its linear structure, the Lie algebra is often more convenient for analysis
than the group itself. In addition to the linear structure, the Lie algebra has a
bilinear antisymmetric operation defined between its elements which is called the
bracket or commutator. The bracket operation between vectors in g is the usual
vector cross product . When the elements of the Lie algebra are written as matrices ,
the bracket operation becomes a commutation relation, i.e.
[A,B] == AB - BA.

(7)

As expected, the commutation relations of the basis matrices of the Lie algebra (of
the 3D rotation group) are equivalent to the vector product:

(8)
Finally, in accordance with (1), every rotation matrix is obtained by exponentiation:

R(8) = EXP(OxLx +OyLy +OzLz).

(9)

where 8 stands for the three component angles .

2

CO-ORDINATE SYSTEMS FOR ROTATIONS

In linear spaces the ""position"" of a point is simply parameterized by the co-ordinates
w.r.t. the principal axes (a chosen orthonormal basis). For a non-linear space (such
as the rotation group) we define local co-ordinate charts that look like pieces of
a vector space ~ n. Several co-ordinate systems for rotations are based on the
fact that group elements can be written as exponents of elements of the Lie algebra (1). The angles 8 appearing in the exponent serve as the co-ordinates.
The underlying property which is essential for comparing these systems is the noncommutativity of rotations. For usual real numbers, e.g. Cl and C2, commutativity implies expCI exp C2 = expCI +C2. A corresponding equation for non-commuting
elements is the Campbell-Baker-Hausdorff formula (CBH) which is a Taylor series

A. A. HANDZEL. T. FLASH

120

expansion using repeated commutators between the elements of the Lie algebra.
The expansion to third order is (Choquet-Bruhat et al., 1982):

EXP(Xl)EXP(X2) = EXP (Xl

+ X2 + ~[Xl' X2] + 112 [Xl -

X2, [Xl, X2]])

(10)

where Xl, X2 are variables that stand for elements of the Lie algebra.
One natural parameterization uses the representation of a rotation by the axis and
the angle of rotation. The angles which appear in (9) are then called canonical
co-ordinates of the first kind (Varadarajan, 1974). Gimbal systems constitute a
second type of parameterization where the overall rotation is obtained by a series
of consecutive rotations about the principal axes. The component angles are then
called canonical co-ordinates of the second kind. In the present context, the first
type of co-ordinates are advantageous because they correspond to single axis rotations which in turn represent natural eye movements. For convenience, we will use
the name canonical co-ordinates for those of the first kind, whereas those of the
second type will simply be called gimbals. The gimbals of Fick and Helmholtz are
commonly used in the study of oculomotor control (Van Opstal, 1993). A rotation
matrix in Fick gimbals is

RF(Bx,Oy,Oz)

= EXP(OzL z )

. EXP(ByLy) . EXP(OxLx),

(11)

and in Helmholtz gimbals the order of rotations is different:

RH(Ox, By,Oz) = EXP(ByLy) . EXP(OzL z ) . EXP(OxLx).

(12)

The CBH formula (10) can be used as a general tool for obtaining transformations
between various co-ordinate systems (Gilmore, 1974) such as (9,11,12). In particular, we apply (10) to the product of the two right-most terms in (11) and then again
to the product of the result with the third term. We thus arrive at an expression
whose form is the same as the right hand side of (10). By equating it with the
expression for canonical angles (9) and then taking the log of the exponents on
both sides of the equation, we obtain the transformation formula from Fick angles
to canonical angles. Repeating this calculation for (12) gives the equivalent formula
for Helmholtz angles l . Both transformations are given by the following three equations where OF,H stands for an angle either in Fick or in Helmholtz co-ordinates; for
Helmholtz angles there is a plus sign in front of the last term of the first equation
and a minus sign in the case of Fick angles:

Be
- OF,H
x x

(1 _ ...L ((BF,H)2 + (OF,H)2)) ? lOF,HOF,H
12

Y

z

2 Y

z

Of

= O:,H ( 1 - /2 (( O;,H)2 + (O:,H)2) ) + ~O;,H O:,H

Of

= O;,H ( 1 - /2 (( B;,H? + (B:,H)2))

(13)

- !O;,H O:,H

The error caused by the above approximation is smaller than 0.1 degree within most
of the oculomotor range.
We mention in closing two additional parameterizations, namely quaternions and
rotation vectors. Unit quaternions lie on the 3D sphere S3 (embedded in lR 4) which
constitutes the same manifold as the group of unitary rotations SU(2). The latter
is the double covering group of SO(3) having the same local structure. This enables
to use quaternions to parameterize rotations. The popular rotation vectors (written
as tan(Oj2)n, n being the axis of rotation and B its angle) are closely related to
1 In contrast to this third order expansion, second order approximations usually appear
in the literature; see for example equation B2 in (Van Rijn & Van Den Berg, 1993).

121

The Geometry of Eye Rotations and Listing's Law

quaternions because they are central (gnomonic) projections of a hemisphere of S3
onto the 3D affine space tangent to the quaternion qe = (1,0,0,0) E ]R4 . 2

3

LISTING'S LAW AND DONDERS' LAW

A customary choice of a head fixed coordinate system is the following: ex IS III
the straight ahead direction in the horizontal plane, e y is in the lateral direction
and e z points upwards in the vertical direction . ex and e z thus define the midsagittal plane; e y and e z define the coronal plane. The principal axes of rotations
(Lx, Ly, Lz) are set parallel to the head fixed co-ordinate system. A reference eye orientation called the primary position is chosen with the gaze direction being (1,0,0)
in the above co-ordinates. How is Listing's law expressed in terms of the Lie algebra
of SO(3)? The allowed positions are generated by linear combinations of Lz and
Ly only. This 2D subspace of the Lie algebra,

1 = Span{Ly, L z },

(14)

is Listing's plane. Denoting Span{ Lx} by h, we have a decomposition of the Lie
algebra so(3) into a direct sum of two linear subspaces:
9 = 1 EB h.

(15)

Every vector v E 9 can be projected onto its component which is in I:
V

= +
VI

proj.
Vh ----t VI.

(16)

Until now, only the linear structure has been considered. In addition, h is closed
under the bracket operation:
(17)

and because h is closed both under vector addition and the Lie bracket, it is a
sub algebra of g. In contrast, I is not a sub algebra because it is not closed under
commutation (8) . The fact that h stands as an algebra on its own implies that it
has a corresponding group H, just as 9 = so(3) corresponds to G = SO(3). The
subalgebra h generates rotations about the x axis, and therefore H is SO(2), the
group of rotations in a plane.
The group G = SO(3) does not have a linear structure. We may still ask whether
some kind of decomposition and projection can be achieved in G in analogy to
(15,16). The answer is positive and the projection is performed as follows: take any
element of the group , a E G , and multiply it by all the elements of the subgroup H.
This gives a subset in G which is considered as a single object a called a coset:

a = {ab I bEH} .

(18)

The set of all cosets constitutes the quotient space. It is written as

S == G / H = SO(3)/ SO(2)

(19)

because mapping the group to the quotient space can be understood as dividing G
by H. The quotient space is not a group , and this corresponds to the fact that the
subspace I above (14) is not a sub algebra. The quotient space has been constructed
algebraically but is difficult to visualize; however, it is mathematically equivalent
2 Geometrically, each point q E S3 can be connected to the center of the sphere by a
line. Another line runs from qe in the direction parallel to the vector part of q within the
tangent space. The intersection of the two lines is the projected point. Numerically, one
simply takes the vector part of q divided by its scalar part.

A. A.HANDZEL,T. FLASH

122

Table 1: Summary table of biological notions and the corresponding mathematical
representation, both in terms of the rotation group and its Lie algebra.
Biological notion
general eye position
primary position
eye torsion
""allowed"" eye
positions

Lie Algebra

9

= so(3) = h El71
O.q E 9
h = Span{Lx}

1= Span{ L y, LzJ
(Listing's plane)

Rotation Group
G = SO(3)
eE G
H SO(2)
S ~/H SO(3)/SO(2)
~ S2 (Donders' sphere
of gaze directions)

=

=
=

to another space - the unit sphere S2 (embedded in ~3). This equivalence can be
seen in the following way: a unit vector in ~3, e.g. e = (1,0,0), can be rotated so
that its head reaches every point on the unit sphere S2; however, for any such point
there are infinitely many rotations by which the point can be reached. Moreover,
all the rotations around the x axis leave the vector e above invariant. We therefore
have to ""factor out"" these rotations (of H =SO(2? in order to eliminate the above
degeneracy and to obtain a one-to-one correspondence between the required subset
of rotations and the sphere. This is achieved by going to the quotient space.
The matrix of a torsion less rotation (generated by elements in Listing's plane) is
obtained by setting Ox =0 in (9):
cosO
R = ( - sin 0 sin ljJ
- sin 0 cos ljJ

where

sin 0 sin ljJ
cos 0 + (1 - cos 0) cos 2 ljJ
cos ljJ sin ljJ(l - cos 0)

sin 0 cos ljJ
)
cos ljJ sin ljJ(l - cos 0)
,(20)
2
cos 0 + (1 - cos 0) sin ljJ

0= .)0;+0; is the total angle of rotation and ljJ is the angle between 0and

the y axis in the Oy -Oz plane, i.e. (0, ljJ) are polar co-ordinates in Listing's plane.
Notice that the first column on the left constitutes the Cartesian co-ordinates of a
point on a sphere of unit radius (Gilmore, 1974).
As we have just seen, there is an exact correspondence between the group level and
the Lie algebra level. In fact, the two describe the same reality, the former in a
global manner and the latter in an infinitesimal one. Table 1 summarizes the important biological notions concerning Listing's law together with their corresponding
mathematical representations. The connection between Donders' law and Listing's
law can now be seen in a clear and intuitive way. The sphere, which was obtained
by eliminating torsion, is the space of gaze directions. Recall that Donders' law
states that the orientation of the eye is determined uniquely by its gaze direction.
Listing's law implies that we need only take into consideration the gaze direction
and disregard torsion. In order to emphasize this point, we use the fact that locally,
SO(3) looks like a product of topological spaces: 3
p =

u x SO(2)

where

(21)

U parameterizes gaze direction and SO(2) - torsion. Donders' law restricts eye
orientation to an unknown 2D submanifold of the product space P. Listing's law
shows that the submanifold is U, a piece of the sphere. This representation is
advantageous for biological modelling, because it mathematically sets apart the
degrees of freedom of gaze orientation from torsion, which also differ functionally.
350(3) is a principal bundle over S2 with fiber 50(2).

The Geometry of Eye Rotations and Listing's Law

4

123

AXES OF ROTATION FOR LISTING'S LAW

As mentioned in the introduction, moving between two (non-primary) positions
requires a rotation whose axis (i.e. angular velocity vector) lies outside Listing's
plane. This is a result of the group structure of SO(3). Had the axis of rotation
been contained within Listing's plane, the matrices of the quotient space (20) should
have been closed under multiplication so as to form a subgroup of SO(3). In other
words, if ri and rJ are matrices representing the current and target orientations of
the eye corresponding to axes in Listing's plane, then rJ . r;l should have been a
matrix of the same form (20); however, as explained in Section 3, this condition is
not fulfilled.
Finally, since normal saccades involve rotations about a single axis, they are oneparameter subgroups generated by a single element of the Lie algebra (1). In addition, they have the property of being geodesic curves in the group manifold under
the natural metric which is given by the bilinear Cartan-Killing form of the group
(Choquet-Bruhat et al., 1982).

5

CONCLUSION

We have analysed the geometry of eye rotations using basic Lie group theory and
differential geometry. The unifying view presented here can serve to improve the
understanding of the oculomotor system. It may also be extended to study the
three dimensional rotations of the joints of the upper limb.
Acknowledgements

We would like to thank Stephen Gelbart, Dragana Todoric and Yosef Yomdin for
instructive conversations on the mathematical background and Dario Liebermann
for fruitful discussions. Special thanks go to Stan Gielen for conversations which
initiated this work.
References

Choquet-Bruhat Y., De Witt-Morette C. & Dillard-Bleick M., Analysis, Manifolds
and Physics, North-Holland (1982).
Gilmore R.,LieGroups, Lie Algebras, and Some of Their Applications, Wiley (1974).
Hepp K., Commun. Math. Phys. 132 (1990) 285-292.
Hestenes D., Neural Networks 7, No.1 (1994) 65-77.
Minken A.W.H. Van Opstal A.J. & Van Gisbergen J.A .M., Exp. Brain Research
93 (1993) 521-533.
Tweed, D. & Vilis T., J. Neurophysiology 58 (1987) 832-849.
Tweed D. & Vilis T., Vision Research 30 (1990) 111-127.
Van Opstal J., ""Representations of Eye Positions in Three Dimensions"", in Multisensory Control of Movement, ed. Berthoz A., (1993) 27-4l.
Van Rijn L.J. & Van Den Berg A.V., Vision Research 33, No. 5/6 (1993) 691-708.
Varadarajan V.S., Lie Groups, Lie Algebras, and Their Reps., Prentice-Hall (1974).
Westheimer G., Journal of the Optical Society of America 47 (1957) 967-974.

"
1042,1995,Reinforcement Learning by Probability Matching,,1042-reinforcement-learning-by-probability-matching.pdf,Abstract Missing,"Reinforcement Learning by Probability
Matching

Philip N. Sabes

Michael I. Jordan

sabes~psyche.mit.edu

jordan~psyche.mit.edu

Department of Brain and Cognitive Sciences
Massachusetts Institute of Technology
Cambridge, MA 02139

Abstract
We present a new algorithm for associative reinforcement learning. The algorithm is based upon the idea of matching a network's
output probability with a probability distribution derived from the
environment's reward signal. This Probability Matching algorithm
is shown to perform faster and be less susceptible to local minima
than previously existing algorithms. We use Probability Matching to train mixture of experts networks, an architecture for which
other reinforcement learning rules fail to converge reliably on even
simple problems. This architecture is particularly well suited for
our algorithm as it can compute arbitrarily complex functions yet
calculation of the output probability is simple.

1

INTRODUCTION

The problem of learning associative networks from scalar reinforcement signals is
notoriously difficult . Although general purpose algorithms such as REINFORCE
(Williams, 1992) and Generalized Learning Automata (Phansalkar, 1991) exist, they
are generally slow and have trouble with local minima. As an example, when we
attempted to apply these algorithms to mixture of experts networks (Jacobs et al. ,
1991), the algorithms typically converged to the local minimum which places the
entire burden of the task on one expert.
Here we present a new reinforcement learning algorithm which has faster and more
reliable convergence properties than previous algorithms. The next section describes
the algorithm and draws comparisons between it and existing algorithms. The
following section details its application to Gaussian units and mixtures of Gaussian
experts. Finally, we present empirical results.

1081

Reinforcement Learning by Probability Matching

2

REINFORCEMENT PROBABILITY MATCHING

We begin by formalizing the learning problem. Given an input x E X from the
environment, the network must select an output y E y. The network then receives
a scalar reward signal r, with a mean r and distribution that depend on x and
y. The goal of the learner is to choose an output which maximizes the expected
reward. Due to the lack of an explicit error signal, the learner must choose its
output stochastically, exploring for better rewards. Typically the learner starts with
a parameterized form for the conditional output density P8(ylx), and the learning
problem becomes one of finding the parameters 0 which maximize the expected
reward:

Jr(O) =

1

p(x)p8(ylx)r(x, y)dydx.

X,Y

We present an alternative route to the maximum expected reward cost function,
and in doing so derive a novel learning rule for updating the network's parameters.
The learner's task is to choose from a set of conditional output distributions based
on the reward it receives from the environment. These rewards can be thought of as
inverse energies; input/output pairs that receive high rewards are low energy and
are preferred by the environment. Energies can always be converted into probabilities through the Boltzmann distribution, and so we can define the environment's
conditional distribution on Y given x,
*( I) exp( _T- 1 E(x, y?
exp(T-1r(x, y?
pyx =
ZT(X)
=
ZT(X)
,
where T is a temperature parameter and ZT(X) is a normalization constant which
depends on T . This distribution can be thought of as representing the environment's
ideal input-output mapping, high reward input-output pairs being more typical or
likely than low reward pairs. The temperature parameter determines the strength of
this preference: when T is infinity all outputs are equally likely; when T is zero only
the highest reward output is chosen. This new distribution is a purely theoretical
construct, but it can be used as a target distribution for the learner. If the 0 are
adjusted so that P8(ylx) is nearly equal to p*(ylx), then the network's output will
typically result in high rewards.
The agreement between the network and environment conditional output densities
can be measured with the Kullback-Liebler (KL) divergence:

K L(p II P*)
=

-1
-~ 1
=

(1)

p(x)p8(ylx) [logp*(Ylx) -logp8(ylx)] dydx

X,Y

p(x)p8(ylx)[r(x,y) - Tr8(X,y)]dydx+

X,Y

f p(x)logZT(x)dx,
Jx

where r8(x, y) is defined as the logarithm of the conditional output probability and
can be thought of as the network's estimate of the mean reward. This cost function
is always greater than or equal to zero, with equality only when the two probability
distributions are identical.
Keeping only the part of Equation 1 which depends on 0, we define the Probability
Matching (PM) cost function:

JpM(O)

= - Jx,y
f p(x)p8(ylx)[r(x, y) -

Tr8(X, y)] dydx

= -Jr(O) -

TS(P8)

The PM cost function is analogous to a free energy, balancing the energy, in the
form of the negative of the average reward, and the entropy S(P8) of the output

P. N. SABES, M. I. JORDAN

1082

-1

-1

T=.5

T=l

T= .2

-0.5

0

0.5

T= .05

Figure 1: p*'s (dashed) and PM optimal Gaussians (solid) for the same bimodal reward
function and various temperatures. Note the differences in scale.

distribution. A higher T corresponds to a smoother target distribution and tilts the
balance of the cost function in favor of the entropy term, making diffuse output distributions more favorable. Likewise, a small T results in a sharp target distribution
placing most of the weight on the reward dependent term of cost function, which is
always optimized by the singular solution of a spike at the highest reward output.
Although minimizing the PM cost function will result in sampling most often at
high reward outputs, it will not optimize the overall expected reward if T > O.
There are two reasons for this. First, the output y which maximizes ro(x, y) may
not maximize rex, y). Such an example is seen in the first panel of Figure 1:
the network's conditional output density is a Gaussian with adjustable mean and
variance, and the environment has a bimodal reward function and T = 1. Even in
the realizable case, however, the network will choose outputs which are suboptimal
with respect to its own predicted reward, with the probability of choosing output y
falling off exponentially with ro(x, y). The key point here is that early in learning
this non-optimality is exactly what is desired. The PM cost function forces the
learner to maintain output density everywhere the reward, as measure by p*l/T, is
not much smaller than its maximum. When T is high, the rewards are effectively
flattened and even fairly small rewards look big. This means that a high temperature
ensures that the learner will explore the output space.
Once the network is nearly PM optimal, it would be advantageous to ""sharpen up""
the conditional output distribution, sampling more often at outputs with higher
predicted rewards. This translates to decreasing the entropy of the output distribution or lowering T. Figure 1 shows how the PM optimal Gaussian changes as the
temperature is lowered in the example discussed above; at very low temperatures
the output is almost always near the mode of the target distribution. In the limit
of T = 0, J PM becomes original reward maximization criterion Jr. The idea of the
Probability Matching algorithm is to begin training with a large T, say unity, and
gradually decrease it as the performance improves, effectively shifting the bias of
the learner from exploration to exploitation.
We now must find an update rule for 0 which minimizes JpM(O). We proceed by
looking for a stochastic gradient descent step. Differentiating the cost function gives
\T OJpM(O) =

-1

X,Y

p(x)po(Ylx) [rex, y) - Tro(x, y)] \T oro(x, y)dydx.

Thus, if after every action the parameters are updated by the step

t:.o =

a [r - Tro(x, y)] \T oro (x, y),

(2)

where alpha is a constant which can vary over time, then the parameters will on
average move down the gradient of the PM cost function. Note that any quantity

Reinforcement Learning by Probability Matching

1083

which does not depend on Y or r can be added to the difference in the update rule,
and the expected step will still point along the direction of the gradient.
The form of Equation 2 is similar to the REINFORCE algorithm (Williams, 1992),
whose update rule is
t:.() = a(r - b)V' elogpe(Ylx),
where b, the reinforcement baseline, is a quantity which does not depend on Y or r.
Note that these two update rules are identical when T is zero.! The advantage of the
PM rule is that it allows for an early training phase which encourages exploration
without forcing the output distribution to converge on suboptimal outputs. This
will lead to striking qualitative differences in the performance of the algorithm for
training mixtures of Gaussian experts.

3

UPDATE RULES FOR GAUSSIAN UNITS AND
MIXTURES OF GAUSSIAN EXPERTS

We employ Gaussian units with mean I' = w T x and covariance 0""21. The learner
must select the matrix wand scalar 0"" which minimize JpM(W, 0""). Applying the
update rule in Equation 2, we get
""
1
a[r - Tr(x,y)] 2""(Y -I'?x

t:.w

0""

""

a [r - Tr(x, y)]

1 (""Y -I'W

0""2

0""2

-

1) .

In practice, for both single Gaussian units and the mixtures presented below we
avoid the issue of constraining 0"" > 0 by updating log 0"" directly.
We can generalize the linear model by considering a conditional output distribution
in the form of a mixture of Gaussian experts (Jacobs et al., 1991),
N

p(Ylx)

1

1

= Lgi(x)(27r0""1)-~ exp(--2I1y -l'iW)?
i=!

20""i

r

Expert i has mean I'i = w x and covariance 0""[1. The prior probability given x of
choosing expert i, gi(X), is determined by a single layer gating network with weight
matrix v and softmax output units. The gating network learns a soft partitioning
of the input space into regions for which each expert is responsible .
Again, we can apply Equation 2 to get the PM update rules:
t:.Vi

a [r - Tf(x,y)] (hi - gi)X

t:.Wi

a [r - Tf(x, y)]

hi~(Y -

l'i?X

O""i

6.O""i

a[r-Tf(x,Y)]h i

:

1 (""Y~riW

-1),

where hi = giPi(ylx)jp(ylx) is the posterior probability of choosing expert i given
y. We note that the PM update rules are equivalent to the supervised learning
gradient descent update rules in (Jacobs et al., 1991) modulated by the difference
between the actual and expected rewards.
lThis fact implies that the REINFORCE step is in the direction of the gradient of JR(B),
as shown by (Williams, 1992). See Williams and Peng, 1991, for a similar REINFORCE
plus entropy update rule.

1084

P. N. SABES, M. I. JORDAN

Table 1: Convergence times and gate entropies for the linear example (standard errors
in parentheses). Convergence times: An experiment consisting of 50 runs was conducted
for each algorithm, with a wide range of learning rates and both reward functions. Best
results for each algorithm are reported. Entropy: Values are averages over the last 5,000
time steps of each run. 20 runs of 50,000 time steps were conducted.
Algorithm
PM, T= 1
PM, T=.5
PM, T =.1
REINFORCE
REINF-COMP

I Convergence Time I
1088 (43)
-

2998 (183)
1622 (46)

Entropy
.993 .0011
.97 .02
.48 .04
.21 .03
.21 .03

Both the hi and r depend on the overall conditional probability p(ylx), which in
turn depends on each Pi(ylx). This adds an extra step to the training procedure.
After receiving the input x, the network chooses an expert based on the priors gi(X)
and an output y from the selected expert's output distribution . The output is then
. sent back to each of the experts in order to compute the likelihood of their having
generated it. Given the set of Pi'S, the network can update its parameters as above.

4

SIMULATIONS

We present three examples designed to explore the behavior of the Probability
Matching algorithm. In each case, networks were trained using Probability Matching, REINFORCE, and REINFORCE with reinforcement comparison (REINFCOMP), where a running average of the reward is used as a reinforcement baseline (Sutton, 1984). In the first two examples an optimal output function y*(x)
was chosen and used to calculate a noisy error, c = Ily - y*(x) - zll, where z was
i.i.d. zero-mean Gaussian with u = .1. The error signal determined the reward
by one of two functions, r
-c 2 /2 or exp( _c 2 /2). When the RMSE between the
network mean and the optimal output was less that .05 the network was said to
have converged.

=

4.1

A Linear Example

In this example x was chosen uniformly from [-1,1]2 x {I}, and the optimal output
was y* = Ax, for a 2 x 3 matrix A. A mixture of three Gaussian experts was trained.
The details of the simulation and results for each algorithm are shown in Table 1.
Probability Matching with constant T
1 shows almost a threefold reduction
in training time compared to REINFORCE and about a 50% improvement over
REINF-COMP.

=

The important point of this example is the manner in which the extra Gaussian
units were employed. We calculated the entropy of the gating network, normalized
so that a value of one means that each expert has equal probability of being chosen
and a value of zero means that only one expert is ever chosen. The values after
50,000 time steps are shown in the second column of Table 1. When T ~ 1, the
Probability Matching algorithm gives the three experts roughly equal priors. This
is due to the fact that small differences in the experts' parameters lead to increased
output entropy if all experts are used. REINFORCE on the other hand always
converges to a solution which employs only one expert. This difference in the
behavior of the algorithms will have a large qualitative effect in the next example.

Reinforcement Learning by Probability Matching

J085

Table 2: Results for absolute value. The percentage of trials that converged and the
average time to convergence for those trials. Standard errors are in parentheses. 50 trials
were conducted for a range of learning rates and with both reward functions; the best
results for each algorithm are shown.
Algorithm
PM
REINFORCE
REINF-COMP

I Successful Trials I Convergence Time I
100%
48%
38%

6,052 313)
76,775 3,329)
42,105 3,869)

8 .0
110
100
10
60

2.0

.0
0 .0

10

0.0

(a)

1.0

(b)

1 .0

'.0

' .0

(c)

' .0

-2.0
0.0

1.0

2.0

'.0

'.0

(d)

Figure 2: Example 4.3. The environment's probability distribution for T = 1: (a) density
plot of p. vs. y / x, (b) cross-sectional view with Y2 = o. Locally weighted mean and
variance of Y2/X over representative runs: (c) T

4.2

= 1,

(d) T

= 0 (i.e.

REINFORCE).

Absolute Value

We used a mixture of two Gaussian units to learn the absolute value function.
The details of the simulation and the best results for each algorithm are shown in
Table 2. Probability Matching with constant T = 1 converged to criterion on every
trial, in marked contrast to the REINFORCE algorithm. With no reinforcement
baseline, REINFORCE converged to criterion in only about half of the cases, less
with reinforcement comparison. In almost all of the trials that didn't converge, only
one expert was active on the domain of the input. Neither version of REINFORCE
ever converged to criterion in less than 14,000 time steps.
This example highlights the advantage of the Probability Matching algorithm. During training, all three algorithms initially use both experts to capture the overall
mean of the data. REINFORCE converges on this local minimum, cutting one
expert off before it has a chance to explore the rest of the parameter space. The
Probability Matching algorithm keeps both experts in use. Here, the more conservative approach leads to a stark improvement in performance.
4.3

An Example with Many Local Maxima

In this example, the learner's conditional output distribution was a bivariate Gaussian with It = [Wl, W2]T x, and the environment's rewards were a function of y/x.
The optimal output distribution p*(y/x) is shown in Figures 2(a,b). These figures
can also be interpreted as the expected value of p* for a given w. The weight vector
is initially chosen from a uniform distribution over [-.2, .2]2, depicted as the very
small while dot in Figure 2(a). There are a series of larger and larger local maxima
off to the right, with a peak of height 2n at Wl = 2n.
The results are shown in Table 3. REINFORCE, both with and without reinforcement comparison, never got past third peak; the variance of the Gaussian unit would

'.0

P. N. SABES, M. I. JORDAN

1086

Table 3: Results for Example 4.3. These values represent 20 runs for 50,000 time steps
each. The first and second columns correspond to number of the peak the learner reached.
Algorithm
PM, T= 2
PM, T
1
PM, T=.5
REINFORCE
REINF-COMP

=

Mean Final
log2 Wl
28,8
6.34
3.06
2.17
2.05

Range of Final
log2 Wl'S
[19.1,51.0]
5.09,8.08
3.04,3.07
2.00,2.90
2.05,2.06

Mean Final
(T

> 101>
13.1
.40
.019
.18

very quickly close down to a small value making further exploration of the output
space impossible. Probability Matching, on the other hand, was able to find greater
and greater maxima, with the variance growing adaptively to match the local scale
of the reward function. These differences can be clearly seen in Figures 2( c,d),
which show typical behavior for the Probability Matching algorithm with T = 1
and T O.

=

5

CONCLUSION

We have presented a new reinforcement learning algorithm for associative networks
which converges faster and more reliably than existing algorithms. The strength of
the Probability Matching algorithm is that it allows for a better balance between
exploration of the output space and and exploitation of good outputs. The parameter T can be adjusted during learning to allow broader output distributions early
in training and then to force the network to sharpen up its distribution once nearly
optimal parameters have been found.
Although the applications in this paper were restricted to networks with Gaussian units, the Probability Matching algorithm can be applied to any reinforcement
learning task and any conditional output distribution. It could easily be employed,
for example, on classification problems using logistic or multinomial (softmax) output units or mixtures of such units. Finally, the simulations presented in this paper
are of simple examples. Preliminary results indicate that the advantages of the
Probability Matching algorithm scale up to larger, more interesting problems.

References
Jacobs, R . A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. (1991). Adaptive
mixtures of local experts. Neural Computation, 3:79-87.
Phansalkar, V. V. (1991). Learning automata algorithms for connectionist systems
- local and global convergence. PhD Thesis, Dept. of Electrical Engineering,
India Institute of Science, Bangalore.
Sutton, R. S. (1984). Temporal credit assignment in reinforcement learning.
PhD Thesis, Dept. of Computer and Information Science, University of Massachusetts, Amherst, MA.
Williams, R. J. (1992) . Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229-256.
Williams, R. J. and Peng, J. (1991). Function optimization using connectionist
reinforcement learning algorithms. Connection Science, 3:241-268.

"
1043,1995,Neural Control for Nonlinear Dynamic Systems,,1043-neural-control-for-nonlinear-dynamic-systems.pdf,Abstract Missing,"Neural Control for Nonlinear Dynamic Systems

Ssu-Hsin Yu
Department of Mechanical Engineering
Massachusetts Institute of Technology
Cambridge, MA 02139
Email: hsin@mit.edu

Anuradha M. Annaswamy
Department of Mechanical Engineering
Massachusetts Institute of Technology
Cambridge, MA 02139
Email: aanna@mit.edu

Abstract
A neural network based approach is presented for controlling two distinct
types of nonlinear systems. The first corresponds to nonlinear systems
with parametric uncertainties where the parameters occur nonlinearly.
The second corresponds to systems for which stabilizing control structures cannot be determined. The proposed neural controllers are shown
to result in closed-loop system stability under certain conditions.

1

INTRODUCTION

The problem that we address here is the control of general nonlinear dynamic systems
in the presence of uncertainties. Suppose the nonlinear dynamic system is described as
f(x , u , 0) , y = h(x, u, 0) where u denotes an external input, y is the output, x is the
state, and 0 is the parameter which represents constant quantities in the system. The control
objectives are to stabilize the system in the presence of disturbances and to ensure that
reference trajectories can be tracked accurately, with minimum delay. While uncertainties
can be classified in many different ways, we focus here on two scenarios. One occurs
because the changes in the environment and operating conditions introduce uncertainties
in the system parameter O. As a result, control objectives such as regulation and tracking,
which may be realizable using a continuous function u = J'(x, 0) cannot be achieved since
is unknown. Another class of problems arises due to the complexity of the nonlinear
function f. Even if 0, f and h can be precisely determined, the selection of an appropriate
J' that leads to stabilization and tracking cannot be made in general. In this paper, we
present two methods based on neural networks which are shown to be applicable to both
the above classes of problems. In both cases, we clearly outline the assumptions made,
the requirements for adequate training of the neural network, and the class of engineering
problems where the proposed methods are applicable. The proposed approach significantly
expands the scope of neural controllers in relation to those proposed in (Narendra and
Parthasarathy, 1990; Levin and Narendra, 1993; Sanner and Slotine, 1992; Jordan and
Rumelhart, 1992).

x=

o

Neural Control for Nonlinear Dynamic Systems

1011

The first class of problems we shall consider includes nonlinear systems with parametric
uncertainties. The field of adaptive control has addressed such a problem, and over the
past thirty years, many results have been derived pertaining to the control of both linear
and nonlinear dynamic systems (Narendra and Annaswamy, 1989). A common assumption
in almost all of the published work in this field is that the uncertain parameters occur
linearly. In this paper, we consider the control of nonlinear dynamic systems with nonlinear
parametrizations. We design a neural network based controller that adapts to the parameter
and show that closed-loop system stability can be achieved under certain conditions. Such
a controller will be referred to as a O-adaptive neural controller. Pertinent results to this
class are discussed in section 2.

o

The second class of problems includes nonlinear systems, which despite being completely
known, cannot be stabilized by conventional analytical techniques. The obvious method for
stabilizing nonlinear systems is to resort to linearization and use linear control design methods. This limits the scope of operation of the stabilizing controller. Feedback linearization
is another method by which nonlinear systems can be stably controlled (lsidori, 1989). This
however requires fairly stringent set of conditions to be satisfied by the functions! and h.
Even after these conditions are satisfied, one cannot always find a closed-form solution to
stabilize the system since it is equivalent to solving a set of partial differential equations.
We consider in this paper, nonlinear systems, where system models as well as parameters
are known, but controlIer structures are unknown. A neural network based controller is
shown to exist and trained so that a stable closed-loop system is achieved. We denote this
class of controllers as a stable neural controller. Pertinent results to this class are discussed
in section 3.

2

O-ADAPTIVE NEURAL CONTROLLER

The focus of the nonlinear adaptive controller to be developed in this paper is on dynamic
systems that can be written in the d-step ahead predictor form as follows:

Yt+d

= !r(Wt,Ut,O)

(I)

where wi = [Yt,"" . ,Yt-n+l, Ut-I, ' "", Ut-m-d+l], n ~ I, m ~ 0, d ~ I, m + d = n,
YI, U I C ~ containing the origin and 8 1 C ~k are open, ir : Y1 x U;n+d x 8 1 - t ~, Yt
and Ut are the output and the input of the system at time t respectively, and 0 is an unknown
parameter and occurs nonlinearly in ir.1 The goal is to choose a control input 'It such that
the system in (1) is stabilized and the plant output is regulated around zero.
Letxi

~

[Yt+d-I , '"" , Yt+l , wil T , Am = [e2,""', en+d-I, 0 , en+d+I,""', e n +m+2d-2,

0], Bm = [el' en+d], where e, is an unit vector with the i-th term equal to I. The following
assumptions are made regarding the system in Eq. (I ).
(AI) For every 0 E 8

1,

ir(O , O, O) = 0 .

CA2) There exist open and convex neighborhoods of the origin Y2 C YI and U2 C U I , an
open and convex set 82 C 8 1 , and a function K : 0.2 x Y2 x 8 2 ---> U I such that for
every Wt E 0.2 , Yt+d E Y2 and 0 E 8 2 , Eq. (1) can be written as Ut = K(wt, Yt+d, 0),
where 0.2 ~ Y 2

X

u;,,+d-I.

(A3) K is twice differentiable and has bounded first and second derivatives on EI ~ 0. 2 X
Y2 X 8 2 , while ir is differentiable and has a bounded derivative on 0.2 x I{ (E I ) x 8 2 .
(A4) There exists bg

o,BE28 , 11 1Here,

>

?

such that for every YI E ir(o.2, K(o.2' 0 , 8 2), 8 2), W E 0.2 and

(8K(w,y ,O) _ 8K(w,y,9))1 _
ay
ay
Y-

YI

. 8f,(w ,u ,O)
au

I - I > bg'
U-UI

as well as in the following sections, An denotes the n-th product space of the set A .

1012

S. YU, A. M. ANNASWAMY

(A5) There exist positive definite matrices P and Q of dimensions (n + m + 2d - 2) such
T'
-T T
T T
T
that x T
t (AmPAm - P)Xt+ J( BrnPBmK + 2x t ArnPBmK ~ -Xt QXt, where
[( = [0, K(wt, 0 , O)]T.
Since the objective is to control the system~n (1) where 0 is unknown, in order to stabilize
the output Y at the origin with an estimate Of, we choose the control input as

(2)

2.1

PARAMETER ESTIMATION SCHEME

Suppose the estimation algorithm for updating Ot is defined recursively as /10t ~ OtOt-I = R(Yt,Wt-d,Ut-d,Ot-l) the problem is to determine the function R such that Ot
converges to 0 asymptotical1y. In general, R is chosen to depend on Yt, Wt-d, 1?t-d and
Ot-l since they are measurable and contain information regarding O. For example, in the
case of linear systems which can be cast in the input predictor form, 1?t = <b[ 0, a wel\known linear parameter estimation method is to adjust /10 as (Goodwin and Sin, 1984)

/10t = 1+4>'t~~t-'/ [1?t-d -

?LdOt-d?

In other words, the mechanism for carrying out
parameter estimation is realized by R. In the case of general nonlinear systems, the task
of determining such a function R is quite difficult, especial\y when the parameters occur
nonlinearly. Hence, we propose the use of a neural network parameter estimation algorithm
denoted O-adaptive neural network (TANN) (Annaswamy and Yu, 1996). That is, we adjust
Ot as
if /1Vd, < - f
(3)
otherwise
where the inputs of the neural network are Yt, Wt-d, 1?t-d and
f defines a dead-zone where parameter adaptation stops.

Ot-I, the output is /10t, and

The neural network is to be trained so that the resulting network can improve the parameter estimation over time for any possible 0 in a compact set. In addition, the
trained network must ensure that the overal1 system in Eqs. (1), (2) and (3) is stable.
Toward this end, N in TANN algorithm is required to satisfy the fol1owing two properties: (PI) IN(Yt,wt - d,1?t - d,Ot-l)12
fl

>

A Tf
0 , where L.l.Vt
--

IiiUt 12

_

~ a(llfJt;~~,~1~2)2uLd' and (P2) /1Vt -/1Vd, < fl,

IiiUt - I,
12

ii - II _
Ut
- Ut

II

u,

2+ IC( <f;, _,,)1 1 1?-2
d, -- -a ( 1+
IC(
)12)2 t- d'
, 4>,-./

AV,
L.l.

= (~~

(Wt,Yt+d,O)lo=oo)T, Ut = Ut - K(Wt,Yt+d,Ot+d - I), (fit = [WT,Yt+djT,
a E (0, I) and 00 is the point where K is linearized and often chosen to be the mean value
C(?t)

of parameter variation.

2.2

TRAINING OF TANN FOR CONTROL

In the previous section, we proposed an algorithm using a neural network for adjusting
the control parameters. We introduced two properties (PI) and (P2) of the identification
algorithm that the neural network needs to possess in order to maintain stability of the
closed-loop system. In this section, we discuss the training procedure by which the weights
of the neural network are to be adjusted so that the network retains these properties,
The training set is constructed off-line and should compose of data needed in the training phase. If we wan..!. the algorithm in Eq. (3) to be valid on the specified sets Y3 and
U3 for various 0 and 0 in 83, the training set should cover those variables appearing in
Eq. (3) in their respective ranges. Hence, we first sample W in the set Y;- x U;:+d-I,

1013

Neural Control for Nonlinear Dynamic Systems

and B, 8 in the set 83. Their values are, say, WI, BI and 81 respectively. For the
particular
and BI we sample 8 again in the set {B E 8 3 1 IB - BII
181 - BI I},
and its value is, say, 8t. Once WI, BI , 81 and 8t are sampled, other data can then
be calculated, such as UI = K(WI' 0, 8d and YI = fr(WI, UI, Bd. We can also ob. th ecorrespon d?mg C(A-)
BK (
B) All: - -a(I+IC(?I)i2)2
2+IC(?dI 2 (UI -'ttl
~)2 an d
tam
'1'1 -- ao
WI , YI, 0, i l d l -

fh

:s:

_
IC(?IW
LI - a (1+IC(<I>I)I2)2

~ 2
UI) ,where

(UI -

element can then be formed as (Yl
ner, by choosing various w s , Bs ,

- _
?I -

T

[WI

~

T

_

,yJ} and UI -

~d
K(WI' YI,( 1 )?

A data

8t, BI , ~ Vd l , Ld. Proceeding in the same manand 8~ in their respective ranges, we form a typical

,WI ,UI,

1f.

training set Ttram = { (Ys , W s, us,1f~ , Bs , ~ Vd d Ls) 11
s :s: M}, where M denotes the
total number of patterns in the training set. If the quadratic penalty function method (Bertsekas, 1995) is used, properties (PI) and (P2) can be satisfied by training the network on
the training set to minimize the following cost function:

:s:

M

mJpl

~ mJ,n~~{(max{0, ~VeJ)2+ ;2 (max{0, IN i(W)1 2 -

L t})2}

(4)

To find a W which minimizes the above unconstrained cost function 1, we can apply
algorithms such as the gradient method and the Gauss-Newton method.

2.3

STABILITY RESULT

With the plant given by Eq. (1), the controller by Eq. (2), and the TANN parameter
estimation algorithm by Eq. (3), it can be shown that the stability of the closed-loop system
is guaranteed.
Based on the assumptions of the system in (1) and properties (PI) and (P2) that TANN
satisfies, the stability result of the closed-loop system can be concluded in the following
theorem. We refer the reader to (Yu and Annaswamy, 1996) for further detail.

Theorem 1 Given the compact sets Y;+ I X U:;+d x 8 3 where the neural network in Eq. (3)
is trained. There exist EI, E > 0 such that for any interior point B of 8 3 , there exist open
sets Y4 C Y3, U4 C U3 and a neighborhood 8 4 of B such that if Yo , ... , Yn+d-2 E Y4,
Uo, .. . , U n -2 E U4 , and 8n - l , ... ,8n +d - 2 E 8 4 , then all the signals in the closed-loop
system remain bounded and Yt converges to a neighborhood of the origin.
2.4

SIMULATION RESULTS

In this section, we present a simulation example of the TANN controller proposed in this
.
Th e system IS
. 0 f the f orm Yt+1 = I+e
lIy, ( I-y,)
h
B?IS the parameter to be
sectton.
U.USH"", + Ut, were
determined on-line. Prior information regarding the system is that () E [4, 10]. Based on
8 (I)
~
Eq. (2), the controller was chosen to be Ut = - ,y, 0 ,-;'Y' ' where Bt denotes the parameter
I+e - ? """"
estimate at time t. According to Eq. (3), B was estimated using the TANN algorithm with
inputs YHI, Yt. Ut and~, and E = 0.01. N is a Gaussian network with 700 centers. The
training set and the testing set were composed of 6,040 and 720 data elements respectively.
After the training was completed, we tested the TANN controller on the system with six
different values of B, 4.5, 5.5, 6.5, 7.~, 8.5 and 9.5, while the initial parameter estimate
and the initial output were chosen as BI = 7 and Yo = -0.9 respectively. The results are
plotted in Figure 1. It can be seen that Yt can be stabilized at the origin for all these values
of B. For comparison, we also simulated the system under the same conditions but with 8

1014

S. YU, A. M. ANNASWAMY

~

0

-1

-1

-2

-2

o

o

10

50
100

10

50

4

100

Figure 1: Yt (TANN Controller)

4

Figure 2: Yt (Extended Kalman Filter)

estimated using the extended Kalman filter (Goodwin and Sin, 1984). Figure 2 shows the
output responses. It is not surprising that for some values of fJ, especially when the initial
estimation error is large, the responses either diverge or exhibit steady state error.

3
3.1

STABLE NEURAL CONTROLLER
STATEMENT OF THE PROBLEM

Consider the following nonlinear dynamical system

X= j(x,u),

Y = h(x)

(5)

where x E Rn and u E RTn. Our goal is to construct a stabilizing neural controller as
u = N(y; W) where N is a neural network with weights W, and establish the conditions
under which the closed-loop system is stable.
The nonlinear system in (5) is expressed as a combination of a higher-order linear part and
a nonlinear part as
A x + Bu + RI (x, u) and y = Cx + R 2 (x), where j(O, O) = 0
and h(O) = O. We make the following assumptions: (AI) j, h are twice continuously
differentiable and are completely known. (A2) There exists a K such that (A - BKC) is
asymptotically stable.

x=

3.2 TRAINING OF THE STABLE NEURAL CONTROLLER
In order for the neural controller in Section 3.1 to result in an asymptotically stable c1osedloop system, it is sufficient to establish that a continuous positive definite function of the state
variables decreases monotonically through output feedback. In other words, if we can find a
scalar definite function with a negative definite derivative of all points in the state space, we
can guarantee stability of the overall system. Here, we limit our choices of the Lyapunov
function candidates to the quadratic form, i.e. V = x T Px, where P is positive definite,
and the goal is to choose the controller so that V< 0 where V= 2xT P j(x, N(h(x), W)).
Based on the above idea, we define a ""desired"" time-derivative Vd as Vd= -xTQx where
Q = QT > O. We choose P and Q matrices as follows. First, according to (AI), we can
find a matrix K to make (A - BKC) asymptotically stable. We can then find a (P, Q)
pair by choosing an arbitrary positive definite matrix Q and solving the Lyapunov equation,
(A - BKC)T P + P(A - BKC) = -Q to obtain a positive definite P.

1015

Neural Control for Nonlinear Dynamic Systems

With the contro\1er of the form in Section 3.1, the goal is to find W in the neural network
which yields V:::; V d along the trajectories in a neighborhood X C ~n of the origin in the
state space. Let Xi denote the value of a sample point where i is an index to the sample
variable X E X in the state space. To establish V:::; V d, it is necessary that for every Xl in
a neighborhood X C ~n of the origin, Vi:::;V d"" where Vi= 2x;Pf(x l ,N(h(:rl ) , W))
and

Vd, =

-x;

QX i .

That is, the goal is to find a W such that the inequality constraints

tlVe , :::; 0, where i = 1,??? , M, is satisfied, where tlVe , =V l - V d, and M denotes the
total number of sample points in X. As in the training of TANN controller, this can also
be posed as an optimization problem. If the same quadratic penalty function method is
used, the problem is to find W to minimize the fo\1owing cost function over the training
set, which is described as Ttra in = {(Xl' Yi, V d.)\l :::; i :::; M}:

1M

rwn J

6.

mJp 2 I: (max{O, tlVe ,})2

(6)

i= 1

3.3 STABILITY OF THE CLOSED-LOOP SYSTEM
Assum~tions

(A 1) and (A2) imply that a stabilizing controller u = - J( y exists so that
V = X Px is a candidate Lyapunov function . More genera\1y, suppose a continuous but
unknown function ,,((y) exists such that for V = x T Px, a control input 1t = ""((y) leads to
V:::; -x T Qx, then we can find a neural network N (y) which approximates ""((y) arbitrarily
closely in a compact set leading to closed-loop stability. This is summarized in Theorem 2
(Yu and Annaswamy, 1995).

Theorem 2 Let there be a continuous function ""((h(x)) such that 2xT P f(x , ""((h(x))) +
x T Qx :::; 0 for every X E X where X is a compact set containing the origin as an interior
point. Then, given a neighborhood 0 C X of the origin, there exists a neural controlierH =
N(h(x); W) and a compact set Y E X such that the solutions of
f(x , N(h(x); W))
converge to 0, for every initial condition x(to) E y.

x=

3.4

SIMULATION RESULTS

In this section, we show simulation results for a discrete-time nonlinear systems using the
proposed neural network contro\1er in Section 3, and compare it with a linear contro\1er to
illustrate the difference. The system we considered is a second-order nonlinear system Xt =
f(xt-I , Ut-I), where f = [II, 12]T, h = Xl t _ 1X (1 +X2'_ 1)+X2t-1 x (l-u t- I +uLI) and
12 = XT'_I + 2X2'_1 +Ut-I (1 + X2'_I)? It was assumed that X is measurable, and we wished
to stabilize the system around the origin. The controller is of the form Ht = N (x It, X2 t ).
The neural network N used is a Gaussian network with 120 centers. The training set and
the testing set were composed of 441 and 121 data elements respectively.
After the training was done, we plotted the actual change of the Lyapunov function, tlV,
using the linear controller U = - K x and the neural network controller in Figures 3 and 4
respectively. It can be observed from the two figures that if the neural network contro\1er is
used, tl V is negative definite except in a small neighborhood of the origin, which assures
that the closed-loop system would converge to vicinity of the origin; whereas, if the linear
controller is used, tl V becomes positive in some region away from the origin, which implies
that the system can be unstable for some initial conditions. Simulation results confirmed
our observation.

S. YU, A. M. ANNASWAMY

1016

-0 01

-0 I

-0 J
-0 J

Figure 3: ~V(u

= -K x )

-()2

-O J

Figure 4: ~V(u = N(x))

Acknowledgments
This work is supported in part by Electrical Power Research Institute under contract No.
8060-13 and in part by National Science Foundation under grant No. ECS-9296070.

References
[1] A. M. Annaswamy and S. Yu.

O-adaptive neural networks: A new approach to
parameter estimation. IEEE Transactions on Neural Networks, (to appear) 1996.

[2] D. P. Bertsekas. Nonlinear Programming. Athena Scientific, Belmont, MA, 1995.
[3] G. C. Goodwin and K. S. Sin. Adaptive Filtering Prediction and Control. PrenticeHall, Inc., 1984.
[4] A. Isidori. Nonlinear Control Systems. Springer-Verlag, New York, NY, 1989.
[5] M. L Jordan and D. E. Rumelhart. Forward models: Supervised learning with a distal
teacher. Cognitive Science , 16:307-354, 1992.
[6] A. U. Levin and K. S. Narendra. Control of nonlinear dynamical systems using neural
networks: Controllability and stabilization. IEEE Transactions on Neural Networks,
4(2): 192-206, March 1993.
[7] K. S. Narendra and A . M. Annaswamy. Stable Adaptive Systems. Prentice-Hall, Inc.,
1989.
[8] K. S. Narendra and K. Parthasarathy. Identification and control of dynamical systems
using neural networks. IEEE Transactions on Neural Networks, 1(I ):4-26, March
1990.
[9] R. M. Sanner and J.-J. E. Slotine. Gaussian networks for direct adaptive control. IEEE
Transactions on Neural Networks, 3(6):837-863, November 1992.
[10] S. Yu and A. M. Annaswamy. Adaptive control of nonlinear dynamic systems using
O-adaptive neural networks. Technical Report 9601 , Adaptive Control Laboratory,
Department of Mechanical Engineering, M.LT., 1996.
[11] S.-H. Yu and A. M. Annaswamy. Control of nonlinear dynamic systems using a
stability based neural network approach. In Technical report 9501, Adaptive Control
Laboratory, MIT, Submitted to Proceedings of the 34th IEEE Conference on Decision
and Control, New Orleans, LA, 1995.

"
1044,1995,Learning with ensembles: How overfitting can be useful,,1044-learning-with-ensembles-how-overfitting-can-be-useful.pdf,Abstract Missing,"Learning with ensembles: How
over-fitting can be useful

Peter Sollich
Department of Physics
University of Edinburgh, U.K.
P.SollichGed.ac.uk

Anders Krogh'""
NORDITA, Blegdamsvej 17
2100 Copenhagen, Denmark
kroghGsanger.ac.uk

Abstract
We study the characteristics of learning with ensembles. Solving
exactly the simple model of an ensemble of linear students, we
find surprisingly rich behaviour. For learning in large ensembles,
it is advantageous to use under-regularized students, which actually over-fit the training data. Globally optimal performance can
be obtained by choosing the training set sizes of the students appropriately. For smaller ensembles, optimization of the ensemble
weights can yield significant improvements in ensemble generalization performance, in particular if the individual students are subject to noise in the training process. Choosing students with a wide
range of regularization parameters makes this improvement robust
against changes in the unknown level of noise in the training data.

1

INTRODUCTION

An ensemble is a collection of a (finite) number of neural networks or other types
of predictors that are trained for the same task. A combination of many different predictors can often improve predictions, and in statistics this idea has been
investigated extensively, see e.g. [1, 2, 3]. In the neural networks community, ensembles of neural networks have been investigated by several groups, see for instance
[4, 5, 6, 7]. Usually the networks in the ensemble are trained independently and
then their predictions are combined.
In this paper we study an ensemble of linear networks trained on different but
overlapping training sets. The limit in which all the networks are trained on the
full data set and the one where all the data sets are different has been treated in
[8] . In this paper we treat the case of intermediate training set sizes and overlaps
?Present address: The Sanger Centre, Hinxton, Cambs CBIO IRQ, UK.

Learning with Ensembles: How Overfitting Can Be Useful

191

exactly, yielding novel insights into ensemble learning. Our analysis also allows us to
study the effect of regularization and of having different predictors in an ensemble.

2

GENERAL FEATURES OF ENSEMBLE LEARNING

We consider the task of approximating a target function fo from RN to R. It
will be assumed that we can only obtain noisy samples of the function, and the
(now stochastic) target function will be denoted y(x) . The inputs x are taken
to be drawn from some distribution P(x). Assume now that an ensemble of K
independent predictors fk(X) of y(x) is available. A weighted ensemble average is
denoted by a bar, like
(1)
lex) = L,wkfk(X),
k

which is the final output of the ensemble. One can think of the weight Wk as the
belief in predictor k and we therefore constrain the weights to be positive and sum
to one. For an input x we define the error of the ensemble c(x), the error of the
kth predictor ck(X), and its ambiguity ak(x)
c(x)
ck(X)

(y(x) -lex)?
(y(x) - fk(X)?
(fk(X) -1(x?2.

=

(2)
(3)
(4)

=

The ensemble error can be written as c(x)
lex) - a(x) [7], where lex)
L,k Wkck(X) is the average error over the individual predictors and a(x) =
L,k Wkak(X) is the average of their ambiguities, which is the variance of the output
over the ensemble. By averaging over the input distribution P(x) (and implicitly
over the target outputs y(x?, one obtains the ensemble generalization error
(5)
where c(x) averaged over P(x) is simply denoted c, and similarly for land a. The
first term on the right is the weighted average of the generalization errors of the individual predictors, and the second is the weighted average of the ambiguities, which
we refer to as the ensemble ambiguity. An important feature of equation (5) is that
it separates the generalization error into a term that depends on the generalization
errors of the individual students and another term that contains all correlations between the students. The latter can be estimated entirely from unlabeled data, i. e.,
without any knowledge of the target function to be approximated. The relation (5)
also shows that the more the predictors differ, the lower the error will be, provided
the individual errors remain constant.

In this paper we assume that the predictors are trained on a sample of p examples
of the target function, (xt',yt'), where yt' = fo(xt') + TJt' and TJt' is some additive
noise (Jl. = 1, ... ,p). The predictors, to which we refer as students in this context
because they learn the target function from the training examples, need not be
trained on all the available data. In fact, since training on different data sets will
generally increase the ambiguity, it is possible that training on subsets of the data
will improve generalization. An additional advantage is that, by holding out for
each student a different part of the total data set for the purpose of testing, one
can use the whole data set for training the ensemble while still getting an unbiased
estimate of the ensemble generalization error. Denoting this estimate by f, one has
(6)
where Ctest = L,k WkCtest,k is the average of the students' test errors. As already
pointed out, the estimate ft of the ensemble ambiguity can be found from unlabeled
data.

P. SOLLICH, A. KROGH

192

So far, we have not mentioned how to find the weights Wk. Often uniform weights
are used, but optimization of the weights in some way is tempting. In [5, 6] the
training set was used to perform the optimization, i.e., the weights were chosen to
minimize the ensemble training error. This can easily lead to over-fitting, and in [7]
it was suggested to minimize the estimated generalization error (6) instead. If this
is done, the estimate (6) acquires a bias; intuitively, however, we expect this effect
to be small for large ensembles.

3

ENSEMBLES OF LINEAR STUDENTS

In preparation for our analysis of learning with ensembles of linear students we now
briefly review the case of a single linear student, sometimes referred to as 'linear
perceptron learning'. A linear student implements the input-output mapping
1 T
J(x) = ..JNw x
parameterized in terms of an N-dimensional parameter vector w with real components; the scaling factor 1/..JN is introduced here for convenience, and . ..T denotes
the transpose of a vector. The student parameter vector w should not be confused with the ensemble weights Wk. The most common method for training such
a linear student (or parametric inference models in general) is minimization of the
sum-of-squares training error
E = L:(y/J - J(x/J))2 + Aw2
/J
where J.L = 1, ... ,p numbers the training examples. To prevent the student from
fitting noise in the training data, a weight decay term Aw2 has been added. The size
of the weight decay parameter A determines how strongly large parameter vectors
are penalized; large A corresponds to a stronger regularization of the student.
For a linear student, the global minimum of E can easily be found. However,
in practical applications using non-linear networks, this is generally not true, and
training can be thought of as a stochastic process yielding a different solution each
time. We crudely model this by considering white noise added to gradient descent
updates of the parameter vector w. This yields a limiting distribution of parameter
vectors P(w) ex: exp(-E/2T), where the 'temperature' T measures the amount of
noise in the training process.
We focus our analysis on the 'thermodynamic limit' N - t 00 at constant normalized
number of training examples, ex = p/N. In this limit, quantities such as the training
or generalization error become self-averaging, i.e., their averages over all training
sets become identical to their typical values for a particular training set. Assume
now that the training inputs x/J are chosen randomly and independently from a
Gaussian distribution P(x) ex: exp( - ~x2), and that training outputs are generated
by a linear target function corrupted by additive noise, i.e., y/J = w'f x/J /..IN + 1]/J,
where the 1]/J are zero mean noise variables with variance u 2 ? Fixing the length of the
parameter vector of the target function to w~ = N for simplicity, the generalization
error of a linear student with weight decay A and learning noise T becomes [9]
(; = (u 2 + T)G + A(U 2

-

8G

A) 8A .

(7)

On the r.h.s. of this equation we have dropped the term arising from the noise on
the target function alone, which is simply u 2 , and we shall follow this convention
throughout . The 'response function' Gis [10, 11]

G = G(ex, A) = (1 - ex - A+ )(1 - ex - A)2 + 4A)/2A.

(8)

193

Learning with Ensembles: How Overfitting Can Be Useful

For zero training noise, T = 0, and for any a, the generalization error (7} is minimized when the weight decay is set to A = (T2j its value is then (T2G(a, (T2), which
is the minimum achievable generalization error [9].

3.1

ENSEMBLE GENERALIZATION ERROR

We now consider an ensemble of K linear students with weight decays Ak and
learning noises Tk (k = 1 . . . K). Each ,student has an ensemble weight Wk and
is trained on N ak training examples, with students k and I sharing N akl training
examples (of course, akk = ak). As above, we consider noisy training data generated
by a linear target function. The resulting ensemble generalization error can be
calculated by diagrammatic [10] or response function [11] methods. We refer the
reader to a forthcoming publication for details and only state the result:

(9)
where
(10)
Here Pk is defined as Pk = AkG(ak, Ak). The Kronecker delta in the last term
of (10) arises because the training noises of different students are uncorrelated. The
generalization errors and ambiguities of the individual students are

ak = ckk - 2 LWlckl
I

+ LWIWmclm;
1m

the result for the Ck can be shown to agree with the single student result (7). In
the following sections, we shall explore the consequences of the general result (9) .
We will concentrate on the case where the training set of each student is sampled
randomly from the total available data set of size NO', For the overlap of the training
sets of students k and I (k 'II) one then has akl/a = (ak/a)(al/a) and hence

ak/ = akal/a

(11)
up to fluctuations which vanish in the thermodynamic limit. For finite ensembles
one can construct training sets for which akl < akal/a. This is an advantage,
because it results in a smaller generalization error, but for simplicity we use (11).

4

LARGE ENSEMBLE LIMIT

We now use our main result (9) to analyse the generalization performance of an ensemble with a large number K of students, in particular when the size of the training
sets for the individual students are chosen optimally. If the ensemble weights Wk
are approximately uniform (Wk ~ 1/ K) the off-diagonal elements of the matrix
(ckl) dominate the generalization error for large K, and the contributions from the
training noises
are suppressed. For the special case where all students are identical and are trained on training sets of identical size, ak = (1 - c)a, the ensemble
generalization error is shown in Figure 1(left). The minimum at a nonzero value
of c, which is the fraction of the total data set held out for testing each student,
can clearly be seen. This confirms our intuition: when the students are trained
on smaller, less overlapping training sets, the increase in error of the individual
students can be more than offset by the corresponding increase in ambiguity.

n

The optimal training set sizes ak can be calculated analytically:
_

Ck

=1-

ak/ a

1 - Ak/(T2

= 1 + G(a, (T2) '

(12)

P. SOLLICH, A. KROGH

194
1.0 r - - - , - - - - - , r - - - . , - - - - , . - - - - : .

1.0 r - - - , - - - - - , - - - . - - - - r - - - - ""

0.8

0.8

0.6

0.6

w

.'

w
0.4

,...-------

0.2
/

/

0.0 /

0.0

/

0.2

0.4

0.6

,,
,
0.8

0.2

------,

1.0

0.0

0.0

C

...

0.2

0.4

0.6

0.8

1.0

C

Figure 1: Generalization error and ambiguity for an infinite ensemble of identical
students. Solid line: ensemble generalization error, fj dotted line: average generalization error of the individual students, l; dashed line: ensemble ambiguity, a.
For both plots a = 1 and (72 = 0.2 . The left plot corresponds to under-regularized
students with A = 0.05 < (72. Here the generalization error of the ensemble has
a minimum at a nonzero value of c. This minimum exists whenever>' < (72. The
right plot shows the case of over-regularized students (A = 0.3 > (72), where the
generalization error is minimal at c = O.
The resulting generalization error is f = (72G(a, (72) + 0(1/ K), which is the globally
minimal generalization error that can be obtained using all available training data,
as explained in Section 3. Thus, a large ensemble with optimally chosen training
set sizes can achieve globally optimal generalization performance. However, we see
from (12) that a valid solution Ck > 0 exists only for Ak < (72, i.e., if the ensemble
is under-regularized. This is exemplified, again for an ensemble of identical students, in Figure 1(right) , which shows that for an over-regularized ensemble the
generalization error is a: monotonic function of c and thus minimal at c = o.
We conclude this section by discussing how the adaptation of the training set sizes
could be performed in practice, for simplicity confining ourselves to an ensemble of
identical students, where only one parameter c = Ck = 1- ak/a has to be adapted.
If the ensemble is under-regularized one expects a minimum of the generalization
error for some nonzero c as in Figure 1. One could, therefore, start by training
all students on a large fraction of the total data set (corresponding to c ~ 0), and
then gradually and randomly remove training examples from the students' training
sets. Using (6), the generalization error of each student could be estimated by
their performance on the examples on which they were not trained, and one would
stop removing training examples when the estimate stops decreasing. The resulting
estimate of the generalization error will be slightly biased; however, for a large
enough ensemble the risk of a strongly biased estimate from systematically testing
all students on too 'easy' training examples seems small, due to the random selection
of examples.

5

REALISTIC ENSEMBLE SIZES

We now discuss some effects that occur in learning with ensembles of 'realistic' sizes.
In an over-regularized ensemble nothing can be gained by making the students more
diverse by training them on smaller, less overlapping training sets. One would also

195

Learning with Ensembles: How Overfitting Can Be Useful

Figure 2: The generalization error of
an ensemble with 10 identical students as a function of the test set
fraction c. From bottom to top the
curves correspond to training noise
T = 0,0.1,0.2, ... ,1.0. The star on
each curve shows the error of the optimal single perceptron (i. e., with optimal weight decay for the given T)
trained on all examples, which is independent of c. The parameters for
this example are: a = 1, A = 0.05,
0'2 = 0.2.

0.2
0.0 L - _ - - ' - _ - - - '_ _-'--_--'-_~
0.0
0.2
0.4
0.6
0.8
1.0
C

expect this kind of 'diversification' to be unnecessary or even counterproductive
when the training noise is high enough to provide sufficient 'inherent' diversity of
students. In the large ensemble limit, we saw that this effect is suppressed, but
it does indeed occur in finite ensembles. Figure 2 shows the dependence of the
generalization error on c for an ensemble of 10 identical, under-regularized students
with identical training noises Tk = T. For small T, the minimum of f. at nonzero c
persists. For larger T, f. is monotonically increasing with c, implying that further
diversification of students beyond that caused by the learning noise is wasteful. The
plot also shows the performance of the optimal single student (with A chosen to
minimize the generalization error at the given T), demonstrating that the ensemble
can perform significantly better by effectively averaging out learning noise.
For realistic ensemble sizes the presence of learning noise generally reduces the
potential for performance improvement by choosing optimal training set sizes. In
such cases one can still adapt the ensemble weights to optimize performance, again
on the basis of the estimate of the ensemble generalization error (6). An example is
1.0

1.0

,,

0.8

I
I

0.8
,-

,-

/

I

0.6

0.6

I

tV

tV

0.4 ..... -_ .................

0.4
0.2
... ....
0.0
0.001

---0.010

0.2

0.100

02

1.000

0.0
0.001

0.010

02

0.100

1.000

Figure 3: The generalization error of an ensemble of 10 students with different
weight decays (marked by stars on the 0'2-axis) as a function of the noise level
0'2. Left: training noise T = 0; right: T = 0.1. The dashed lines are for the
ensemble with uniform weights, and the solid line is for optimized ensemble weights.
The dotted lines are for the optimal single perceptron trained on all data. The
parameters for this example are: a = 1, c = 0.2.

196

P. SOu...ICH, A. KROGH

shown in Figure 3 for an ensemble of size 1< = 10 with the weight decays >'k equally
spaced on a logarithmic axis between 10- 3 and 1. For both of the temperatures T
shown, the ensemble with uniform weights performs worse than the optimal single
student. With weight optimization, the generalization performance approaches that
of the optimal single student for T = 0, and is actually better at T = 0.1 over
the whole range of noise levels rr2 shown. Even the best single student from the
ensemble can never perform better than the optimal single student, so combining the
student outputs in a weighted ensemble average is superior to simply choosing the
best member of the ensemble by cross-validation, i.e., on the basis of its estimated
generalization error. The reason is that the ensemble average suppresses the learning
noise on the individual students.

6

CONCLUSIONS

We have studied ensemble learning in the simple, analytically solvable scenario of
an ensemble of linear students. Our main findings are: In large ensembles, one
should use under-regularized students in order to maximize the benefits of the
variance-reducing effects of ensemble learning. In this way, the globally optimal
generalization error on the basis of all the available data can be reached by optimizing the training set sizes of the individual students. At the same time an estimate
of the generalization error can be obtained. For ensembles of more realistic size, we
found that for students subjected to a large amount of noise in the training process
it is unnecessary to increase the diversity of students by training them on smaller,
less overlapping training sets. In this case, optimizing the ensemble weights can
still yield substantially better generalization performance than an optimally chosen
single student trained on all data with the same amount of training noise. This
improvement is most insensitive to changes in the unknown noise levels rr2 if the
weight decays of the individual students cover a wide range. We expect most of these
conclusions to carryover, at least qualitatively, to ensemble learning with nonlinear
models, and this correlates well with experimental results presented in [7].

References
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]

C. Granger, Journal of Forecasting 8, 231 (1989).
D. Wolpert, Neural Networks 5, 241 (1992) .
L. Breimann, Tutorial at NIPS 7 and personal communication.
L. Hansen and P. Salamon, IEEE Trans. Pattern Anal. and Mach. Intell. 12,
993 (1990).
M. P. Perrone and L. N. Cooper, in Neural Networks for Speech and Image
processing, ed. R. J. Mammone (Chapman-Hall, 1993).
S. Hashem: Optimal Linear Combinations of Neural Networks. Tech. Rep .
PNL-SA-25166, submitted to Neural Networks (1995) .
A. Krogh and J. Vedelsby, in NIPS 7, ed. G. Tesauro et al., p. 231 (MIT Press,
1995).
R. Meir, in NIPS 7, ed. G. Tesauro et al., p. 295 (MIT Press, 1995).
A. Krogh and J. A. Hertz, J. Phys. A 25,1135 (1992).
J. A. Hertz, A. Krogh, and G. I. Thorbergsson, J. Phys. A 22, 2133 (1989).
P. Sollich, J. Phys. A 27, 7771 (1994).

"
1045,1995,SEEMORE: A View-Based Approach to 3-D Object Recognition Using Multiple Visual Cues,,1045-seemore-a-view-based-approach-to-3-d-object-recognition-using-multiple-visual-cues.pdf,Abstract Missing,"SEEMORE: A View-Based Approach to
3-D Object Recognition Using Multiple
Visual Cues
Bartlett W. Mel
Department of Biomedical Engineering
University of Southern California
Los Angeles, CA 90089
mel@quake.usc.edu

Abstract
A neurally-inspired visual object recognition system is described
called SEEMORE, whose goal is to identify common objects from
a large known set-independent of 3-D viewiag angle, distance,
and non-rigid distortion. SEEMORE's database consists of 100 objects that are rigid (shovel), non-rigid (telephone cord), articulated (book), statistical (shrubbery), and complex (photographs of
scenes). Recognition results were obtained using a set of 102 color
and shape feature channels within a simple feedforward network architecture. In response to a test set of 600 novel test views (6 of
each object) presented individually in color video images, SEEMORE
identified the object correctly 97% of the time (chance is 1%) using
a nearest neighbor classifier. Similar levels of performance were
obtained for the subset of 15 non-rigid objects. Generalization behavior reveals emergence of striking natural category structure not
explicit in the input feature dimensions.

1

INTRODUCTION

In natural contexts, visual object recognition in humans is remarkably fast, reliable,
and viewpoint invariant. The present approach to object recognition is ""view-based""
(e.g. see [Edelman and Bulthoff, 1992]), and has been guided by three main dogmas.
First, the ""natural"" object recognition problem faced by visual animals involves a
large number of objects and scenes, extensive visual experience, and no artificial

866

B. W.MEL

distinctions among object classes, such as rigid, non-rigid, articulated, etc.
Second, when an object is recognized in the brain, the ""heavy lifting"" is done by
the first wave of action potentials coursing from the retina to the inferotemporal
cortex (IT) over a period of 100 ms [Oram and Perrett, 1992]. The computations
carried out during this time can be modeled as a shallow but very wide feedforward
network of simple image filtering operations. Shallow means few processing levels,
wide means a sparse, high-dimensional representation combining cues from multiple
visual submodalities, such as color, texture, and contour [Tanaka et al., 1991].
Third, more complicated processing mechanisms, such as those involving focal attention, segmentation, binding, normalization, mental rotation, dynamic links, parts
recognition, etc., may exist and may enhance recognition performance but are not
necessary to explain rapid, robust recognition with objects in normal visual situations.

In this vein, the main goal of this project has been to explore the limits of performance of a shallow-but very wide-feedforward network of simple filtering operations
for viewpoint-invariant 3-D object recognition, where the filter ""channels"" themselves have been loosely modeled after the shape- and color-sensitive visual response
properties seen in the higher levels of the primate visual system [Tanaka et al., 1991].
Architecturally similar approaches to vision have been most often applied in the domain of optical character recognition [Fukushima et al., 1983, Le Cun et al., 1990].
SEEMORE'S architecture is also similar in spirit to the color histogramming approach
of [Swain and Ballard, 1991], but includes spatially-structured features that provide
also for shape-based generalization.

Figure 1: The database includes 100 objects of many different types, including rigid
(soup can), non-rigid (necktie), statistical (bunch of grapes), and photographs of
complex indoor and outdoor scenes.

SEEMORE: A View-based Approach to 3-D Object Recognition

2

867

SEEMORE'S VISUAL WORLD

SEEMORE's database contains 100 common 3-D objects and photogaphs of scenes,
each represented by a set of pre-segmented color video images (fig. 1). The training
set consisted of 12-36 views of each object as follows. For rigid objects, 12 training
views were chosen at roughly 60? intervals in depth around the viewing sphere, and
each view was then scaled to yield a total of three images at 67%, 100%, and 150%.
Image plane orientation was allowed to vary arbitrarily. For non-rigid objects, 12
training views were chosen in random poses.
During a recognition trial, SEEMORE was required to identify novel test images of
the database objects. For rigid objects, test images were drawn from the viewpoint
interstices of the training set, excluding highly foreshortened views (e.g. bottom of
can). Each test view could therefore be presumed to be correctly recognizable, but
never closer than roughly 30-> in orientation in depth or 22% in scale to the nearest
training view of the object, while position and orientation in the image plane could
vary arbitrarily. For non-rigid objects, test images consisted of novel random poses.
Each test view depicted the isolated object on a smooth background.

2.1

FEATURE CHANNELS

SEEMORE's internal representation of a view of an object is encoded by a set
of feature channels. The ith channel is based on an elemental nonlinear filter
fi(z, y, (h, (J2, .? .), parameterized by position in the visual field and zero or more
internal degrees of freedom. Each channel is by design relatively sensitive to changes
in the image that are strongly related to object identity, such as the object's shape,
color, or texture, while remaining relatively insensitive to changes in the image that
are unrelated to object identity, such as are caused by changes in the object's pose.
In practice, this invariance is achieved in a straightfOl'ward way for each channel by
subsampling and summing the output of the elemental channel filter over the entire
visual field and one or more of its internal degrees of freedom, giving a channel
output Fi = Lx,y,(h , .. . fiO. For example, a particular shape-sensitive channel might
""look"" for the image-plane projections of right-angle corners, over the entire visual
field, 360? of rotation in the image plane, 30? of rotation in depth, one octave in
scale, and tolerating partial occlusion and/or slight misorientation of the elemental
contours that define the right angle. In general, then, Fi may be viewed as a ""cell""
with a large receptive field whose output is an estimate of the number of occurences
of distal feature i in the workspace over a large range of viewing parameters.
SEEMORE'S architecture consists of 102 feature channels, whose outputs form an
input vector to a nearest-neighbor classifer. Following the design of the individual
channels, the channel vector F = {FI, ... F 102 } is (1) insensitive to changes in image
plane position and orientation of the object, (2) modestly sensitive to changes in
object scale, orientation in depth, or non-rigid deformation, but (3) highly sensitive
to object ""quality"" as pertains to object identity. Within this representation, total
memory storage for all views of an object ranged from 1,224 to 3,672 integers.
As shown in fig . 2, SEEMORE's channels fall into in five groups: (1) 23 color channels, each of which responds to a small blob of color parameterized by ""best"" hue
and saturation, (2) 11 coarse-scale intensity corner channels parameterized by open
angle, (3) 12 ""blob"" features, parameterized by the shape (round and elongated) and

868

B.

W.MEL

size (small, medium, and large) of bright and dark intensity blobs, (4) 24 contour
shape features, including straight angles, curve segments of varying radius, and parallel and oblique line combinations, and (5) 16 shape/texture-related features based
on the outputs of Gabor functions at 5 scales and 8 orientations. The implementations of the channel groups were crude, in the interests of achieving a working,
multiple-cue system with minimal development time. Images were grabbed using an
off-the-shelf Sony S-Video Camcorder and SunVideo digitizing board.
Colors

Blobs

Angles

o.

Contours

0.1

e.
oe
oe

.e

o

00

??
??
??

0

c

??

oe
o

=:>

Gabor-Based Features

./"" 1: -

sin2 +cos2 .......... 0 _
2

energy @ scale i
energy variance
@scalei

0

6
45 90

<30
>30

Figure 2: SEEMORE's 102 channels fall into 5 groups, sensitive to (1) colors, (2) intensity corners, (3) circular and elongated intensity blobs, (4) contour shape features,
and (5) 16 oriented-energy and relative-orientation features based on the outputs of
Gabor functions at several scales and orientations.

3

RECOGNITION

SEEMORE's recognition performance was assesed quantitatively as follows. A test
set consisting of 600 novel views (100 objects x 6 views) was culled from the database, and presented to SEEMORE for identification. It was noted empirically that
a compressive transform on the feature dimensions (histogram values) led to improved classification performance; prior to all learning and recognition operations,

SEEMORE: A View-based Approach to 3-D Object Recognition

869

Figure 3: Generalization using only shape-related channels. In each row, a novel
test view is shown at far left. The sequence of best matching training views (one
per object) is shown to right, in order of decreasing similarity.

therefore, each feature value was replaced by its natural logarithm (0 values were
first replaced with a small positive constant to prevent the logarithm from blowing
up). For each test view, the city-block distance was computed to every training view
in the database and the nearest neighbor was chosen as the best match. The log
transform of the feature dimension:.; thus tied this distance to the ratios of individual
feature values in two images rather than their differences.

4

RESULTS

Recognition time on a Sparc-20 was 1-2 minutes per view; the bulk of the time was
devoted to shape processing, with under 2 seconds required for matching.
Recognition results are reported as the proportion of test views that were correctly
classified. Performance using all 102 channels for the 600 novel object views in the
intact test set was 96.7%; the chance rate of correct classification was 1%. Across
recognition conditions, second-best matches usually accounted for approximately
half the errors. Results were broken down in terms of the separate contributions
to recognition performance of color-related vs. shape-related feature channels. Performance using only the 23 color-related channels was 87.3%, and using only the
79 shape-related channels was 79.7%. Remarkably, very similar performance figures
were obtained for the subset of 90 test views of the non-rigid objects, which included
several scarves, a bike chain, necklace, belt, sock, necktie, maple-leaf cluster, bunch
of grapes, knit bag, and telephone cord. Thus, a novel random configuration of a
telephone cord was as easily recognized as a novel view of a shovel.

870

5

B. W.MEL

GENERALIZATION BEHAVIOR

Numerical indices of recognition performance are useful, but do not explicitly convey
the similarity structure of the underlying feature space. A more qualitative but
extremely informative representation of system performance lies in the sequence of
images in order of increasing distance from a test view. Records of this kind are
shown in fig. 3 for trials in which only shape-related channels were used. In each, a
test view is shown at the far left, and the ordered set of nearest neighbors is shown
to the right. When a test view's nearest neighbor (second image from left) was not
the correct match, the trial was classified as an error.
As shown in row (1), a view of a book is judged most similar to a series of other books
(or the bottom of a rectangular cardboard box)---each a view of a rectangular object
with high-frequency surface markings. A similar sequence can be seen in subsequent
rows for (2) a series of cans, each a right cylinder with detailed surface markings, (3)
a series of smooth, not-quite-round objects, (4) a series of photographs of complex
scenes, and (5) a series of dinosaurs (followed by a teddy bear). In certain cases,
SEEMORE'S shape-related similarity metric was more difficult to visually interpret
or verbalize (last two rows), or was different from that of a human observer.

6

DISCUSSION

The ecology of natural object vision gives rise to an apparent contradiction: (i)
generalization in shape-space must in some cases permit an object whose global
shape has been grossly perturbed to be matched to itself, such as the various tangled
forms of a telephone cord, but (ii) quasi-rigid basic-level shape categories (e.g. chair,
shoe, tree) must be preserved as well, and distinguished from each other.
A partial It wi uti on to this conundrum lies in the observation tbat locally-cumputed
shape statistics are in large part preserved under the global shape deformations that
non-rigid common objects (e.g. scarf, bike-chain) typically undergo. A feature-space
representation with an emphasis on locally-derived shape channels will therefore
exhibit a significant degree of invariance to global nonrigid shape deformations. The
definition of shape similarity embodied in the present approach is that two objects
are similar if they contain similar profiles (histograms) of their shape measures,
which emphasize locality. One way of understanding the emergence of global shape
categories, then, such as ""book"", ""can"", ""dinosaur"", etc., is to view each as a set of
instances of a single canonical object whose local shape statistics remain quasi-stable
as it is warped into various global forms. In many cases, particularly within rigid
object categories, exemplars may share longer-range shape statistics as well.
It is useful to consider one further aspect of SEEMORE'S shape representation, pertaining to an apparent mismatch between the simplicity of the shape-related feature channels and the complexity of the shape categories that can emerge from
them. Specifically, the order of binding of spatial relations within SEEMORE's shape
channels is relatively low, i.e. consisting of single simple open or closed curves,
or conjunctions of two oriented contours or Gabor patches. The fact that shape
categories, such as ""photographs of rooms"", or ""smooth lumpy objects"", cluster
together in a feature space of such low binding order would therefore at first seem
surprising. This phenomenon relates closely to the notion of ""wickelfeatures"" (see
[Rumelhart and McClelland, 1986], ch. 18), in which features (relating to phonemes)

SEEMORE: A View-based Approach to 3-D Object Recognition

871

that bind spatial information only locally are nonetheless used to represent global
patterns (words) with little or no residual ambiguity.
The pre segmentation of objects is a simplifying assumption that is clearly invalid in
the real world. The advantage of the assumption from a methodological perspective
is that the object similarity structure induced by the feature dimensions can be
studied independently from the problem of segmenting or indexing objects imbedded
in complex scenes. In continuing work, we are pursuing a leap to sparse very-highdimensional space (e.g. 10,000 dimensions), whose advantages for classification in
the presence of noise (or clutter) have been discussed elsewhere [Kanerva, 1988,
Califano and Mohan, 1994].
Acknowledgements
Thanks to J6zsef Fiser for useful discusf!ions and for development of the Gabor-based
channel set, to Dan Lipofsky and Scott Dewinter for helping in the construction of
the image database, and to Christof Koch for providing support at Caltech where
this work was initiated. This work was funded by the Office of Naval Research, and
the McDonnell-Pew Foundation.

References
[Califano and Mohan, 1994] Califano, A. and Mohan, R. (1994). Multidimensional
indexing for recognizing visual shapes. IEEE Trans. on PAMI, 16:373-392.
[Edelman and Bulthoff, 1992] Edelman, S. and Bulthoff, H. (1992). Orientation dependence in the recognition of familiar and novel views of three-dimensional objects. Vision Res., 32:2385-2400.
[Fukushima et al., 1983] Fukushima, K., Miyake, S., and Ito, T. (1983). Neocognitron: A neural network model for a mechanism of visual pattern recognition.
IEEE Trans. Sys. Man & Cybernetics, SMC-13:826-834.
[Kanerva, 1988] Kanerva, P. (1988). Sparse distributed memory. MIT Press, Cambridge, MA.
[Le Cun et al., 1990] Le Cun, Y., Matan, 0., Boser, B., Denker, J., Henderson, D.,
Howard, R., Hubbard, W., Jackel, L., and Baird, H. (1990). Handwritten zip
code recognition with multilayer networks. In Proc. of the 10th Int. Conf. on
Patt. Rec. IEEE Computer Science Press.
[Oram and Perrett, 1992] Oram, M. and Perrett, D. (1992). Time course of neural
responses discriminating different views of the face and head. J. Neurophysiol.,
68(1) :70-84.
[Rumelhart and McClelland, 1986] Rumelhart, D. and McClelland, J. (1986). Parallel distributed processing. MIT Press, Cambridge, Massachusetts.
[Swain and Ballard, 1991] Swain, M. and Ballard, D. (1991). Color indexing. Int.
J. Computer Vision, 7:11-32.
[Tanaka et al., 1991] Tanaka, K., Saito, H., Fukada, Y., and Moriya, M. (1991).
Coding visual images of objects in the inferotemporal cortex of the macaque
monkey. J. Neurophysiol., 66:170-189.

PART VIII
APPLICATIONS

"
1046,1995,Analog VLSI Processor Implementing the Continuous Wavelet Transform,,1046-analog-vlsi-processor-implementing-the-continuous-wavelet-transform.pdf,Abstract Missing,"Analog VLSI Processor Implementing the
Continuous Wavelet Transform

R. Timothy Edwards and Gert Cauwenberghs
Department of Electrical and Computer Engineering
Johns Hopkins University
3400 North Charles Street
Baltimore, MD 21218-2686
{tim,gert}@bach.ece.jhu.edu

Abstract
We present an integrated analog processor for real-time wavelet decomposition and reconstruction of continuous temporal signals covering the
audio frequency range. The processor performs complex harmonic modulation and Gaussian lowpass filtering in 16 parallel channels, each clocked
at a different rate, producing a multiresolution mapping on a logarithmic
frequency scale. Our implementation uses mixed-mode analog and digital circuits, oversampling techniques, and switched-capacitor filters to
achieve a wide linear dynamic range while maintaining compact circuit
size and low power consumption. We include experimental results on the
processor and characterize its components separately from measurements
on a single-channel test chip.

1 Introduction
An effective mathematical tool for multiresolution analysis [Kais94], the wavelet transform
has found widespread use in various signal processing applications involving characteristic
patterns that cover multiple scales of resolution, such as representations of speech and vision.
Wavelets offer suitable representations for temporal data that contain pertinent features both
in the time and frequency domains; consequently, wavelet decompositions appear to be
effective in representing wide-bandwidth signals interfacing with neural systems [Szu92].
The present system performs a continuous wavelet transform on temporal one-dimensional
analog signals such as speech, and is in that regard somewhat related to silicon models
of the cochlea implementing cochlear transforms [Lyon88], [Liu92] , [Watt92], [Lin94].
The multiresolution processor we implemented expands on the architecture developed
in [Edwa93], which differs from the other analog auditory processors in the way signal
components in each frequency band are encoded. The signal is modulated with the center

Analog VLSI Processor Implementing the Continuous Wavelet Transform

1m

_: '\/'V

-I

~
x

Multiplier

s'(t)

LPF

set)

x(t)

693

x(t)

~ ~ yet)
Lff

LPF

~

~

get)

yet)

h(t)

h(t)

Prefilter

(a)

Multiplexer
(b)

Figure 1: Demodulation systems, (a) using multiplication, and (b) multiplexing.
frequency of each channel and subsequently lowpass filtered, translating signal components
taken around the center frequency towards zero frequency. In particular. we consider wavelet
decomposition and reconstruction of analog continuous-time temporal data with a complex
Gaussian kernel according to the following formulae:

Yk(t)

{teo x(e) exp (jWke- Q(Wk(t - e))2) de
(decomposition)

x'(t)

(1)

C 2::y\(t) exp(-jwkt)
k

(reconstruction)
where the center frequencies Wk are spaced on a logarithmic scale. The constant Q sets the
relative width of the frequency bins in the decomposition , and can be adjusted (together
with C) alter the shape of the wavelet kernel. Successive decomposition and reconstruction
transforms yield an approximate identity operation; it cannot be exact as no continuous
orthonormal basis function exists for the CWT [Kais94].

2

Architecture

The above operations are implemented in [Edwa93] using two demodulator systems per
channel, one for the real component of (1), and another for the imaginary component, 90?
out of phase with the first. Each takes the form of a sinusoidal modulator oscillating at
the channel center frequency, followed by a Gaussian-shaped lowpass filter, as shown in
Figure 1 (a). This arrangement requires a precise analog sine wave generator and an accurate
linear analog multiplier. In the present implementation, we circumvent both requirements
by using an oversampled binary representation of the modulation reference signal.

2.1

Multiplexing vs. MUltiplying

Multiplication of an analog signal x(t) with a binary (? 1) sequence is naturally implemented
with high precision using a mUltiplexer, which alternates between presenting either the
input or its inverse -x(t) to the output. This principle is applied to simplify harmonic
modulation. and is illustrated in Figure 1 (b). The multiplier has been replaced by an analog
inverter followed by a multiplexer, where the multiplexer is controlled by an oversampled
binary periodic sequence representing the sine wave reference. The oversampled binary
sequence is chosen to approximate the analog sine wave as closely as possible. disregarding
components at high frequency which are removed by the subsequent lowpass filter. The
assumption made is that no high frequency components are present in the input signal

R. T. EDWARDS, G. CAUWENBERGHS

694
SiglUll

,---- ? ________ ----I In

,,:

In Seltrt

CLK2

eLK]

f ---<.-iK4----1:CLKS-- -: ;----- -- ---- ---- --1

eLK/:

,,

II

E

?

I

II

""

I'

'I

.---f-..., ::

::

Ret'onJlructed :

: cmLy,

0.,.

.--...L..--,::
I

,

I

:

~~--+,~,

'-==:.J

,

: ~ ____ __________ __J

'

I. ________________ :

Reconstruction Input

i

:,

""

Wav elet
Reconstruction

Mult;pl;er

Gaussian Filter

Output Mux ing

Figure 2: Block diagram of a single channel in the wavelet processor, showing test points
A through E.
under modulation, which otherwise would convolve with corresponding high frequency
components in the binary sequence to produce low frequency distortion components at the
output. To that purpose, an additionallowpass filter is added in front of the multiplexer.
Residual low-frequency distortion at the output is minimized by maximizing roll-off of the
filters, placing proper constraints on their cutofffrequencies, and optimally choosing the bit
sequence in the oversampled reference [Edwa95]. Clearly, the signal accuracy that can be
achieved improves as the length N of the sequence is extended. Constraints on the length
N are given by the implied overhead in required signal bandwidth, power dissipation, and
complexity of implementation.

2.2

Wavelet Gaussian Function

The reason for choosing a Gaussian kernel in (l) is to ensure optimal support in both
time and frequency [Gros89]. A key requirement in implementing the Gaussian filter
is linear phase, to avoid spectral distortion due to non-uniform group delays. A worryfree architecture would be an analog FIR filter; however the number of taps required to
accommodate the narrow bandwidth required would be prohibitively large for our purpose.
Instead, we approximate a Gaussian filter by cascading several first-order lowpass filters .
From probabilistic arguments, the obtained lowpass filter approximates a Gaussian filter
increasingly well as the number of stages increases [Edwa93] .

3

Implementation

Two sections of a wavelet processor, each containing 8 parallel channels, were integrated
onto a single 4 mm x 6 mm die in 2 /lm CMOS technology. Both sections can be configured
to perform wavelet decomposition as well as reconstruction. The block diagram for one
of the channels is shown in Figure 2. In addition, a separate test chip was designed which
performs one channel of the wavelet function . Test points were made available at various
points for either input or output, as indicated in boldface capitals, A through E, in Figure 2.
Each channel performs complex harmonic modulation and Gaussian lowpass filtering, as
defined above. At the front end of the chip is a sample-and-hold section to sample timemultiplexed wavelet signals for reconstruction . In cases of both signal decomposition
and reconstruction, each channel removes the input DC component removed, filters the
result through the premultiplication lowpass (PML) filter, inverts the result, and passes
both non-inverted and inverted signals onto the multiplexer. The multiplexer output is
passed through a postmultiplication lowpass filter (PML, same architecture) to remove high
frequency components of the oversampled sequence, and then passed through the Gaussianshaped lowpass filter. The cutoff frequencies of all filters are controlled by the clock rates

695

Analog VLSI Processor Implementing the Continuous Wavelet Transform

(CLKI to CLK4 in Figure 2). The remainder of the system is for reconstruction and for
time-multiplexing the output.

3.1

MUltiplier

The multiplier is implemented by use of the above multiplexing scheme, driven by an
oversampled binary sequence representing a sine wave. The sequence we used was 256
samples in length, created from a 64-sample base sequence by reversal and inversion . The
sequence length of256 generates a modulator wave of 4 kHz (useful for speech applications)
from a clock of about 1 MHz.
We derived a sequence which, after postfiltering through a 3rd-order lowpass filter of the
fonn of the PML prefilter (see below), produces a sine wave in which all hannonics are
more than 60 dB down from the primary [Edwa95]. The optimized 64-bit base sequence
consists of 11 zeros and 53 ones, allowing a very simple implementation in which an address
decoder decodes the ""zero"" bits. The binary sequence is shown in Figure 4. The magnitude
of the prime hannonic of the sequence is approximately 1.02, within 2% of unity.
The process of reversing and inverting the sequence is simplified by using a gray code
counter to produce the addresses for the sequence, with only a small amount of combinatorial
logic needed to achieve the desired result [Edwa95]. It is also straightforward to generate
the addresses for the cosine channel, which is 90? out of phase with the original.

3.2 Linear Filtering
All filters used are implemented as linear cascades of first-order, single-pole filter sections.
The number of first-order sections for the PML filters is 3. The number of sections for the
""Gaussian"" filter is 8, producing a suitable approximation to a Gaussian filter response for
all frequencies of interest (Figure 5).
Figure 3 shows one first-order lowpass section of the filters as implemented. This standard

>-.+--o

v,,,

va""'

+

Figure 3: Single discrete-time lowpass filter section.
switched-capacitor circuit implements a transfer function containing a single pole, approximately located in the Laplace domain at s = Is / a for large values of the parameter a, with
Is being the sampling frequency. The value for this parameter a is fixed at the design stage
as the ratio of two capacitors in Figure 3, and was set to be 15 for the The PML filters and
12 for the Gaussian filters.

4 Measured Results
4.1

Sine wave modulator

We tested the accuracy of the sine wave modulation signal by applying two constant voltages
at test points A and B, such that the sine wave modulation signal is effectively multiplied

R. T. EDWARDS, G. CAUWENBERGHS

696

Sine sequence and filtered sine wave output

Binary sine sequence
Simulated filtered output
x
Measured output
-1.5 L -_ _ _- - '_ _ _ _- ' -_ _ _ _........_ _ _ _- ' -_ _ _ _.J...J

o

50

100

150

200

250

Time (us)

Figure 4: Filtered sine wave output.

by a constant. The output of the mUltiplier is filtered and the output taken at test point D,
before the Gaussian filter. Figure 4 shows the (idealized) multiplexer output at test point
C, which accurately creates the desired binary sequence. Figure 4 also shows the measured
sine wave after filtering with the PML filter and the expected output from the simulation
model, using a deviating value of 8.0 for the capacitor ratio a, as justified below. FFT
analysis of Figure 4 has shown that the resulting sine wave has all harmonics below about
-49 dB . This is in good agreement with the simulation model, provided a correction is made
for the value of the capacitor ratio a to account for fringe and (large) parasitic capacitances.
The best fit for the measured data from the postmultiplication filter is a = 8.0, compared to
the desired value of a = 15.0. The transform of the simulated output shown in the figure
takes into account the smaller value of a. Because the postmultiplication filter is followed
by the Gaussian filter, the bandwidth of the output can be directly controlled by proper
clocking ofthe Gaussian filter, so the distortion in the sine wave is ultimately much smaller
than that measured at the output of the postmultiplication filter.

4.2

Gaussian filter

The Gaussian filter was tested by applying a signal at test point D and measuring the
response at test point E. Figure 5 shows the response of the Gaussian filter as compared to
expected responses. There are two sets of curves, one for a filter clocked at 64 kHz, and the
other clocked at 128 kHz; these curves are normalized by plotting time relative to the clock
frequency is . The solid line indicates the best match for an 8th-order lowpass filter, using
the capacitor ratio, a, as a fitting parameter. The best-fit value of a is approximately 6.8.
This is again much lower than the capacitor area ratio of 12 on the chip. The dotted line is
the response of the ideal Gaussian characteristic exp ( _w 2 / (2aw~)) approximated by the
cascade of first-order sections with capacitor ratio a.
Figure 5 (b) shows the measured phase response of the Gaussian filter for the 128 kHz
clock. The phase response is approximately linear throughout the passband region.

Analog VLSI Processor Implementing the Continuous Wavelet Transform

697

Gaussian filter response

o~~~~--~-=~~~~~~~--~

x
o

x
0

iii'-10
~

Chip data at 64kHz clock
Chip data at 128kHz clock
8th-order filter ideal response
Gaussian filter ideal response

500

<lJ

]1

-20

~

?E

-30

""0
<lJ

o

N

~ -40

Theoretical 8-stage phase
Measured response

?

o
Z -50
0.01

0.07

Frequency (units fs)

0.08

0.0 I

0.02

0.03

0.04

0.05

0.06

0.07

Frequency (units fs)

Figure 5: Gaussianfilter transfer functions: theoretical and actual. (a) Relative amplitude;
(b) Phase.

4.3

Wavelet decomposition

Figure 6 shows the test chip performing a wavelet transform on a simple sinusoidal input,
illustrating the effects of (oversampled) sinusoidal modulation followed by lowpass filtering
through the Gaussian filter. The chip multiplier system is clocked at 500 kHz. The input
wave is approximately 3.1 kHz, close to the center frequency of the modulator signal,
which is the clock rate divided by 128, or about 3.9 kHz (a typical value for the highestfrequency channel in an auditory application). The top trace in the figure shows the filtered
and inverted input, taken from test point B. The middle trace shows the output of the
multiplexer (test point C), wherein the output is multiplexed between the signal and its
inverse. The bottom trace is taken from the system output (labeled Cosine Out in Figure 2)
and shows the demodulated signal of frequency 800 Hz (= 3.9 kHz - 3.1 kHz). Not shown
is the cosine output, which is 90? out of phase with the one shown. This demonstrates
the proper operation of complex demodulation in a single channel configured for wavelet
decomposition. In addition, we have tested the full16-channel chip decomposition, and all
individual parts function properly. The total power consumption of the 16-channel wavelet
chip was measured to be less than 50mW, of which a large fraction can be attributed to
external interfacing and buffering circuitry at the periphery of the chip.

5

Conclusions

We have demonstrated the full functionality of an analog chip performing the continuous
wavelet transform (decomposition). The chip is based on mixed analog/digital signal
processing principles, and uses a demodulation scheme which is accurately implemented
using oversampling methods. Advantages of the architecture used in the chip are an
increased dynamic range and a precise control over lateral synchronization of wavelet
components. An additional advantage inherent to the modulation scheme used is the
potential to tune the channel bandwidths over a wide range, down to unusually narrow
bands, since the cutoff frequency of the Gaussian filter and the center frequency of the
modulator are independently adjustable and precisely controllable parameters.

References
G. Kaiser, A Friendly Guide to Wavelets, Boston, MA: Birkhauser, 1994.

T. Edwards and M. Godfrey, ""An Analog Wavelet Transform Chip,"" IEEE Int'l Can! on

0.08

698

R. T. EDWARDS, G. CAUWENBERGHS

Figure 6: Scope trace of the wavelet transform: filtered input (top), multiplexed signal
(middle), and wavelet output (bottom).
Neural Networks, vol. III, 1993, pp. 1247-1251.
T. Edwards and G. Cauwenberghs, ""Oversampling Architecture for Analog Harmonic

Modulation,"" to appear in Electronics Letters, 1996.
A Grossmann, R Kronland-Martinet, and J. MorIet, ""Reading and understanding continuous wavelet transforms,"" Wavelets: Time-Frequency Methods and Phase Space. SpringerVerlag, 1989, pp. 2-20.
W. Liu, AG. Andreou, and M.G. Goldstein, ""Voiced-Speech Representation by an Analog
Silicon Model ofthe Auditory Periphery,"" IEEE T. Neural Networks, vol. 3 (3), pp 477-487,
1992.
J. Lin, W.-H. Ki, T. Edwards, and S. Shamma, ""Analog VLSI Implementations of Auditory Wavelet Transforms Using Switched-Capacitor Circuits,"" IEEE Trans. Circuits and
Systems-I, vol.41 (9), pp. 572-583, September 1994.
A Lu and W. Roberts, ''A High-Quality Analog Oscillator Using Oversampling D/A Conversion Techniques,"" IEEE Trans. Circuits and Systems-II, vol.41 (7), pp. 437-444, July
1994.

RF. Lyon and C.A Mead, ""An Analog Electronic Cochlea,"" IEEE Trans. Acoustics, Speech
and Signal Proc., vol. 36, pp 1119-1134, 1988.
H.H. Szu, B. Tefter, and S. Kadembe, ""Neural Network Adaptive Wavelets for Signal Representation and Classification,"" Optical Engineering, vol. 31 (9), pp. 1907-1916, September
1992.
L. Watts, D.A Kerns, and RF. Lyon, ""Improved Implementation of the Silicon Cochlea,""
IEEE Journal of Solid-State Circuits, vol. 27 (5), pp 692-700,1992.

"
1047,1995,Selective Attention for Handwritten Digit Recognition,,1047-selective-attention-for-handwritten-digit-recognition.pdf,Abstract Missing,"Selective Attention for Handwritten
Digit Recognition

Ethem Alpaydm
Department of Computer Engineering
Bogazi<1i U ni versi ty
Istanbul, TR-SOS15 Turkey
alpaydin@boun.edu.tr

Abstract
Completely parallel object recognition is NP-complete. Achieving
a recognizer with feasible complexity requires a compromise between parallel and sequential processing where a system selectively
focuses on parts of a given image, one after another. Successive
fixations are generated to sample the image and these samples are
processed and abstracted to generate a temporal context in which
results are integrated over time. A computational model based on a
partially recurrent feedforward network is proposed and made credible by testing on the real-world problem of recognition of handwritten digits with encouraging results.

1

INTRODUCTION

For all-parallel bottom-up recognition, allocating one separate unit for each possible
feature combination, i.e., conjunctive encoding, implies combinatorial explosion. It
has been shown that completely parallel, bottom-up visual object recognition is
NP-complete (Tsotsos, 1990). By exchanging space with time, systems with much
less complexity may be designed. For example, to phone someone at the press of a
button, one needs 10 7 buttons on the phone; the sequential alternative is to have
10 buttons on the phone and press one at a time, seven times.
We propose recognition based on selective attention where we analyze only a small
part of the image in detail at each step, combining results in time. N oton and Stark's
(1971) ""scanpath"" theory advocates that each object is internally represented as a
feature-ring which is a temporal sequence of features extracted at each fixation and
the positions or the motor commands for the eye movements in between. In this
approach, there is an ""eye"" that looks at an image but which can really see only a
small part of it. This part of the image that is examined in detail is the fovea. The

772

E. ALPAYDIN

ASSOCIATIVE

Class Probabilities
(lOx!)

LEVEL

P~r------7-""7

L-L_ _ _ _ _ _/ '

t

softmax
Class Units
(lOxI) 0 /

7

T1

Hidden Units (s x I)
H L..../~_....;...._-_-_-_-~_-_-_-_-_-~_-_-~7-.""

I~-----------------;t~---------- ----;;------------------PRE-ATTENTIVE LEVEL

ATTENTIVE LEVEL

,-------------------- -----,

------

-------------------,

I

:
F

Feature Map
I
(rxI):

Eye Position Map
(pxp)

//p
1

Fovea
1-------'---1-

~

-I

WTA

subsample
and blur

I

- -:- - - - - - - ~

Saliency Map
(n x n)

M
Bitmap Image (n x n)

Figure 1: The block diagram of the implemented system.

fovea's content is examined by the pre-attentive level where basic feature extraction
takes place. The features thus extracted are fed to an a660ciative part together
with the current eye position. If the accumulated information is not sufficient for
recognition, the eye is moved to another part of the image, making a saccade. To
minimize recognition time, the number of saccades should be minimized. This is
done through defining a criterion of being ""interesting"" or saliency and by fixating
only at the most interesting. Thus sucessive fixations are generated to sample the
image and these samples are processed and abstracted to generate a temporal context in which results are integrated over time. There is a large amount of literature
on selective attention in neuroscience and psychology; for reviews see respectively
(Posner and Peterson, 1990) and (Treisman, 1988). The point stressed in this paper
is that the approach is also useful in engineering.

2

AN EXAMPLE SYSTEM FOR OCR

The structure of the implemented system for recognition of handwritten digits is
given in Fig. 1.

Selective Attention for Handwritten Digit Recognition

773

We have an n x n binary image in which the fovea is m x m with m < n. To
minimize recognition time, the system should only attend to the parts of the image
that carry discriminative information. We define a criterion of being ""interesting""
or saliency which is applied to all image locations in parallel to generate a 8aliency
map, S. The saliency measure should be chosen to draw attention to parts that
have the highest information content. Here, the saliency criterion is a low-pass filter
which roughly counts the number of on pixels in the corresponding m x m region
of the input image M. As the strokes in handwritten digits are mostly one or two
pixels wide, a count of the on pixels is a good measure of the discontinuity (and
thus information). It is also simple to compute:

i+lm/2J
Sij =

L

HLm/2J

L

MkIN2 ((i,jl, (Lm/6J)2 *1), i,j = 1. .. n

k=i-Lm/2J l=j-Lm/2J
where N 2 (p., E) is the bivariate normal with mean p. and the covariance E. Note
that we want the convolution kernel to have effect up to Lm/2J and also that the
normal is zero after p.? 30-. In our simulations where n is 16 and m is 5 (typical for
digit recognition), 0- ~ 1. The location that is most salient is the position ofthe next
fixation and as such defines the new center of the fovea. A location once attended
to is no longer interesting; after each fixation, the saliency of all the locations that
currently are in the scope of the fovea are set to 0 to inhibit another fixation there.
The attentive level thus controls the scope of the pre-attentive level. The maximum
of the saliency map through a winner-take-all gives the eye position (i*, j*) at
fixation t.
(i*(t),j*(t))
arg~B:XSij
',J
By thus following the salient regions, we get an input-dependent emergent sequence
in time.

=

Eye-Position Map
The eye p08ition map, P, stores the position of the eye in the current fixation. It is
p x p. p is chosen to be smaller than n for dimensionality reduction for decreasing

complexity and introducing an effect of regularization (giving invariance to small
translations). When p is a factor of n, computations are also simpler. We also blur
the immediate neighbors for a smoother representation:

P( t)

= blur(subsample( winner-take-all( S)))

Pre-Attentive Level: Feature Extraction
The pre-attentive level extracts detailed features from the fovea to generate a feature
map. This information and the current eye position is passed to the associative
system for recognition. There is a trade-off between the fovea size and the number
of saccades required for recognition: As the operation in the pre-attentive level is
carried out in parallel, to minimize complexity the features extracted there should
not be many and the fovea should not be large: Fovea is where the expensive
computation takes place. On the other hand, the fovea should be large enough to
extract discriminative features and thus complete recognition in a small amount of
time. The features to be extracted can be learned through an supervised method
when feedback is available .

774

E. ALPAYDIN

The m x m region symmetrically around (i*, j*) is extracted as the fovea I and is
fed to the feature extractors. The r features extracted there are passed on to the
associative level as the feature map, F. r is typically 4 to 8. Ug denote the weights
of feature 9 and Fg is the value of feature 9 that is found by convolving the fovea
input with the feature weight vector (1(.) is the sigmoid function):

M i o(t)-Lm/2J+i,jo(t)-Lm/2J+j, i,j = 1 ... m

f (

~ ~ U""jI,j(t?) , g = 1. ..

r

Associative Level: Classification
At each fixation, the associative level is fed the feature map from the pre-attentive
level and the eye position map from the attentive level. As a number of fixations
may be necessary to recognize an image, the associative system should have a shortterm memory able to accumulate inputs coming through time. Learning similarly
should be through time. When used for classification, the class units are organized
so as to compete and during recognition the activations of the class units evolve
till one class gets sufficiently active and suppresses the others. When a training
set is available, a temporal supervised method can be used to train the associative
level. Note that there may be more than one scanpath for each object and learning
one sequence for each object fails. We see it is a task of accumulating two types of
information through time: the ""what"" (features extracted) and the ""where"" (eye
position).
The fovea map, F, and the eye position map, P, are concatenated to make a
r + p X P dimensional input that is fed to the associative level. Here we use an
artificial neural network with one hidden layer of 8 units. We have experimented
with various architectures and noticed that recurrency at the output layer is the
best. There are 10 output units.

f (L VhgFg(t) + L L WhabPab(t)) , h =

1. .. s

gab

LTchHh + L RckPk(t - 1), c = 1. .. 10
h

k

exp[Oc(t)]
Lk exp[Ok(t)]
where P denotes the ""softmax""ed output probabilities (Bridle, 1990) and P(t - 1)
are the values in the preceding fixation (initially 0). We use the cross-entropy as
the goodness measure:
C=

L
t

1

t L Dk 10gPc(t), t ~

1

c

Dc is the required output for class c. Learning is gradient-ascent on this goodness
measure. The fraction lit is to give more weight to initial fixations than later ones.
Connections to the output units are updated as follows (11 is the learning factor):

Selective Attention for Handwritten Digit Recognition

Note that we assume 8PIc(t -1)/8Rc lc =
we have:

o.

775

For the connections to the hidden units

c

We can back-propagate one step more to train the feature extractors. Thus the
update equations for the connections to feature units are:

Cg(t) =

L Ch(t)Vhg
h

A series of fixations are made until one of the class units is sufficiently active:
3c, Pc > 8 (typically 0.99), or when the most salient point has a saliency less than a
certain threshold (this condition is rarely met after the first few epochs). Then the
computed changes are summed up and the updates are made like the exaple below:

Backpropagation through time where the recurrent connections are unfolded in time
did not work well in this task because as explained before, for the same class, there is
more than one scanpath. The above-mentioned approach is like real-time recurrent
learning (Williams and Zipser, 1989) where the partial derivatives in the previous
time step is 0, thus ignoring this temporal dependence.

3

RESULTS AND DISCUSSION

We have experimented with various parameter settings and finally chose the architecture given above: When input is 16 x 16 and there are 10 classes, the fovea is
5 x 5 with 8 features and there are 16 hidden units. There are 1,934 images for
training, 946 for cross-validation and 943 for testing. Results are given in Table
1. ( It can be seen that by scanning less than half of the image, we get 80% generalization. Additional to the local high-resolution image provided by the fovea, a
low-resolution image of the surrounding parafovea can be given to the associative
level for better recognition. For example we low-pass filtered and undersampled the
original image to get a 4 x 4 image which we fed to the class units additional to
the attention-based hidden units. Success went up quite high and fewer fixations
were necessary; compare rows 1 and 2 of the Table. The information provided by
the 4 x 4 map is actually not much as can be seen from row 3 of the table where
only that is given as input. Thus the idea is that when we have a coarse input,
looking only at a quarter of the image in detail is sufficient to get 93% accuracy.
Both features (what) and eye positions (where) are necessary for good recognition.
When only one is used without the other, success is quite low as can be seen in rows
4 and 5. In the last row, we see the performance of a multi layer percept ron with
10 hidden units that does all-parallel recognition.
Beyond a certain network size, increasing the number of features do not help much.
Decreasing 8, the certainty threshold, decreases the number of fixations necessary

776

E. ALPAYDIN

Table 1: Results of handwritten digit recognition with selective attention. Values
given are average and standard deviation of 10 independent runs. See text for
comments.
NO OF
PARAMS

TEST
SUCCESS

TRAINING
EPOCHS

NO OF
FIXATIONS

SA system
SA+parafovea
Only parafovea
Only what info
Only where info

878
1,038
170
622
440

79.7, 1.8
92.5,0.8
86.9,0.2
49.0,21.0
54.2, 1.4

74.5, 17.1
54.2, 10.2
52.3,8.2
66.6, 30.6
92.9,6.5

6.5,0.2
3.9,0.3
1.0, 0.0
7.5,0.1
7.6,0.0

MLP, 10 hiddens

2,680

95.1, 0.6

13.5,4.1

1.0,0.0

METHOD

which we want, but decreases success too which we don't. Smaller foveas decrease
the number of free parameters but decrease success and require a larger number
of fixations. Similarly larger foveas decrease the number of fixations but increase
complexity.
The simple low-pass filter used here as a saliency measure is the simplest measure.
Previously it has been used by Fukushima and Imagawa (1993) for finding the next
character, i.e., segmentation, and also by Olshausen et al. (1992) for translation
invariance. More robust measures at the expense of more computations, are possible; see (Rimey and Brown, 1990; Milanese et al., 1993). Salient regions are those
that are conspicious, i.e., different from their surrounding where there is a change
in X where X can be brightness or color (edges), orientation (corners), time (motion), etc. It is also possible that top-down, task-dependent saliency measures be
integrated to minimize further recognition time implying a remembered explicit
sequence analogous to skilled motor behaviour (probably gained after many repetitions).
Here a partially recurrent network is used for temporal processing. Hidden Markov
Models like used in speech recognition are another possibility (Rimey and Brown,
1990; Haclsalihzade et al., 1992). They are probabilistic finite automata which can
be trained to classify sequences and one can have more than one model for an object.
It should be noted here that better approaches for the same problem exists (Le Cun
et al., 1989). Here we advocate a computational model and make it plausible by
testing it on a real-world problem. It is necessary for more complicated problems
where an all-parallel approach would not work. For example Le Cun et al. 's model
for the same type of inputs has 2,578 free parameters. Here there are

(mx m+1) x r+(r+pxp+ 1) x 8+(S+ 1) x 10+10 x 10
,

#'

iT

v';w

#~~

T

R

free parameters which make 878 when m = 5, r = 8, S = 16. This is the main
advantage of selective attention which is that the complexity of the system is heavily
reduced at the expense of slower recognition, both in overt form of attention through
foveation and in its covert form, for binding features - For this latter type of
attention not discussed here, see (Ahmad, 1992). Also note that low-level feature
extraction operations like carried out in the pre-attentive level are local convolutions

Selective Attention for Handwritten Digit Recognition

777

and are appropriate for parallel processing, e.g., on a SIMD machine. Higherlevel operations require larger connectivity and are better carried out sequentially.
Nature also seems to have taken this direction.
Acknowledgements
This work is supported by Tiibitak Grant EEEAG-143 and Bogazi<;;i University
Research Funds 95HA108. Cenk Kaynak prepared the handwritten digit database
based on the programs provided by NIST (Garris et al., 1994).
References
S. Ahmad. (1992) VISIT: A Neural Model of Covert Visual Attention. In J. Moody,
S. Hanson, R. Lippman (Eds.) Advances in Neural Information Processing Systems
4,420-427. San Mateo, CA: Morgan Kaufmann.
J.S. Bridle. (1990) Probabilistic Interpretation of Feedforward Classification Network Outputs with Relationships to Statistical Pattern Recognition. In Neurocomputing, F. Fogelman-Soulie, J. Herault, Eds. Springer, Berlin, 227-236.
K. Fukushima, T. Imagawa. (1993) Recognition and Segmentation of Connected
Characters with Selective Attention, Neural Networks, 6: 33-41.
M.D. Garris et al. (1994) NIST Form-Based Handprint Recognition System, NISTIR 5469, NIST Computer Systems Laboratory.
S.S. Haclsalihzade, L.W. Stark, J .S. Allen. (1992) Visual Perception and Sequences
of Eye Movement Fixations: A Stochastic Modeling Approach, IEEE SMC, 22,
474-481.
Y. Le Cun et al. (1991) Handwritten Digit Recognition with a Back-Propagation
Network. In D.S. Touretzky (ed.) Advances in Neural Information Processing
Systems 2, 396-404. San Mateo, CA: Morgan Kaufmann.
R. Milanese et al. (1994) Integration of Bottom-U p and Top- Down Cues for Visual
Attention using Non-Linear Relaxation IEEE Int'l Conf on CVPR, Seattle, WA,
USA.

D. Noton and L. Stark. (1971) Eye Movements and Visual Perception, Scientific
American, 224: 34-43.
B. Olshausen, C. Anderson, D. Van Essen. (1992) A Neural Model of Visual Attention and Invariant Pattern Recognition, CNS Memo 18, CalTech.
M.L Posner, S.E. Petersen. (1990) The Attention System of the Human Brain,
Ann. Rev. Neurosci., 13:25-42.
R.D. Rimey, C.M. Brown. (1990) Selective Attention as Sequential Behaviour: Modelling Eye Movements with an Augmented Hidden Markov Model, TR-327, Computer Science, Univ of Rochester.
A. Treisman. (1988) Features and Objects, Quarterly Journ. of Ezp . Psych., 40:
201-237.
J.K. Tsotsos. (1990) Analyzing Vision at the Complexity Level, Behav. and Brain
Sci. 13: 423-469.
R.J. Williams, D. Zipser. (1989) A Learning Algorithm for Continually Running
Fully Recurrent Neural Networks Neural Computation, 1, 270-280.

"
1048,1995,Gaussian Processes for Regression,,1048-gaussian-processes-for-regression.pdf,Abstract Missing,"Gaussian Processes for Regression

Christopher K. I. Williams
Neural Computing Research Group
Aston University
Birmingham B4 7ET, UK

Carl Edward Rasmussen
Department of Computer ,Science
University of Toronto
Toronto , ONT, M5S lA4, Canada

c.k.i.williams~aston.ac.uk

carl~cs.toronto.edu

Abstract
The Bayesian analysis of neural networks is difficult because a simple prior over weights implies a complex prior distribution over
functions . In this paper we investigate the use of Gaussian process
priors over functions, which permit the predictive Bayesian analysis for fixed values of hyperparameters to be carried out exactly
using matrix operations. Two methods, using optimization and averaging (via Hybrid Monte Carlo) over hyperparameters have been
tested on a number of challenging problems and have produced
excellent results.

1

INTRODUCTION

In the Bayesian approach to neural networks a prior distribution over the weights
induces a prior distribution over functions. This prior is combined with a noise
model, which specifies the probability of observing the targets t given function
values y, to yield a posterior over functions which can then be used for predictions.
For neural networks the prior over functions has a complex form which means
that implementations must either make approximations (e.g. MacKay, 1992) or use
Monte Carlo approaches to evaluating integrals (Neal , 1993) .
As Neal (1995) has argued , there is no reason to believe that, for real-world problems, neural network models should be limited to nets containing only a ""small""
number of hidden units . He has shown that it is sensible to consider a limit where
the number of hidden units in a net tends to infinity, and that good predictions can
be obtained from such models using the Bayesian machinery. He has also shown
that a large class of neural network models will converge to a Gaussian process prior
over functions in the limit of an infinite number of hidden units.
In this paper we use Gaussian processes specified parametrically for regression problems. The advantage of the Gaussian process formulation is that the combination of

515

Gaussian Processes for Regression

the prior and noise models can be carried out exactly using matrix operations. We
also show how the hyperparameters which control the form of the Gaussian process
can be estimated from the data, using either a maximum likelihood or Bayesian
approach, and that this leads to a form of ""Automatic Relevance Determination""
(Mackay 1993j Neal 1995).

2

PREDICTION WITH GAUSSIAN PROCESSES

A stochastic process is a collection of random variables {Y (x) Ix EX} indexed by a
set X. In our case X will be the input space with dimension d, the number of irlputs .
The stochastic process is specified by giving the probability distribution for every
finite subset of variables Y(x(1)), . .. , Y(x(k)) in a consistent manner. A Gaussian
process is a stochastic process which can be fully specified by its mean function
J.1.(:x:) = E[Y(x)] and its covariance function C(X , X/) = E[(Y(x) - J.1.(x))(Y(x /)J.1.( Xl))]; any finite set of points will have a joint multivariate Gaussian distribution.
Below we consider Gaussian processes which have J.1.( x) == O.
In section 2.1 we will show how to parameterise covariances using hyperparametersj
for now we consider the form of the covariance C as given. The training data
consists of n pairs of inputs and targets {( xCi) , t(i)) , i = 1 .. . n} . The input vector
for a test case is denoted x (with no superscript). The inputs are d-dimensional
Xl, . .. , Xd and the targets are scalar.
The predictive distribution for a test case x is obtained from the n + 1 dimensional
joint Gaussian distribution for the outputs of the n training cases and the test
case, by conditioning on the observed targets in the training set. This procedure is
illustrated in Figure 1, for the case where there is one training point and one test
point. In general, the predictive distribution is Gaussian with mean and variance

k T (x)K- 1t

(1)

C(x,x) - kT (x)K- 1 k(x),

(2)

where k(x) = (C(x, x(1)), ... , C(x, x(n))f , K is the covariance matrix for the
training cases Kij = C(x(i), x(j)), and t = (t(l), ... , t(n))T .
The matrix inversion step in equations (1) and (2) implies that the algorithm has
O( n 3 ) time complexity (if standard methods of matrix inversion are employed) ;
for a few hundred data points this is certainly feasible on workstation computers,
although for larger problems some iterative methods or approximations may be
needed.

2.1

PARAMETERIZING THE COVARIANCE FUNCTION

There are many choices of covariance functions which may be reasonable. Formally,
we are required to specify functions which will generate a non-negative definite
covariance matrix for any set of points (x(1 ), ... , x(k )). From a modelling point of
view we wish to specify covariances so that points with nearby inputs will give rise
to similar predictions. We find that the following covariance function works well:

-t L WI(x~i)
d

Vo exp{

-

x~j))2}

1=1

d

+ao + a1 Lx~i)x~j) + V18(i , j),
1=1

(3)

c. K. I. WILLIAMS, C. E. RASMUSSEN

516

y

y

/

/

p(y)

y1

/

/

Figure 1: An illustration of prediction using a Gaussian process. There is one training
case (x(1), t(1)) and one test case for which we wish to predict y. The ellipse in the lefthand plot is the one standard deviation contour plot of the joint distribution of Yl and
y . The dotted line represents an observation Yl = t(1). In the right-hand plot we see
the distribution of the output for the test case, obtained by conditioning on the observed
target. The y axes have the same scale in both plots.

=

log(vo, V1, W1, . . . , Wd, ao, ad plays the role of hyperparameters 1 . We
where (}
define the hyperparameters to be the log of the variables in equation (4) since these
are positive scale-parameters.
The covariance function is made up of three parts; the first term, a linear regression
term (involving ao and aI) and a noise term V1b(i, j). The first term expresses the
idea that cases with nearby inputs will have highly correlated outputs; the WI parameters allow a different distance measure for each input dimension. For irrelevant
inputs, the corresponding WI will become small, and the model will ignore that input. This is closely related to the Automatic Relevance Determination (ARD) idea
of MacKay and Neal (MacKay, 1993; Neal 1995). The Vo variable gives the overall
scale of the local correlations. This covariance function is valid for all input dimensionalities as compared to splines, where the integrated squared mth derivative is
only a valid regularizer for 2m > d (see Wahba, 1990). ao and a1 are variables
controlling the scale the of bias and linear contributions to the covariance. The last
term accounts for the noise on the data; VI is the variance of the noise.
Given a covariance function , the log likelihood of the training data is given by

1= -

~ logdet I< - ~tT I<-lt - !!.log27r.
222

In section 3 we will discuss how the hyperparameters
response to the training data.

2.2

III

(4)

C can be adapted, in

RELATIONSHIP TO PREVIOUS WORK

The Gaussian process view provides a unifying framework for many regression methods . ARMA models used in time series analysis and spline smoothing (e.g. Wahba,
1990 and earlier references therein) correspond to Gaussian process prediction with
1 We call () the hyperparameters as they correspond closely to hyperparameters in neural
networks; in effect the weights have been integrated out exactly.

Gaussian Processes for Regression

517

a particular choice of covariance function 2 . Gaussian processes have also been used
in the geostatistics field (e .g. Cressie, 1993) , and are known there as ""kriging"", but
this literature has concentrated on the case where the input space is two or three
dimensional , rather than considering more general input spaces.
This work is similar to Regularization Networks (Poggio and Girosi, 1990; Girosi,
Jones and Poggio, 1995), except that their derivation uses a smoothness functional
rather than the equivalent covariance function. Poggio et al suggested that the
hyperparameters be set by cross-validation. The main contributions of this paper
are to emphasize that a maximum likelihood solution for 8 is possible, to recognize
the connections to ARD and to use the Hybrid Monte Carlo method in the Bayesian
treatment (see section 3).

3

TRAINING A GAUSSIAN PROCESS

The partial derivative of the log likelihood of the training data I with respect to
all the hyperparameters can be computed using matrix operations, and takes time
O( n 3 ) . In this section we present two methods which can be used to adapt the
hyperparameters using these derivatives.

3.1

MAXIMUM LIKELIHOOD

In a maximum likelihood framework, we adjust the hyperparameters so as to maximize that likelihood of the training data. We initialize the hyperparameters to
random values (in a reasonable range) and then use an iterative method, for example conjugate gradient, to search for optimal values of the hyperparameters. Since
there are only a small number of hyperparameters (d + 4) a relatively small number
of iterations are usually sufficient for convergence. However, we have found that
this approach is sometimes susceptible to local minima, so it is advisable to try a
number of random starting positions in hyperparameter space.

3.2

INTEGRATION VIA HYBRID MONTE CARLO

According to the Bayesian formalism, we should start with a prior distribution P( 8)
over the hyperparameters which is modified using the training data D to produce
a posterior distribution P(8ID). To make predictions we then integrate over the
posterior; for example, the predicted mean y( x) for test input x is given by

y(x) =

J

Y8(x)P(8I D )d8

(5)

where Y8( x) is the predicted mean (as given by equation 1) for a particular value of
8. It is not feasible to do this integration analytically, but the Markov chain Monte
Carlo method of Hybrid Monte Carlo (HMC) (Duane et ai, 1987) seems promising
for this application. We assign broad Gaussians priors to the hyperparameters, and
use Hybrid Monte Carlo to give us samples from the posterior.
HMC works by creating a fictitious dynamical system in which the hyperparameters
are regarded as position variables, and augmenting these with momentum variables
p. The purpose of the dynamical system is to give the hyperparameters ""inertia""
so that random-walk behaviour in 8-space can be avoided. The total energy, H, of
the system is the sum of the kinetic energy, J{, (a function of the momenta) and the
potential energy, E. The potential energy is defined such that p(8ID) ex: exp(-E).
We sample from the joint distribution for 8 and p given by p(8,p) ex: exp(-E2Technically splines require generalized covariance functions.

C. K. I. WILUAMS, C. E. RASMUSSEN

518

I<); the marginal of this distribution for 8 is the required posterior. A sample of
hyperparameters from the posterior can therefore be obtained by simply ignoring
the momenta.
Sampling from the joint distribution is achieved by two steps: (i) finding new points
in phase space with near-identical energies H by simulating the dynamical system
using a discretised approximation to Hamiltonian dynamics, and (ii) changing the
energy H by doing Gibbs sampling for the momentum variables.
Hamiltonian Dynamics
Hamilton's first order differential equations for H are approximated by a discrete
step (specifically using the leapfrog method). The derivatives of the likelihood
(equation 4) enter through the derivative of the potential energy. This proposed
state is then accepted or rejected using the Metropolis rule depending on the final
energy H* (which is not necessarily equal to the initial energy H because of the
discretization). The same step size c is used for all hyperparameters , and should be
as large as possible while keeping the rejection rate low .
Gibbs Sampling for Momentum Variables
The momentum variables are updated using a modified version of Gibbs sampling,
thereby allowing the energy H to change. A ""persistence"" of 0.95 is used; the new
value of the momentum is a weighted sum of the previous value (with weight 0.95)
and the value obtained by Gibbs sampling (weight (1 - 0.95 2)1/ 2). With this form
of persistence, the momenta change approximately twenty times more slowly, thus
increasing the ""inertia"" of the hyperparameters, so as to further help in avoiding
random walks. Larger values of the persistence will further increase the inertia, but
reduce the rate of exploration of H .
Practical Details
The priors over hyperparameters are set to be Gaussian with a mean of -3 and a
standard deviation of 3. In all our simulations a step size c = 0.05 produced a very
low rejection rate ? 1%). The hyperparameters corresponding to V1 and to the
WI ' S were initialised to -2 and the rest to O.
To apply the method we first rescale the inputs and outputs so that they have mean
of zero and a variance of one on the training set. The sampling procedure is run
for the desired amount of time, saving the values of the hyperparameters 200 times
during the last two-thirds of the run . The first third of the run is discarded; this
""burn-in"" is intended to give the hyperparameters time to come close to their equilibrium distribution. The predictive distribution is then a mixture of 200 Gaussians.
For a squared error loss, we use the mean of this distribution as a point estimate.
The width of the predictive distribution tells us the uncertainty of the prediction.

4

EXPERIMENTAL RESULTS

We report the results of prediction with Gaussian process on (i) a modified version
of MacKay's robot arm problem and (ii) five real-world data sets.

4.1

THE ROBOT ARM PROBLEM

We consider a version of MacKay's robot arm problem introduced by Neal (1995).
The standard robot arm problem is concerned with the mappings
Y1

= r1 cos Xl + r2 COS(X1 + X2)

Y2

= r1 sin Xl + r2 sin(x1 + X2)

(6)

Gaussian Processes for Regression

Method
Gaussian process
Gaussian process
MacKay
Neal
Neal

519

No . of inputs
2
6
2
2
6

sum squared test error
1.126
1.138
1.146
1.094
1.098

Table 1: Results on the robot arm task. The bottom three lines of data were obtained
from Neal (1995) . The MacKay result is the test error for the net with highest ""evidence"".
The data was generated by picking Xl uniformly from [-1.932, -0.453] and [0.453 ,
1.932] and picking X2 uniformly from [0 .534, 3.142]. Neal added four further inputs,
two of which were copies of Xl and X2 corrupted by additive Gaussian noise of
standard deviation 0.02 , and two further irrelevant Gaussian-noise inputs with zero
mean and unit variance. Independent zero-mean Gaussian noise of variance 0.0025
was then added to the outputs YI and Y2 . We used the same datasets as Neal and
MacKay, with 200 examples in the training set and 200 in the test set .
The theory described in section 2 deals only with the prediction of a scalar quantity
Y , so predictors were constructed for the two outputs separately, although a joint

prediction is possible within the Gaussian process framework (see co-kriging, ?3 .2.3
in Cressie, 1993).
Two experiments were conducted, the first using only the two ""true"" inputs, and
the second one using all six inputs. In this section we report results using maximum likelihood training; similar results were obtained with HMC . The log( v),s
and loge w )'s were all initialized to values chosen uniformly from [-3.0, 0.0], and
were adapted separately for the prediction of YI and Y2 (in these early experiments
the linear regression terms in the covariance function involving aa and al were not
present) . The conjugate gradient search algorithm was allowed to run for 100 iterations, by which time the likelihood was changing very slowly. Results are reported
for the run which gave the highest likelihood of the training data, although in fact
all runs performed very similarly. The results are shown in Table 1 and are encouraging, as they indicate that the Gaussian process approach is giving very similar
performance to two well-respected techniques. All of the methods obtain a level of
performance which is quite close to the theoretical minimum error level of 1.0 ....Jt is
interesting to look at the values of the w's obtained after the optimization; for the
Y2 task the values were 0.243,0.237,0.0639,7.0 x 10- 4 , 2.32 x 10- 6 ,1.70 x 10- 6 ,
and Va and VI were 7.5278 and 0.0022 respectively. The w values show nicely that
the first two inputs are the most important, followed by the corrupted inputs and
then the irrelevant inputs. During training the irrelevant inputs are detected quite
quickly, but the w 's for the corrupted inputs shrink more slowly, implying that the
input noise has relatively little effect on the likelihood.
4.2

FIVE REAL-WORLD PROBLEMS

Gaussian Processes as described above were compared to several other regression
algorithms on five real-world data sets in (Rasmussen, 1996; in this volume). The
data sets had between 80 and 256 training examples, and the input dimension
ranged from 6 to 16. The length of the HMC sampling for the Gaussian processes
was from 7.5 minutes for the smallest training set size up to 1 hour for the largest
ones on a R4400 machine. The results rank the methods in the order (lowest error
first) a full-blown Bayesian treatment of neural networks using HMC, Gaussian

C. K. I. WILLIAMS, C. E. RASMUSSEN

520

processes, ensembles of neural networks trained using cross validation and weight
decay, the Evidence framework for neural networks (MacKay, 1992), and MARS.
We are currently working on assessing the statistical significance of this ordering.

5

DISCUSSION

We have presented the method of regression with Gaussian processes, and shown
that it performs well on a suite of real-world problems.
We have also conducted some experiments on the approximation of neural nets (with
a finite number of hidden units) by Gaussian processes, although space limitations
do not allow these to be described here. Some other directions currently under
investigation include (i) the use of Gaussian processes for classification problems by
softmaxing the outputs of k regression surfaces (for a k-class classification problem),
(ii) using non-stationary covariance functions, so that C(x , Xl) f:- C(lx - XII) and
(iii) using a covariance function containing a sum of two or more terms of the form
given in line 1 of equation 3.
We hope to make our code for Gaussian process prediction publically available in the
near future. Check http://www.cs.utoronto.ca/neuron/delve/delve.html for details.

Acknowledgements
We thank Radford Neal for many useful discussions, David MacKay for generously providing the robot arm data used in this paper, and Chris Bishop, Peter Dayan, Radford Neal
and Huaiyu Zhu for comments on earlier drafts. CW was partially supported by EPSRC
grant GRjJ75425.

References
Cressie, N. A. C. (1993) . Statistics for Spatial Data. Wiley.
Duane, S., Kennedy, A. D., Pendleton, B. J., and Roweth, D. (1987). Hybrid Monte Carlo.
Physics Letters B, 195:216-222.
Girosi, F., Jones, M., and Poggio, T. (1995). Regularization Theory and Neural Networks
Architectures. Neural Computation, 7(2):219-269.
MacKay, D . J. C. (1992). A Practical Bayesian Framework for Backpropagation Networks.
Neural Computation, 4(3):448-472.
MacKay, D. J. C. (1993). Bayesian Methods for Backpropagation Networks. In van
Hemmen, J. L., Domany, E., and Schulten, K., editors, Models of Neural Networks
II. Springer.
Neal, R. M. (1993). Bayesian Learning via Stochastic Dynamics. In Hanson, S. J., Cowan,
J. D., and Giles, C. L., editors, Neural Information Processing Systems, Vol. 5, pages
475-482. Morgan Kaufmann, San Mateo, CA.
Neal, R. M. (1995). Bayesian Learning for Neural Networks. PhD thesis, Dept. of Computer Science, University of Toronto.
Poggio, T. and Girosi, F. (1990). Networks for approximation and learning. Proceedings
of IEEE, 78:1481-1497.
Rasmussen, C. E. (1996). A Practical Monte Carlo Implementation of Bayesian Learning.
In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, Advances in Neural
Information Processing Systems 8. MIT Press.
Wahba, G. (1990). Spline Models for Observational Data. Society for Industrial and Applied Mathematics. CBMS-NSF Regional Conference series in applied mathematics.

"
1049,1995,Modern Analytic Techniques to Solve the Dynamics of Recurrent Neural Networks,,1049-modern-analytic-techniques-to-solve-the-dynamics-of-recurrent-neural-networks.pdf,Abstract Missing,"Modern Analytic Techniques to Solve the
Dynamics of Recurrent Neural Networks

A.C.C. Coolen
Dept. of Mathematics
King's College London
Strand, London WC2R 2LS, U.K.

S.N. Laughton
Dept. of Physics - Theoretical Physics
University of Oxford
1 Keble Road, Oxford OX1 3NP, U.K.

D. Sherrington ..
Center for Non-linear Studies
Los Alamos National Laboratory
Los Alamos, New Mexico 87545

Abstract
We describe the use of modern analytical techniques in solving the
dynamics of symmetric and nonsymmetric recurrent neural networks near saturation. These explicitly take into account the correlations between the post-synaptic potentials, and thereby allow
for a reliable prediction of transients.

1

INTRODUCTION

Recurrent neural networks have been rather popular in the physics community,
because they lend themselves so naturally to analysis with tools from equilibrium
statistical mechanics. This was the main theme of physicists between, say, 1985
and 1990. Less familiar to the neural network community is a subsequent wave of
theoretical physical studies, dealing with the dynamics of symmetric and nonsymmetric recurrent networks. The strategy here is to try to describe the processes
at a reduced level of an appropriate small set of dynamic macroscopic observables.
At first, progress was made in solving the dynamics of extremely diluted models
(Derrida et al, 1987) and of fully connected models away from saturation (for a
review see (Coolen and Sherrington, 1993)). This paper is concerned with more
recent approaches, which take the form of dynamical replica theories, that allow
for a reliable prediction of transients, even near saturation. Transients provide the
link between initial states and final states (equilibrium calculations only provide
?On leave from Department of Physics - Theoretical Physics, University of Oxford

A. C. C. COOLEN, S. N. LAUGHTON, D. SHERRINGTON

254

information on the possible final states). In view of the technical nature of the
subject, we will describe only basic ideas and results for simple models (full details
and applications to more complicated models can be found elsewhere).

2

RECURRENT NETWORKS NEAR SATURATION

Let us consider networks of N binary neurons ai E {-I, I}, where neuron states
are updated sequentially and stochastically, driven by the values of post-synaptic
potentials hi . The probability to find the system at time t in state 0' = (a1,' .. , aN)
is denoted by Pt(O'). For the rates Wi(O') of the transitions ai -t -(7i and for the
potentials hi (0') we make the usual choice
1
Wi (0') = - [1-ai tanh [,Bhi (0')]]
hi(O') =
Jijaj

L

2

j:f:i

The parameter ,B controls the degree of stochasticity: the ,B = 0 dynamics is completely random, whereas for ,B = 00 we find the deterministic rule ai -t sgn[hi(O')].
The evolution in time of Pt(O') is given by the master equation
d
N
(1)
dtPt (0') =
[Pt (FkO' )Wk (FkO') - Pt (0' )Wk (0')]

l:
k=l

with Fk<P(O') = <P(a1 , ... ,-(7k, ... , aN)' For symmetric models, where Jij = Jji
for all (ij), the dynamics (1) leads asymptotically to the Boltzmann equilibrium
distribution Peq(O') '"" exp [-,BE(O')], with the energy E(O') = - Li<j adijaj.
For associative memory models with Hebbian-type synapses, required to store a set
of P random binary patterns e/.1 = (?i, .. . , ?~ ), the relevant macroscopic observable
is the overlap m between the current microscopic state 0' and the pattern to be
retrieved (say, pattern 1): m = -Iv Li ?lai. Each post-synaptic potential can now
be written as the sum of a simple signal term and an interference-noise term , e.g.
1 p=o:N
(2)
hi(O') = m?l + ~
?f ?jaj
Jij = N
?f?j

L

l: l:

/.1=1

/.1>1

j:f: i

All complications arise from the noise terms.
The 'Local Chaos Hypothesis' (LCH) consists of assuming the noise terms to be
independently distributed Gaussian variables. The macroscopic description then
consists of the overlap m and the width ~ of the noise distribution (Amari and
Maginu, 1987). This, however, works only for states near the nominated pattern,
see also (Nishimori and Ozeki, 1993). In reality the noise components in the potentials have far more complicated statistics l . Due to the build up of correlations
between the system state and the non-nominated patterns, the noise components
can be highly correlated and described by bi-modal distributions. Another approach
involves a description in terms of correlation- and response functions (with two timearguments). Here one builds a generating functional, which is a sum over all possible
trajectories in state space, averaged over the distribution of the non-nominated patterns. One finds equations which are exact for N -t 00 , but, unfortunately, also
rather complicated. For the typical neural network models solutions are known
only in equilibrium (Rieger et aI, 1988); information on transients has so far only
been obtained through cumbersome approximation schemes (Horner et aI, 1989).
We now turn to a theory that takes into account the non-trivial statistics of the
post-synaptic potentials, yet involves observables with one time-argument only.
lCorrelations are negligible only in extremely diluted (asymmetric) networks (Derrida
et aI , 1987) , and in networks with independently drawn (asymmetric) random synapses

Modem Analytic Techniques to Solve the Dynamics of Recurrent Neural Networks

3

255

DYNAMICAL REPLICA THEORIES

The evolution of macroscopic observables n( 0') = (0 1 (0'), ... , OK (0')) can be described by the so-called Kramers-Moyal expansion for the corresponding probability
distribution pt(n) (derived directly from (1)). Under certain conditions on the sensitivity of n to single-neuron transitions (7i -t -1J'i, one finds on finite time-scales
and for N -t 00 the macroscopic state n to evolve deterministically according to:
~n
dt

=

EO' pt(O')8 [n-n(O')] Ei Wi(O') [n(FiO')-n(O')]
EO' pt(O')8 [n-n(O')]

(3)

This equation depends explicitly on time through Pt(O'). However, there are two natural ways for (3) to become autonomous: (i) by the term Ei Wi(O') [n(FiO') -n(O')]
depending on u only through n(O') (as for attractor networks away from saturation), or (ii) by (1) allowing for solutions of the form Pt(O') = fdn(O')] (as for
extremely diluted networks). In both cases Pt(O') drops out of (3). Simulations further indicate that for N -t 00 the macroscopic evolution usually depends only on
the statistical properties of the patterns {ell}, not on their microscopic realisation
('self-averaging'). This leads us to the following closure assumptions:
1. Probability equipartitioning in the n subshells of the ensemble: Pt(O') '""
8 [nt-n(O')]. If n indeed obeys closed equations, this assumption is safe.

2. Self-averaging of the n flow with resfect to the microscopic details of the
non-nominated patterns:
n -t (dt n)patt.

tt

Our equations (3) are hereby transformed into the closed set:
~n _ (EO' 8 [n-n(O')] Ei Wi(O') [n(FiO') - n(O')])
dt
EO' 8 [n-n(O')]
patt

The final observation is that the tool for averaging fractions is replica theory:
lim lim
ddt n = n--tO
N --too

~
(~Wi(O'l) [n(FiO'1)-n(O' 1)] rrn 8[n-n(O'
~ ~

O'I ???O'

n

i

O

)])patt

(4)

0=1

The choice to be made for the observables n(O'), crucial for the closure assumptions
to make sense, is constrained by requiring the theory to be exact in specific limits:
exactness for a -t 0 :
exactness for t -t 00:

4

n = (m, ... )
n = (E, ... )

(for symmetric models only)

SIMPLE VERSION OF THE THEORY

For the Hopfield model (2) the simplest two-parameter theory which is exact for a -t
-t 00 is consequently obtained by choosing n = (m,E). Equivalently
we can choose n = (m,r), where r(O') measures the 'interference energy':

o and for t
m

= ~ L~I(7i
i

The result of working out (4) for

J
=; J

!m

""21 dtd r

n

= (m, r) is:

=

dz Dm,r[z] tanh,B (m+z) - m

1

dz Dm,r[z]z tanh,B (m+z)

+1-

r

256

A. C. C. COOLEN, S. N. LAUGHTON, D. SHERRINGTON

15

~----------------------------~

r

/

/
/

/

/
I

o

L -_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

o

~

1

m

Figure 1: Simulations (N = 32000, dots) versus simple RS theory (solid lines), for
a = 0.1 and j3 = 00. Upper dashed line: upper boundary of the physical region.
Lower dashed line: upper boundary of the RS region (the AT instability).
in which Dm,r[z] is the distribution of 'interference-noise' terms in the PSP's, for
which the replica calculation gives the outcome (in so-called RS ansatz):

Dm,r[z] =

e-~2
{l-jDY
2 27rar

tanh [>.y

+e-~)2 {1-jDY
h2
= apr->.2jp
{q, {t, p}
2 27rar

with Dy = [27rj-t edy, ~
the remaining parameters
m

=j

Dy tanh[>'y+{tj

tanh [>.y

[~]
t+(~+Z)-~apr+{tl}
apr

[~]
t +(~-Z)~-{tl}
apr
apr

and>' = pyaq[l-p(l-q)]-l, and with
to be solved from the coupled equations:

q = j Dy tanh 2 [>.y+{t]

r

1-p(1-q)2

= [1-p(1-q)]2

Here we only give (partly new) results of the calculation; details can be found
in (Coolen and Sherrington, 1994). The noise distribution is not Gaussian (in
agreement with simulations, in contrast to LCH). Our simple two-parameter theory
is found to be exact for t '"" 0, t -7 00 and for a -7 O. Solving numerically the
dynamic equations leads to the results shown in figures 1 and 2. We find a nice
agreement with numerical simulations in terms of the flow in the (m, r) plane.
However, for trajectories leading away from the recall state m '"" 1, the theory
fails to reproduce an overall slowing down. These deviations can be quantified by
comparing cumulants of the noise distributions (Ozeki and Nishimori, 1994), or by
applying the theory to exactly solvable models (Coolen and Franz, 1994). Other
recent applications include spin-glass models (Coolen and Sherrington, 1994) and
more general classes of attractor neural network models (Laughton and Coolen,
1995). The simple two-parameter theory always predicts adequately the location of
the transients in the order parameter plane, but overestimates the relaxation speed.
In fact, figure 2 shows a remarkable resemblance to the results obtained for this
model in (Horner et al, 1989) with the functional integral formalism; the graphs of
m(t) are almost identical, but here they are derived in a much simpler way.

Modem Analytic Techniques to Solve the Dynamics of Recurrent Neural Networks

257

1

.8

10
--...,
...,

2 ?6
~

.....

..... .....

.4

'-'
!--

..... .....

.... ....

.... ........
.2

-- ---

--

5

.... ....

--

0

0
0

2

6

4

B

10

0

2

4

t

6

B

10

t

Figure 2: Simulations (N = 32000, dots) versus simple RS theory (RS stable: solid
lines, RS unstable: dashed lines), now as functions of time, for Q; = 0.1 and f3 = 00.

5

ADVANCED VERSION OF THE THEORY

Improving upon the simple theory means expanding the set n beyond n = (m,E).
Adding a finite number of observables will only have a minor impact; a qualitative
step forward, on the other hand, results from introducing a dynamic order parameter
function. Since the microscopic dynamics (1) is formulated entirely in terms of
neuron states and post-synaptic potentials we choose for n (u) the joint distribution:
1
D[(, h](u) = N
<5 [( -O""i] <5 [h-hi(U)]

L
i

This choice has the advantages that (a) both m and (for symmetric systems) E are
integrals over D[(, h], so the advanced theory automatically inherits the exactness
at t = 0 and t = 00 of the simple one, (b) it applies equally well to symmetric and
nonsymmetric models and (c) as with the simple version, generalisation to models
with continuous neural variables is straightforward. Here we show the result of
applying the theory to a model of the type (1) with synaptic interactions:

Jij =

~ ~i~j +

.iN [cos(~

)Xij +sin(~ )Yij ]

Xij = Xji, Yij = -Yji (independent random Gaussian variables)
(describing a nominated pattern being stored on a 'messy' synaptic background).
The parameter w controls the degree of synaptic symmetry (e.g. w = 0: symmetric,
w = 7r: anti-symmetric) . Equation (4) applied to the observable D[(, h](u) gives:
~

8

mDt[C h] = J2[1-(O""tanh(f3H))D t ] 8h2Dt [(,h]

+ :h

8

+ 8h A [( , h;Dt]

{DdCh] [h-Jo(tanh(f3 H ))Dt]}

1
1
+2 [l+(tanh(f3h)] Dd--(, h] - 2 [l-(tanh(f3h)] DdC h]

258

A. C. C. COOLEN, S. N. LAUGHfON, D. SHERRINGTON
o .------,------.------.------.------.------~

- .2

-.4

E
""-

-.6

""-

'~

~-

--- --- -- - --

- .8

_ 1 L-____- L_ _ _ _ _ _L -_ __ _

o

~

_ _ _ __ _

2

~

_ __ _

~

_ __ _ _ _

~

4

6

t
Figure 3: Comparison of simulations (N = 8000, solid line), simple two-parameter
theory (RS stable: dotted line, RS unstable: dashed line) and advanced theory
(solid line) , for the w = a (symmetric background) model, with Jo = 0, f3 = 00.
Note that the two solid lines are almost on top of each other at the scale shown.

"".
0.5

0 .0

E
-0.5

-0.5

o

2

4

t

6

o

2

4

6

t

Figure 4: Advanced theory versus N = 5600 simulations in the w = ~7r (asymmetric
background) model, with f3 = 00 and J = 1. Solid: simulations; dotted: solving the
RS diffusion equation.

Modem Analytic Techniques to Solve the Dynamics of Recurrent Neural Networks

259

with (f(a,H))D = L:"". JdH D[a,H]J(a, H). All complications are concentrated in
the kernel A[C h ; DJ, which is to be solved from a nontrivial set of equations emerging from the replica formalism. Some results of solving these equations numerically
are shown in figures 3 and 4 (for details of the calculations and more elaborate comparisons with simulations we refer to (Laughton, Coolen and Sherrington, 1995;
Coolen, Laughton and Sherrington, 1995)). It is clear that the advanced theory
quite convincingly describes the transients of the simulation experiments, including
the hitherto unexplained slowing down, for symmetric and nonsymmetric models.

6

DISCUSSION

In this paper we have described novel techniques for studying the dynamics of recurrent neural networks near saturation. The simplest two-parameter theory (exact
for t = 0, for t --+ 00 and for 0: --+ 0) , which employs as dynamic order parameters
the overlap with a pattern to be recalled and the total 'energy' per neuron, already
describes quite accurately the location of the transients in the order parameter
plane. The price paid for simplicity is that it overestimates the relaxation speed.
A more advanced version of the theory, which describes the evolution of the joint
distribution for neuron states and post-synaptic potentials, is mathematically more
involved, but predicts the dynamical data essentially perfectly, as far as present
applications allow us conclude. Whether this latter version is either exact, or just
a very good approximation, still remains to be seen.

In this paper we have restricted ourselves to models with binary neural variables,
for reasons of simplicity. The theories generalise in a natural way to models with
analogue neurons (here, however , already the simple version will generally involve
order parameter functions as opposed to a finite number of order parameters).
Ongoing work along these lines includes, for instance, the analysis of analogue and
spherical attractor networks and networks of coupled oscillators near saturation.
References

B. Derrida, E. Gardner and A. Zippelius (1987), Europhys. Lett. 4: 167-173
A.C .C. Coolen and D. Sherrington (1993), in J.G. Taylor (ed.), Mathematical Approaches to Neural Networks, 293-305. Amsterdam: Elsevier.
S. Amari and K. Maginu (1988), Neural Networks 1: 63-73
H. Nishimori and T. Ozeki (1993), J. Phys. A 26: 859-871
H. Rieger, M. Schreckenberg and J. Zittartz (1988), Z. Phys. B 72: 523-533
H. Horner, D. Bormann, M. Frick, H. Kinzelbach and A. Schmidt (1989), Z. Phys.
B 76: 381-398
A.C.C. Coolen and D. Sherrington (1994), Phys. Rev. E 49(3): 1921-1934
H. Nishimori and T. Ozeki (1994), J . Phys. A 27: 7061-7068
A.C.C. Coolen and S. Franz (1994), J. Phys. A 27: 6947-9954
A.C.C. Coolen and D. Sherrington (1994), J. Phys. A 27: 7687-7707
S.N. Laughton and A.C.C. Coolen (1995), Phys. Rev. E 51: 2581-2599
S.N. Laughton, A.C.C. Coolen and D. Sherrington (1995), J. Phys. A (in press)
A.C.C. Coolen, S.N . Laughton and D. Sherrington (1995), Phys. Rev. B (in press)

"
105,1988,Backpropagation and Its Application to Handwritten Signature Verification,,105-backpropagation-and-its-application-to-handwritten-signature-verification.pdf,Abstract Missing,"340

BACKPROPAGATION AND ITS
APPLICATION TO HANDWRITTEN
SIGNATURE VERIFICATION
Dorothy A. Mighell
Electrical Eng. Dept.
Info. Systems Lab
Stanford University
Stanford, CA 94305

Timothy S. Wilkinson
Electrical Eng. Dept.
Info. Systems Lab
Stanford University
Stanford, CA 94305

Joseph W. Goodman
Electrical Eng. Dept.
Info. Systems Lab
Stanford University
Stanford, CA 94305

ABSTRACT
A pool of handwritten signatures is used to train a neural network for the task of deciding whether or not a given signature is a
forgery. The network is a feedforward net, with a binary image as
input. There is a hidden layer, with a single unit output layer. The
weights are adjusted according to the backpropagation algorithm.
The signatures are entered into a C software program through the
use of a Datacopy Electronic Digitizing Camera. The binary signatures are normalized and centered. The performance is examined
as a function of the training set and network structure. The best
scores are on the order of 2% true signature rejection with 2-4%
false signature acceptance.

INTRODUCTION
Signatures are used everyday to authorize the transfer of funds for millions of people.
We use our signature as a form of identity, consent, and authorization. Bank checks,
credit cards, legal documents and waivers all require the everchanging personalized
signature. Forgeries on such transactions amount to millions of dollars lost each
year. A trained eye can spot most forgeries, but it is not cost effective to handcheck
all signatures due to the massive number of daily transactions. Consequently, only
disputed claims and checks written for large amounts are verified. The consumer
would certainly benefit from the added protection of automated verification. Neural
networks lend themselves very well to signature verification. Previously, they have
proven applicable to other signal processing tasks, such as character recognition
{Fukishima, 1986} {Jackel, 1988}, sonar target classification {Gorman, 1986}, and
control- as in the broom balancer {Tolat, 1988}.

HANDWRITING ANALYSIS
Signature verification is only one aspect of the study of handwriting analysis.
Recognition is the objective, whether it be of the writer or the characters. Writer
recognition can be further broken down into identification and verification. Identi-

Backpropagation and Handwritten Signature Verification

fication selects the author of a sample from among a group of writers. Verification
confirms or rejects a written sample for a single author. In both cases, it is the
style of writing that is important.
Deciphering written text is the basis of character recognition. In this task, linguistic
information such as the individual characters or words are extracted from the text.
Style must be eliminated to get at the content. A very important application of
character recognition is automated reading of zip-codes in the post office {Jackel,
1988}.
Data for handwriting analysis may be either dynamic or static. Dynamic data
requires special devices for capturing the temporal characteristics of the sample.
Features such as pressure, velocity, and position are examined in the dynamic
framework. Such analysis is usually performed on-line in real time.
Static analysis uses the final trace of the writing, as it appears on paper. Static
analysis does not require any special processing devices while the signature is being
produced. Centralized verification becomes possible, and the processing may be
done off-line.
Work has been done in both static and dynamic analysis {Sato, 1982} {Nemcek,
1974}. Generally, signature verification efforts have been more successful using
the dynamic information. It would be extremely useful though, to perform the
verification using only the written signature. This would eliminate the need for
costly machinery at every place of business. Personal checks may also be verified
through a static signature analysis.

TASK
The handwriting analysis task with which this paper is concerned is that of signature verification using an off-line method to detect casual forgeries. Casual forgeries
are non-professional forgeries, in which the writer does not practice reproducing
the signature. The writer may not even have a copy of the true signature. Casual
forgeries are very important to detect. They are far more abundant, and involve
greater monetary losses than professional forgeries. This signature verification task
falls into the writer recognition category, in which the style of writing is the important variable. The off-line analysis allows centralized verification at a lower cost
and broader use.

HANDWRITTEN SIGNATURES
The signatures for this project were gathered from individuals to produce a pool
of 80 true signatures and 66 forgeries. These are signatures, true and false, for one
person. There is a further collection of signatures, both true and false, for other
persons, but the majority of the results presented will be for the one individual. It
will be clear when other individuals are included in the demonstration.
The signatures are collected on 3x5 index cards which have a small blue box as

341

342

Wilkinson, Mighell and Goodman

a guideline. The cards are scanned with a CCD array camera from Datacopy,
and thresholded to produce binary images. These binary images are centered and
normalized to fit into a 128x64 matrix. Either the entire 128x64 image is presented
as input, or a 90x64 image of the three initials alone is presented. It is also possible
to present preprocessed inputs to the network.

SOFTWARE SIMULATION
The type of learning algorithm employed is that of backpropagation. Both dwell
and momentum are included. Dwell is the type of scheduling employed, in which
an image is presented to the network, and the network is allowed to ""dwell"" on that
input for a few iterations while updating its weights. C. Rosenberg and T. Sejnowski
have done a few studies on the effects of scheduling on learning {Rosenberg, 1986}.
Momentum is a term included in the change of weights equation to speed up learning
{Rumelhart, 1986}.
The software is written in Microsoft C, and run on an IBM PC/AT with an 80287
math co-processor chip.
Included in the simulation is a piece-wise linear approximation to the sigmoid transfer function as shown in Figure 1. This greatly improves the speed of calculation,
because an exponential is not calculated. The non-linearity is kept to allow for
layering of the network. Most of the details of initialization and update are the
same as that reported in NetTalk {Sejnowski, 1986}.

OUT

~-111111::::+~----'.

IN

Figure 1. Piece-wise linear transfer function.
Many different nets were trained in this signature verification project, all of which
were feed-forward. The output layer most often consisted of a single output neuron,
but 5 output neurons have been used as well. If a hidden layer was used, then
the number of hidden units ranged from 2 to 53. The networks were both fullyconnected and partially-connected.

SAMPLE RUN
The simplest network is that of a single neuron taking all 128x64 pixels as input,
plus one bias. Each pixel has a weight associated with it, so that the total number
of weights is 128x64 + 1 = 8193. Each white pixel is assigned an input value of + 1,
each black pixel has a value of -1. The training set consists of 10 true signatures

Backpropagation and Handwritten Signature Verification

with 10 forgeries. Figure 2a depicts the network structure of this sample run.

OUT

c::

1

0

-""
u

CD
CD

0.5

-

f- ..

CD

-:J
""-

Q.

0

0.5

0
P(false

~1~111J

111.

1

acceptance)

(b)

""~~~mlla
1

1/

(a)

~

en

LL.

C

0

0.5

f

0
0
(e)

0.5

1

Output Values

(d)
Figure 2. Sample run.
a) Network = one output neuron, one weight per pixel, fully connected. Training set = 10 true signatures + 10 forgeries.
b) ROC plot for the sample run. (Probability of fa1se acceptance
vs probability of true detection). Test set = 70 true signatures
+ 56 forgeries.
c) Clipped picture of the weights for the sample run. White
positive weight, black = negative weight.

=

d) Cumulative distribution function for the true signatures (+) and
for the forgeries (0) of the sample run.
The network is trained on these 20 signatures until all signatures are classified

343

344

Wilkinson, Mighell and Goodman

correctly. The trained network is then tested on the remaining 70 true signatures
and 56 forgeries.
The results are depicted in Figures 2b and 2d. Figure 2b is a radar operating
characteristic curve, or roc plot for short. In this presentation of data, the probability of detecting a true signature is plotted against the probability of accepting a
forgery. Roc plots have been used for some time in the radar sciences as a means
for visualizing performance {Marcum, 1960}. A perfect roc plot has a right angle
in the upper left-hand corner which would show perfect separation of true signatures from forgeries. The curve is plotted by varying the threshold for classification.
Everything above the threshold is labeled a true signature, everything below the
threshold is labeled a forgery. The roc plot in Figure 2b is close to perfect, but
there is some overlap in the output values of the true signatures and forgeries. The
overlap can be seen in the cumulative distribution functions (cdfs) for the true and
false signatures as shown in Figure 2d. As seen in the cdfs, there is fairly good
separation of the output values. For a given threshold of 0.5, the network produces
1% rejection of true signatures as false, with 4% acceptance of forgeries as being
true. IT one lowers the threshold for classification down to 0.43, the true rejection
becomes nil, with a false acceptance of 7% . A simplified picture of the weights is
shown in Figure 2c, with white pixels designating positive weights, and black pixels
negative weights.

OTHER NETWORKS
The sample run above was expanded to include 2 and 3 hidden neurons with the
single output neuron. The results were similar to the single unit network, implying
that the separation is linear.
The 128x64 input image was also divided into regions, with each region feeding into
a single neuron. In one network structure, the input was sectioned into 32 equally
sized regions of 16x16 pixels. The hidden layer thus has 32 neurons, each neuron
receiving 16x16 + 1 inputs. The output neuron had 33 inputs. Likewise, the input
image was divided into 53 regions of 16x16 pixels, this time overlapping.
Finally, only the initials were presented to the network. (Handwriting experts
have noted that leading strokes and separate capital letters are very significant in
classification {Osborn, 1929}.) In this case, two types of networks were devised.
The first had a single output neuron, the second had three hidden neurons plus one
output neuron. Each of the hidden neurons received inputs from only one initial,
rather than from all three. The network with the single output neuron produced
the best results of all, with 2% true rejection and 2% false acceptance.

IMPORTANCE OF FORGERIES IN THE TRAINING SET
In all cases, the networks performed much better when forgeries were included in the
training set. When an all-white image is presented as the only forgery, performance
deteriorates significantly. When no forgeries are present, the network decides that

Backpropagation and Handwritten Signature Verification

all signatures are true signatures. It is therefore desirable to include actual forgeries
in the training set, yet they may be impractical to obtain. One possibility for
avoiding ?the collection of forgeries is to use computer generated forgeries. Another
is to distort the true signatures. A third is to use true signatures of other people as
forgeries for the person in question. The attraction of this last option is that the
masquerading forgeries are already available for use.

NETWORK WITHOUT FORGERIES
To test the use of true signatures of other people for forgeries, the following network
is devised. Once again, the input is the 128x64 pixel image. The output layer is
comprised of five output neurons fully connected to the input image. The function
of each output neuron is to be active when presented with a particular persons'
signature. When a forgery is present, the output is to be low. Figure 3a depicts this
network. The training set has 50 true signatures, ten for each of five people. Each
signature has a desired output of true for one neuron, and false for the remaining
four neurons. Once the network is trained, it is tested on 210 true signatures and
150 forgeries. Figures 3b and 3c record the results. At a threshold of 0.5, the true
rejection is 3% and the false acceptance is 14%. Decreasing the threshold down to
0.41 gives 0% true rejection and 28% false acceptance. These results are similar
to the sample run, though not as good. This is a simple demonstration of the use
of other true signatures as forgeries. More sophisticated techniques could improve
the discrimination. For instance, selecting names with similar lengths or spelling
should improve the classification.

CONCLUSION
Automated signature verification systems would be extremely important in the
business world for verifying monetary transactions. Countless dollars are lost each
day to instances of casual forgeries. An artificial neural network employing the
backpropagation learning algorithm has been trained on both true and false signatures for classification. The results have been very good: 2% rejection of genuine
signatures with 2% acceptance of forgeries. The analysis requires only the static
picture of the signature, there by offering widespread use through centralized verification. True signatures of other people may substitute for the forgeries in the
training set - eliminating the need for collecting non-genuine signatures.

345

346

Wilkinson, Mighell and Goodman

JWG JTH TSW LDK ABH

- r--::iif1l---------..
oC

(a)
lr-----------~~~--_=~

1

( ,)

CD

Q)

.-

(f.

I

~

00.5

""t:S U.5

o

Q)

::J

~

o~----------~--------~

o

0.5

P(false

acceptance)

1

o~~----~~~--------~

o

0.5

1

Output Values

(b)

(c)

Figure 3. Network without forgeries for 5 individuals.
a) Network
5 output neurons, one for each individua~ as indicated by the initials. Training set = 10 true signatures for each
individual.

=

b) ROC plot for the network without forgeries.
210 true signatures + 150 forgeries.
Test set

=

c) Cumulative distribution function for the true signatures (+) and
for the forgeries (0) of the network without forgeries.

Referenees
K. Fukishima and S. Miyake, ""Neocognitron: A biocybernetic approach to visual
pattern recognition Jt , in NHK Laboratorie~ Note, Vol. 336, Sep 1986 (NHK
Science and Technical Research Laboratories, Tokyo).

Backpropagation and Handwritten Signature Verification

P. Gorman and T. J. Sejnowski, ""Learned classification of sonar targets using a
massively parallel network"", in the proceedings of the IEEE ASSP Oct 21,
1986 DSP Workshop, Chatham, MA.
L. D. Jackel, H. P. Graf, W. Hubbard, J. S. Denker, and D. Henderson, ""An
application of neural net chips: handwritten digit recognition"", in IEEE International Oonference on Neural Networks 1988, II 107-115.
J. T. Marcum, ""A statistical theory of target detection by pulsed radar"", in IRE
Transactions in Information Theory, Vol. IT-6 (Apr.), pp 145-267, 1960.
W. F. Nemcek and W. C. Lin, ""Experimental investigation of automatic signature
verification"" in IEEE Transactions on Systems, Man, and Oybernetics, Jan.
1974, pp 121-126.
A. S. Osborn, Questioned Documents, 2nd edition (Boyd Printing Co, Albany NY)
1929.
C. R. Rosenberg and T. J. Sejnowski, ""The spacing effect on NETtalk, a massively parallel network"", in Proceedings of the Eighth Annual Oonference of
the Oognitive Science Society, (Hillsdale, New Jersey: Lawrence Erlbaum
Associates, 1986) 72-89.
D. E. Rumelhart, G. E. Hinton, and R. J. Williams, ""Learning internal representations by error propagation"", in Parallel Distributed Processing: Explorations
in the Microstructures of Oognition. Vol. 1: Foundations, edited by D. E.
Rumelhart & J. L. McClelland, (MIT Press, 1986).
Y. Sato and K. Kogure, ""Online signature verification based on shape, motion,
and writing pressure"", in Proceedings of the 6th International Oonference on
Pattern Recognition, Vol. 2, pp 823-826 (IEEE NY) 1982.
T. J. Sejnowski and C. R. Rosenberg, ""NETtalk: A Parallel Network that Learns
to Read Aloud"", Johns Hopkins University Department of Electrical Engineering and Computer Science Technical Report JHU /EECS-86/01, (1986).
V. V. Tolat and B. Widrow, ""An adaptive 'broom balancer' with visual inputs"" , in
IEEE International Oonference on Neural Networks 1988, II 641-647.

347

"
1050,1995,Family Discovery,,1050-family-discovery.pdf,Abstract Missing,"Family Discovery

Stephen M. Omohundro
NEC Research Institute
4 Independence Way, Princeton, NJ 08540
om@research.nj.nec.com

Abstract
""Family discovery"" is the task of learning the dimension and structure of a parameterized family of stochastic models. It is especially appropriate when the training examples are partitioned into
""episodes"" of samples drawn from a single parameter value. We
present three family discovery algorithms based on surface learning and show that they significantly improve performance over two
alternatives on a parameterized classification task.

1

INTRODUCTION

Human listeners improve their ability to recognize speech by identifying the accent
of the speaker. ""Might"" in an American accent is similar to ""mate"" in an Australian
accent. By first identifying the accent, discrimination between these two words is
improved. We can imagine locating a speaker in a ""space of accents"" parameterized
by features like pitch, vowel formants, ""r"" -strength, etc. This paper considers the
task of learning such parameterized models from data.
Most speech recognition systems train hidden Markov models on labelled speech
data. Speaker-dependent systems train on speech from a single speaker. Speakerindependent systems are usually similar, but are trained on speech from many
different speakers in the hope that they will then recognize them all. This kind of
training ignores speaker identity and is likely to result in confusion between pairs of
words which are given the same pronunciation by speakers with different accents.
Speaker-independent recognition systems could more closely mimic the human approach by using a learning paradigm we call ""family discovery"". The system would
be trained on speech data partitioned into ""episodes"" for each speaker. From this
data, the system would construct a parameterized family of models representing dif-

Family Discovery

Affine
Family

403

Affine Patch
Family

Coupled Map
Family

Figure 1: The structure of the three family discovery algorithms.

ferent accents. The learning algorithms presented in this paper could determine the
dimension and structure of the parameterization. Given a sample of new speech,
the best-fitting accent model would be used for recognition.
The same paradigm applies to many other recognition tasks. For example, an OCR
system could learn a parameterized family of font models (Revow, et. al., 1994).
Given new text, the system would identify the document's font parameters and use
the corresponding character recognizer.
In general, we use ""family discovery"" to refer to the task of learning the dimension
and structure of a parameterized family of stochastic models. The methods we
present are equally applicable to parameterized density estimation, classification,
regression, manifold learning, reinforcement learning, clustering, stochastic grammar learning, and other stochastic settings. Here we only discuss classification and
primarily consider training examples which are explicitly partitioned into episodes.
This approach fits naturally into the neural network literature on ""meta-learning""
(Schmidhuber, 1995) and ""network transfer"" (Pratt, 1994). It may also be considered as a particular case of the ""bias learning"" framework proposed by Baxter at
this conference (Baxter, 1996).
There are two primary alternatives to family discovery: 1) try to fit a single model
to the data from all episodes or 2) use separate models for each episode. The first
approach ignores the information that the different training sets came from distinct
models. The second approach eliminates the possibility of inductive generalization
from one set to another.
In Section 2, we present three algorithms for family discovery based on techniques
for ""surface learning"" (Bregler and Omohundro, 1994 and 1995). As shown in Figure
1, the three alternative representations of the family are: 1) a single affine subspace
of the parameter space, 2) a set of local affine patches smoothly blended together,
and 3) a pair of coupled maps from the parameter space into the model space and
back. In Section 3, we compare these three approaches to the two alternatives on a
parameterized classification task.

404

2

S. M. OMOHUNDRO

THE FIVE ALGORITHMS

Let the space of all classifiers under consideration be parameterized by 0 and assume
that different values of 0 correspond to different classifiers (ie. it is identifiable). For
example, 0 might represent the means, covariances, and class priors of a classifier
with normal class-conditional densities. O-space will typically have a much higher
dimension than the parameterized family we are seeking. We write P9(X) for the
total probability that the classifier 0 assigns to a labelled or unlabelled example x.
The true models are drawn from a d-dimensional family parameterized by , . Let the
training set be partitioned into N episodes where episode i consists of Ni training
examples tij, 1 :S j :S Ni drawn from a single underlying model with parameter
A family discovery learning algorithm uses this training data to estimate the
underlying parameterized family.

0:.

From a parameterized family, we may define the projection operator P from O-space
to itself which takes each 0 to the closest member of the family. Using this projection
operator, we may define a ""family prior"" on O-space which dies off exponentially
with the square distance of a model from the family mp(O) ex e-(9-P(9))2. Each
of the family discovery algorithms chooses a family so as to maximize the posterior
probability of the training data with respect to this prior. If the data is very
sparse, this MAP approximation to a full Bayesian solution can be supplemented
by ""Occam"" terms (MacKay, 1995) or by using a Monte Carlo approximation.
The outer loop of each of the algorithms performs the optimization of the fit of the
data by re-estimation in a manner similar to the Expectation Maximization (EM)
approach (Jordan and Jacobs, 1994). First, the training data in each episode i is
independently fit by a model Oi. Then the dimension of the family is determined
as described later and the family projection operator P is chosen to maximize the
probability that the episode models Oi came from that family
i mp(Oi). The
episode models Oi are then re-estimated including the new prior probability mp.
These newly re-estimated models are influenced by the other episodes through mp
and so exhibit training set ""transfer"". The re-estimation loop is repeated until
nothing changes.

n

The learned family can then be used to classify a set of N test unlabelled test examples Xk, 1 :S k :S N test drawn from a model O;est in the family. First, the parameter
Otest is estimated by selecting the member of the family with the highest likelihood
on the test samples. This model is then used to perform the classification. A good
approximation to the best-fit family member is often to take the image of the best-fit
model in the entire O-space under the projection operator P.
In the next five sections, we describe the two alternative approaches and the three
family discovery algorithms. They differ only in their choice of family representation
as encoded in the projection operator P.

2.1

The Single Model Approach

The first alternative approach is to train a single model on all of the training data.
It selects 0 to maximize the total likelihood L( 0) = n~l n~l P9 (tij ). New test
data is classified by this single selected model.

Family Discovery

2.2

405

The Separate Models Approach

The second alternative approach fits separate models for each training }?isode. It
chooses Bi for 1::; i::; N to maximize the episode likelihood Li(Bi ) = TIj~IPIJ(tij).
Given new test data, it determines which of the individual models Bi fit best and
classifies the data with it.
2.3

The Affine Algorithm

The affine model represents the underlying model family as an affine subspace of
the model parameter space. The projection operator Pal line projects a parameter
vector B orthogonally onto the affine subspace. The subspace is determined by
selecting the top principal vectors in a principal components analysis of the bestfit episode model parameters. As described in (Bregler & Omohundro, 1994) the
dimension is chosen by looking for a gap in the principal values.
2.4

The Affine Patch Algorithm

The second family discovery algorithm is based on the ""surface learning"" procedure described in (Bregler and Omohundro, 1994). The family is represented by
a collection of local affine patches which are blended together using Gaussian influence functions. The projection mapping Ppatch is a smooth convex combination
of projections onto the affine patches Ppatch(B) = 2::=1 10: (B)Ao: (B) where Ao: is
the projection operator for an affine patch and Io:(B) =
is a normalized

E:""J:)(IJ)

Gaussian blending function.
The patches are initialized using k-means clustering on the episode models to choose
k patch centers. A local principal components analysis is performed on the episode
models which are closest to each center. The family dimension is determined by
examining how the principal values scale as successive nearest neighbors are considered. Each patch may be thought of as a ""pancake"" lying in the surface. Dimensions
which belong to the surface grow quickly as more neighbors are considered while
dimensions across the surface grow only because of the curvature of the surface.
The Gaussian influence functions and the affine patches are then updated by the
EM algorithm (Jordan and Jacobs, 1994). With the affine patches held fixed, the
Gaussians Go: are refit to the errors each patch makes in approximating the episode
models. Then with the Gaussians held fixed, the affine patches Ao: are refit to the
epsiode models weighted by the the corresponding Gaussian Go:. Similar patches
may be merged together to form a more parsimonious model.
2.5

The Coupled Map Algorithm

The affine patch approach has the virtue that it can represent topologically complex
families (eg. families representing physical objects might naturally be parameterized
by the rotation group which is topologically a projective plane). It cannot, however,
provide an explicit parameterization of the family which is useful in some applications (eg. optimization searches). The third family discovery algorithm therefore
attempts to directly learn a parameterization of the model family.
Recall that the model parameters define B-space, while the family parameters de-

406

S. M. OMOHUNDRO

fine 'Y-space. We represent a family by a mapping G from B-space to 'Y-space together with a mapping F from 'Y-space back to B-space. The projection operation
is Pmap(B) = F(G(B)). The map G(O) defines the family parameter l' on the full
O-space.
This representation is similar to an ""auto-associator"" network in which we attempt
to ""encode"" the best-fit episode parameters Oi in the lower dimensional 'Y-space
by the mapping G in such a way that they can be correctly reconstructed by the
function F. Unfortunately, if we try to train F and G using back-propagation on
the identity error function, we get no training data away from the family. There is
no reason for G to project points away from the family to the closest family member.
We can rectify this by training F and G iteratively. First an arbitrary G is chosen
and F is trained to send the images 'Yi = G(Oi) back to 0i' G is trained, however,
on images under F corrupted by additive spherical Gaussian noise! This provides
samples away from the family and on average the training signal sends each point
in B space to the closest family member.
To avoid iterative training, our experiments used a simpler approach. G was taken to
be the affine projection operator defined by a global principal components analysis
of the best-fit episode model parameters. Once G is defined, F is chosen to minimize
the difference between F(G(Oi)) and Oi for each best-fit episode parameter Oi.
Any form of trainable nonlinear mapping could be used for F (eg. backprop neural
networks or radial basis function networks). We represent F as a mixture of experts
(Jordan and Jacobs, 1994) where each expert is an affine mapping and the mixture
coefficients are Gaussians. The mapping is trained by the EM algorithm.

3

ALGORITHM COMPARISON

To compare these five algorithms, we consider a two-class classification task with
unit-variance normal class-conditional distributions on a 5-dimensional feature
space. The means of the class distributions are parameterized by a nonlinear twoparameter family:
ml
m2

= (1'1

= ('Yl

+ ~cos??e~1 + ('Y2 + ~sin??e~2
- ~ cos ?>) e~1 + ('Y2 - ~ sin ?>) l2 .

where 0 ~ 1'1, 1'2 ~ 10 and ?> = ('Yl + 1'2)/3. The class means are kept at a unit
distance apart, ensuring significant class overlap over the whole family. The angle
?> varies with the parameters so that the correct classification boundary changes
orientation over the family. This choice of parameters introduces sufficient nonlinearity in the task to distinguish the non-linear algorithms from the linear one.
Figure 1 shows the comparative performance of the 5 algorithms. The x-axis is the
total number of training examples. Each set of examples consisted of approximately
N =
episodes of approximately Ni =
examples each. The classifier parameters for an episode were drawn uniformly from the classifier family. The episode
training examples were then sampled from the chosen classifier according to the
classifier's distribution. Each of the 5 algorithms was then trained on these examples. The number of patches in the surface patch algorithm and the number of affine
components in the surface map algorithm were both taken to be the square-root of

..;x

..;x

Family Discovery
0.52

407

r---.---.---""""T""""----r----,-----r---r---~-__,

Single model
Separate models
Affine family
Affine Patch family
Map Mixture family

0.5

-+-+-_.

-EJ -??x????
-A-.-

0.48
0.46
I!?

g

0.44

'0

0.42

w
c:
0

:uI!!

u.

0.4
0.38
0 .36
0 .34

400

600

800

1000
1200
Number of Examples

1400

1600

1800

2000

Figure 2: A comparison of the 5 family discovery algorithms on the classification
task.
the number of training episodes.
The y-axis shows the percentage correct for each algorithm on an independent test
set. Each test set consisted of 50 episodes of 50 examples each. The algorithms
were presented with unlabelled data and their classification predictions were then
compared with the correct classification label.
The results show significant improvement through the use of family discovery for
this classification task. The single model approach performed significantly worse
than any of the other approaches, especially for larger numbers of episodes (where
the family discovery becomes possible). The separate model approach improves with
the number of episodes, but is nearly always bested by the approaches which take
explicit account of the underlying parameterized family. Because of the nonlinearity
in this task, the simple affine model performs more poorly than the two nonlinear
methods. It is simple to implement, however, and may well be the method of choice
when the parameters aren 't so nonlinear. From this data, there is not a clear winner
between the surface patch and surface map approaches.

4

TRAINING SET DISCOVERY

Throughout this paper, we have assumed that the training set was partitioned into
episodes by the teacher. Agents interacting with the world may not be given this
explicit information. For example, a speech recognition system may not be told
when it is conversing with a new speaker. Similarly, a character recognition system

408

s. M. OMOHUNDRO

would probably not be given explicit information about font changes. Learners can
sometimes use the data itself to detect these changes, however. In many situations
there is a strong prior that successive events are likely to have come from a single
model with only occasional model changes. The EM algorithm is often used for
segmenting unlabelled speech. It may be used in a similar manner to find the
training set episode boundaries. First, a clustering algorithm is used to partition
the training examples into episodes. A parameterized family is then fit to these
episodes. The data is then repartitioned according to the similarity of the induced
family parameters and the process is repeated until it converges. A similar approach
may be applied when the model parameters vary slowly with time rather than
occasionally jumping discontinously.
Acknowledgements

I'd like to thank Chris Bregler for work on the affine patch approach to surface
learning, Alexander Linden for suggesting coupled maps for surface learning, and
Peter Blicher for discussions.
References

Baxter, J. (1995) Learning model bias. This volume.
Bregler, C. & Omohundro, S. (1994) Surface learning with applications to lipreading. In J. Cowan, G. Tesauro and J. Alspector (eds.), Advances in Neural Information Processing Systems 6, pp. 43-50. San Francisco, CA: Morgan Kaufmann
Publishers.
Bregler, C. & Omohundro, S. (1995) Nonlinear image interpolation using manifold
learning. In G. Tesauro, D. Touretzky and T. Leen (eds .), Advances in Neural
Information Processing Systems 7. Cambridge, MA: MIT Press.
Bregler, C. & Omohundro, S. (1995) Nonlinear manifold learning for visual speech
recognition. In W . Grimson (ed.), Proceedings of the Fifth International Conference
on Computer Vision.
Jordan, M. & Jacobs, R. (1994) Hierarchical mixtures of experts and the EM algorithm. Neural Computation, 6:181-214.
MacKay, D. (1995) Probable networks and plausible predictions - a review of practical Bayesian methods for supervised neural networks. Network, to appear.
Pratt, L. (1994) Experiments on the transfer of knowledge between neural networks.
In S. Hanson, G. Drastal, and R. Rivest (eds.) , Computational Learning Theory and
Natural Learning Systems, Constraints and Prospects, pp. 523-560. Cambridge,
MA: MIT Press.
Revow, M., Williams, C. and Hinton, G. (1994) Using generative models for handwritten digit recognition. Technical report, University of Toronto.
Schmidhuber, J. (1995) On learning how to learn learning strategies. Technical
Report FKI-198-94, Fakultat fur Informatik, Technische Universitat Munchen.

"
1051,1995,Neural Networks with Quadratic VC Dimension,,1051-neural-networks-with-quadratic-vc-dimension.pdf,Abstract Missing,"Neural Networks with Quadratic VC
Dimension
Pascal Koiran*
Lab. de l'Informatique du Paraltelisme
Ecole Normale Superieure de Lyon - CNRS
69364 Lyon Cedex 07, France

Eduardo D. Sontag t
Department of Mathematics
Rutgers University
New Brunswick, NJ 08903, USA

Abstract
This paper shows that neural networks which use continuous activation functions have VC dimension at least as large as the square
of the number of weights w. This result settles a long-standing
open question, namely whether the well-known O( w log w) bound,
known for hard-threshold nets, also held for more general sigmoidal
nets. Implications for the number of samples needed for valid generalization are discussed.

1

Introduction

One of the main applications of artificial neural networks is to pattern classification
tasks. A set of labeled training samples is provided, and a network must be obtained
which is then expected to correctly classify previously unseen inputs. In this context,
a central problem is to estimate the amount of training data needed to guarantee
satisfactory learning performance. To study this question, it is necessary to first
formalize the notion of learning from examples.
One such formalization is based on the paradigm of probably approximately correct
(PAC) learning, due to Valiant (1984). In this framework, one starts by fitting some
function /, chosen from a predetermined class F, to the given training data. The
class F is often called the ""hypothesis class"" , and for purposes of this discussion it
will be assumed that the functions in F take binary values {O, I} and are defined on a
common domain X. (In neural networks applications, typically F corresponds to the
set of all neural networks with a given architecture and choice of activation functions.
The elements of X are the inputs, possibly multidimensional.) The training data
consists of labeled samples (Xi,ci), with each Xi E X and each Ci E {O, I}, and
*koiranGlip. ens-lyon. fr.
tsontagGhilbert.rutgers.edu.

198

P. KOIRAN, E. D. SONTAG

""fitting"" by an f means that f(xj) = Cj for each i. Given a new example x, one
uses f( x) as a guess of the ""correct"" classification of x. Assuming that both training
inputs and future inputs are picked according to the same probability distribution
on X, one needs that the space of possible inputs be well-sampled by the training
data, so that f is an accurate fit. We omit the details of the formalization of
PAC learning, since there are excellent references available, both in textbook (e.g.
Anthony and Biggs (1992), Natarajan (1991)) and survey paper (e.g. Maass (1994))
form, and the concept is by now very well-known.
After the work of Vapnik (1982) in statistics and of Blumer et. al. (1989) in computationallearning theory, one knows that a certain combinatorial quantity, called
the Vapnik-Chervonenkis (VC) dimension VC(F) of the class F of interest completely characterizes the sample sizes needed for learnability in the PAC sense. (The
appropriate definitions are reviewed below. In Valiant's formulation one is also interested in quantifying the computational effort required to actually fit a function
to the given training data, but we are ignoring that aspect in the current paper.)
Very roughly speaking, the number of samples needed in order to learn reliably is
proportional to VC(F). Estimating VC(F) then becomes a central concern. Thus
from now on, we speak exclusively of VC dimension, instead of the original PAC
learning problem.
The work of Cover (1988) and Baum and Haussler (1989) dealt with the computation of VC(F) when the class F consists of networks built up from hard-threshold
activations and having w weights; they showed that VC(F)= O(wlogw). (Conversely, Maass (1993) showed that there is also a lower bound of this form.) It
would appear that this definitely settled the VC dimension (and hence also the
sample size) question.
However, the above estimate assumes an architecture based on hard-threshold
(""Heaviside"") neurons. In contrast, the usually employed gradient descent learning
algorithms (""backpropagation"" method) rely upon continuous activations, that is,
neurons with graded responses. As pointed out in Sontag (1989), the use of analog activations, which allow the passing of rich (not just binary) information among
levels, may result in higher memory capacity as compared with threshold nets. This
has serious potential implications in learning, essentially because more memory capacity means that a given function f may be able to ""memorize"" in a ""rote"" fashion
too much data, and less generalization is therefore possible. Indeed, Sontag (1992)
showed that there are conceivable (though not very practical) neural architectures
with extremely high VC dimensions. Thus the problem of studying VC(F) for analog networks is an interesting and relevant issue. Two important contributions in
this direction were the papers by Maass (1993) and by Goldberg and Jerrum (1995),
which showed upper bounds on the VC dimension of networks that use piecewise
polynomial activations. The last reference, in particular, established for that case
an upper bound of O(w2), where, as before, w is the number of weights. However
it was an open problem (specifically, ""open problem number 7"" in the recent survey
by Maass (1993) if there is a matching w 2 lower bound for such networks, and more
generally for arbitrary continuous-activation nets. It could have been the case that
the upper bound O( w 2 ) is merely an artifact of the method of proof in Goldberg
and Jerrum (1995), and that reliable learning with continuous-activation networks
is still possible with far smaller sample sizes, proportional to O( w log w). But this is
not the case, and in this paper we answer Maass' open question in the affirmative.
Assume given an activation (T which has different limits at ?oo, and is such that
there is at least one point where it has a derivative and the derivative is nonzero
(this last condition rules out the Heaviside activation). Then there are architectures with arbitrary large numbers of weights wand VC dimension proportional

Neural Networks with Quadratic VC Dimension

199

to w 2 ? The proof relies on first showing that networks consisting of two types of
activations, Heavisides and linear, already have this power. This is a somewhat
surprising result, since purely linear networks result in VC dimension proportional
to w, and purely threshold nets have, as per the results quoted above, VC dimension
bounded by w log w. Our construction was originally motivated by a related one,
given in Goldberg and Jerrum (1995), which showed that real-number programs (in
the Blum-Shub-Smale (1989) model of computation) with running time T have VC
dimension O(T2). The desired result on continuous activations is then obtained,
approximating Heaviside gates by IT-nets with large weights and approximating linear gates by IT-nets with small weights. This result applies in particular to the
standard sigmoid 1/(1 + e- X ). (However, in contrast with the piecewise-polynomial
case, there is still in that case a large gap between our O( w 2 ) lower bound and
the O( w 4 ) upper bound which was recently established in Karpinski and Macintyre (1995).) A number of variations, dealing with Boolean inputs, or weakening
the assumptions on IT, are discussed. The full version of this paper also includes
some remarks on thresholds networks with a constant number of linear gates, and
threshold-only nets with ""shared"" weights.

Basic Terminology and Definitions
Formally, a (first-order, feedforward) architecture or network A is a connected directed acyclic graph together with an assignment of a function to a subset of its
nodes. The nodes are of two types: those of fan-in zero are called input nodes and
the remaining ones are called computation nodes or gates. An output node is a node
of fan-out zero. To each gate g there is associated a function IT g : IR. -!- IR., called the
activation or gate function associated to g.
The number of weights or parameters associated to a gate 9 is the integer ng equal
to the fan-in of 9 plus one. (This definition is motivated by the fact that each input
to the gate will be multiplied by a weight, and the results are added together with
a ""bias"" constant term , seen as one more weight; see below.) The (total) number
of weights (or parameters) of A is by definition the sum of the numbers n g , over all
the gates 9 of A. The number of inputs m of A is the total number of input nodes
(one also says that ""A has inputs in IR.m,,); it is assumed that m > O. The number
of outputs p of A is the number of output nodes (unless otherwise mentioned, we
assume by default that all nets considered have one-dimensional outputs, that is,
p = 1).
Two examples of gate functions that are of particular interest are the identity or
linear gate: Id( x)
x for all x, and the threshold or H eaviside function: H (x) 1
if x ~ 0, H(x) = 0 if x < O.

=

=

Let A be an architecture. Assume that nodes of A have been linearly ordered as
11""1, ... , 11""m, gl, ... , gl, where the 1I""j 's are the input nodes and the gj 's the gates. For
simplicity, write nj := n g ., for each i = 1, ... , I. Note that the total number of
parameters is n = L:~=1 nj and the fan-in of each gj is nj - 1. To each architecture
A (strictly speaking, an architecture together with such an ordering of nodes) we
associate a function
F : ]Rm x ]Rn -!-]RP ,
where p is the number of outputs of A, defined by first assigning an ""output"" to
each node, recursively on the distance from the the input nodes. Assume given
an input x E ]Rm and a vector of weights w E ]Rn. We partition w into blocks
(WI , ... , WI) of sizes nl, ... , nl respectively. First the coordinates of x are assigned
as the outputs of the input nodes 11""1, ... , 1I""m respectively. For each of the other
gates gj, we proceed as follows. Assume that outputs Yl, ... , Yn. -1 have already

P. KOIRAN, E. D. SONTAG

200

been assigned to the predecessor nodes of gi (these are input and/or computation
nodes, listed consistently with the order fixed in advance). Then the output of gi
is by definition
(1'g.

(Wi,O

+ Wi , lYI + Wi ,2Y2 + ... + wi,n.-lYn.-d

,

where we are writing Wi = (Wi,O, Wi,l, Wi ,2, ... , wi,n.-d. The value of F(x, w) is
then by definition the vector (scalar if p = 1) obtained by listing the outputs of the
output nodes (in the agreed-upon fixed ordering of nodes). We call F the function
computed by the architecture A. For each choice of weights W E IRn, there is a
function Fw : IR m _ IRP defined by Fw(x) := F(x, w) ; by abuse of terminology we
sometimes call this also the function computed by A (if the weight vector has been
fixed).
Assume that A is an architecture with inputs in IR m and scalar outputs, and that
the (unique) output gate has range {O, 1}. A subset A ~ IR m is said to be shattered
by A if for each Boolean function 13 : A - {O, 1} there is some weight W E IRn so
that Fw(x) = f3(x) for all x EA . The Vapnik-Chervonenkis (VC) dimension of A
is the maximal size of a subset A ~ IRm that is shattered by A. If the output gate
can take non-binary values, we implicitly assume that the result of the computation
is the sign of the output. That is, when we say that a subset A ~ IR m is shattered
by A , we really mean that A is shattered by the architecture H(A) in which the
output of A is fed to a sign gate .

2

Networks Made up of Linear and Threshold Gates

Proposition 1 For every n ;::: 1, there is a network architecture A with inputs in
IR 2 and O( VN) weights that can shatter a set of size N = n 2. This architecture is

made only of linear and threshold gates.
Proof. Our architecture has n parameters WI , ... , W n ; each of them is an element
ofT {O.WI . .. Wn ;Wi E {O, 1}}. The shattered set will be S = [n]2
{1, .. . ,nF.

=

=

For a given choice of W = (WI' ... ' W n ), A will compute the boolean function
fw : S - {O, 1} defined as follows: fw(x, y) is equal to the x-th bit of W y . Clearly,
for any boolean function f on S, there exists a (unique) W such that f = fw.
We first consider the obvious architecture which computes the function:
n

(1)
- Wz-dH(y - z + 1/2)
z=2
sending each point Y E [n] to W y. This architecture has n - 1 threshold gates,
3(n - 1) + 1 weights, and just one linear gate.

flv(Y) = WI

+ I)Wz

Next we define a second multi-output net which maps wET to its binary representation j2(w) = (WI' . .. ' wn ). Assume by induction that we have a net N?
that maps W to (WI, ... ,Wi,O.Wi+l ... Wn) . Since Wi+l = H(O .Wi+l . .. Wn -1/2)
and o.Wi+2 ... Wn = 2 x o. Wi+1 . .. Wn - Wi+!, .N;;'l can be obtained by adding one
threshold gate and one linear gate to .N;2 (as well as 4 weights). It follows that N~
has n threshold gates, n linear gates and 4n weights.
Finally, we define a net N3 which takes as input x E [n] and W = (WI , ... , w n ) E
{O, l}n, and outputs W X ? We would like this network to be as follows:
n

f3(X , w) = WI

+L
z=2

n

wzH(x - z + 1/2) -

L wz_IH(x z=2

z

+ 1/2).

201

Neural Networks with Quadratic VC Dimension

This is not quite possible, because the products between the Wi'S (which are inputs
in this context) and the Heavisides are not allowed. However, since we are dealing
with binary variables one can write uv = H(u + v - l.5). Thus N3 has one linear
gate, 4(n - 1) threshold gates and 12(n - 1) + n weights. Note that fw(x, y) =
p (x, P Ulv (y)). This can be realized by means of a net that has n + 2 linear gates,
(n-l)+n+4(n-l) = 6n-5 threshold gates, and (3n-2)+4n+(12n-ll) = 19n-13
weights. 0
The following is the main result of this section:
Theorem 1 For every n ;::: 1, there is a network architecture A with inputs in IR.
and O( VN) weights that can shatter a set of size N = n 2. This architecture is
made only of linear and threshold gates.
Proof. The shattered set will be S = {O, 1, .. . ,n 2 -I}. For every xES, there
are unique integers x, y E {O, 1, ... , n - I} such that u = nx + y. The idea of the
construction is to compute x and y, and then feed (x + 1, y + 1) to the network
constructed in Proposition 1. Note that x is the unique integer such that u - nx E
{O, 1, .. . , n - I}. It can therefore by computed by brute force search as follows:
n-1
X

=

L kH[H(u -

nk)

+ H(n -

1 - (u - nk)) - l.5].

k=O

This network has 3n threshold gates, one linear gate and 8n weights. Then of course
y = u - nx. 0
A Boolean version is as follows.
Theorem 2 For every d ;::: 1, there is a network architecture A with O( VN)
weights that can shatter the N = 22d points of {O, 1F d . This architecture is made
only of linear and threshold gates.

d, one can compute x = 1 + 2::=1 2i-1ui and y 1 +
Proof. Given u E {O,
2:1=12i-1Ui+d with two linear gates. Then (x, y) can be fed to the network of
Proposition 1 (with n 2d ). 0

=

IF

=

In other words, there is a network architecture with 2d weights that can compute
all boolean functions on 2d variables.

3

Arbitrary Sigmoids

We now extend the preceding VC dimension bounds to networks that use just
one activation function tr (instead of both linear and threshold gates). All that is
required is that the gate function have a sigmoidal shape and satisfy a very weak
smoothness property:
l. tr is differentiable at some point Xo (i.e., tr(xo+h) = tr(xo)+tr'(xo)h+o(h))
where tr'(xo)# 0.
2. limx __ oo tr(x) = and limx _+ oo tr(x) = 1 (the limits and 1 can be
replaced by any distinct numbers).

?

?

A function satisfying these two conditions will be called sigmoidal. Given any such

tr, we will show that networks using only tr gates provide quadratic VC dimension.

P. KOIRAN, E. D. SONTAG

202

Theorem 3 Let tT be an arbitrary sigmoidal function. There exist architectures Al
and A2 with O( VN) weights made only of tT gates such that:

? Al can shatter a subset ofIR of cardinality N = n 2 ,-

? A2 can shatter the N = 22d points of {O, 1}2d.
This follows directly from Theorems 1 and 2, together with the following simulation
result:

Theorem 4 Let tT be a an arbitrary sigmoidal function. Let N be a network of

T threshold and L linear gates, with a threshold gate at the output. Then N can

be simulated on any given finite set of inputs by a network N' of T + L gates that
all use the activation function tT (except the output gate which is still a threshold).
Moreover, if N has n weights then N' has O( n) weights.

Proof. Let S be a finite set of inputs. We can assume, by changing the thresholds of
threshold gates if necessary, that the net input Ig (x) to any threshold gate 9 of N
is different from for all inputs xES.

?

Given ? > 0, let N( be the net obtained by replacing the output functions of all gates
by the new output function x 1--+ tT( X / ?) if this output function is the sign function ,
and by x 1--+ tT(x) = [tT(xo+?x)-tT(xo))/[?tT'(xo)] ifit is the identity function. Note
that for any a > 0, lim(_o+ tT(x/?) = H(x) uniformly for x E) - 00, -a] U [a, +00]
and limHo tT(x) = x uniformly for x E [-l/a, l/a].
This implies by induction on the depth of 9 that for any gate 9 of N and any input
XES, the net input Ig,(x) to 9 in the transformed net N( satisfies li~_o IgAx) =
Ig(x) (here, we use the fact that the output function of every 9 is continuous at
Ig(x)). In particular, by taking 9 to be the output gate of N, we see that Nand
N( compute the same function on S if ? is small enough. Such a net N( can be
transformed into an equivalent net N' that uses only tT as gate function by a simple
transformation of its weights and thresholds. The number of weights remains the
same, except at most for a constant term that must be added to each net input to
a gate; thus if N has n weights, N' has at most 2n weights. 0

4

More General Gate Functions

The objective of this section is to establish results similar to Theorem 3, but for
even more arbitrary gate functions, in particular weakening the assumption that
limits exist at infinity. The main result is, roughly, that any tT which is piecewise
twice (continuously) differentiable gives at least quadratic VC dimension, save for
certain exceptional cases involving functions that are almost everywhere linear.
A function tT : IR --+ IR is said to be piecewise C 2 if there is a finite sequence
al < a2 < ... < a p such that on each interval I of the form] - 00, al [, )ai, ai+1 [ or
]a p , +00[, tTll is C2.

(Note: our results hold even if it is only assumed that the second derivative exists in
each of the above intervals; we do not use the continuity of these second derivatives.)

Theorem 5 Let tT be a piecewise C2 function. For every n ~ 1, there exists an
architecture made of tT-gates, and with O( n) weights, that can shatter a subset of
IR 2 of cardinality n 2 , except perhaps in the following cases:

1. tT is piecewise-constant, and in this case the VC dimension of any architecture of n weights is O( n log n),-

Neural Networks with Quadratic VC Dimension

203

2. u is affine, and in this case the VC dimension of any architecture of n
weights is at most n.
3. there are constants af; 0 and b such that u( x) = ax + b except at a finite
nonempty set of points. In this case, the VC dimension of any architecture of n weights is O(n 2 ), and there are architectures of VC dimension

O(nlogn).
Due to the lack of space, the proof cannot be included in this paper. Note that
the upper bound of the first special case is tight for threshold nets, and that of the
second special case is tight for linear functions in ]R n.
Acknowledgements

Pascal Koiran was supported by an INRIA fellowship , DIMACS, and the International Computer Science Institute. Eduardo Sontag was supported in part by US
Air Force Grant AFOSR-94-0293 .
References
M . ANTHONY AND N.L. BIGGS (1992) Computational Learning Th eory: An Introduction,
Cambridge U. Press.
E .B. BAUM AND D . HAUSSLER (1989) What size net gives valid generalization?, Neural
Computation 1, pp. 151-160.
L. BLUM, M. SHUB AND S. SMALE (1989) On the theory of computation and complexity over the real numbers: NP-completeness, recursive functions and universal machines,
Bulletin of the AMS 21 , pp. 1- 46 .
A. BLUMER, A . EHRENFEUCHT, D . HAUSSLER, AND M . WARMUTH (1989) Learnability
and the Vapnik- Chervonenkis dimension , J. of the ACM 36, pp. 929-965.
T.M. COVER (1988) Capacity problems for linear machines, in: Pattern Recognition , L.
Kanal ed. , Thompson Book Co., pp. 283-289.
P. GOLDBERG AND M . JERRUM (1995) Bounding the Vapnik-Chervonenkis dim ension of
concept classes parametrized by real numbers, Machine Learning 18, pp. 131-148.
M . KARPINSKI AND A. MACINTYRE (1995) Polynomial bounds for VC dimension of sigmoidal neural networks, in Proc. 27th ACM Symposium on Theory of Computing, pp. 200208.
W. MAASS (1993) Bounds for the computational power and learning complexity of analog
neural nets, in Proc. of the 25th ACM Symp. Theory of Computing, pp. 335-344.
W . MAASS (1994) Perspectives of current research about the complexity of learning in neural nets, in Theoretical Advances in N eural Computation and Learning , V.P. Roychowdhury, K.Y. Siu, and A . Orlitsky, editors, Kluwer, Boston , pp. 295-336.
B.K . NATARAJAN (1991) Machine Learning : A Theoretical Approach, M . Kaufmann Publishers, San Mateo , CA.
E .D. SONTAG (1989) Sigmoids distinguish better than Heavisides, Neural Computation 1,
pp. 470-472.
E.D. SONTAG (1992) Feedforward nets for interpolation and classification, J. Comp o
Syst. Sci 45 , pp. 20-48.
L.G. VALIANT (1984) A th eory of the learnable, Comm. of the ACM 27, pp. 1134-1142
V .N. VAPNIK (1982) Estimation of Dependencies Based on Empirical Data, Springer,
Berlin.

"
1052,1995,Learning the Structure of Similarity,,1052-learning-the-structure-of-similarity.pdf,Abstract Missing,"Learning the structure of similarity

Joshua B. Tenenbaum
Department of Brain and Cognitive Sciences
Massachusetts Institute of Technology
Cambridge, MA 02139
jbt~psyche.mit.edu

Abstract
The additive clustering (ADCL US) model (Shepard & Arabie, 1979)
treats the similarity of two stimuli as a weighted additive measure
of their common features. Inspired by recent work in unsupervised
learning with multiple cause models, we propose anew, statistically
well-motivated algorithm for discovering the structure of natural
stimulus classes using the ADCLUS model, which promises substantial gains in conceptual simplicity, practical efficiency, and solution
quality over earlier efforts. We also present preliminary results with
artificial data and two classic similarity data sets.

1

INTRODUCTION

The capacity to judge one stimulus, object, or concept as similado another is thought
to play a pivotal role in many cognitive processes, including generalization , recognition, categorization, and inference. Consequently, modeling subjective similarity
judgments in order to discover the underlying structure of stimulus representations
in the brain/mind holds a central place in contemporary cognitive science. Mathematical models of similarity can be divided roughly into two families: spatial models,
in which stimuli correspond to points in a metric (typically Euclidean) space and
similarity is treated as a decreasing function of distance; and set-theoretic models, in
which stimuli are represented as members of salient subsets (presumably corresponding to natural classes or features in the world) and similarity is treated as a weighted
sum of common and distinctive subsets.
Spatial models, fit to similarity judgment data with familiar multidimensional scaling (MDS) techniques, have yielded concise descriptions of homogeneous, perceptual
domains (e.g. three-dimensional color space), often revealing the salient dimensions
of stimulus variation (Shepard, 1980). Set-theoretic models are more general , in
principle able to accomodate discrete conceptual structures typical of higher-level
cognitive domains, as well as dimensional stimulus structures more common in per-

4

1. B. TENENBAUM

ception (Tversky, 1977). In practice, however, the utility of set-theoretic models is
limited by the hierarchical clustering techniques that underlie conventional methods
for discovering the discrete features or classes of stimuli. Specifically, hierarchical
clustering requires that any two classes of stimuli correspond to disjoint or properly
inclusive subsets, while psychologically natural classes may correspond in general to
arbitrarily overlapping subsets of stimuli. For example, the subjective similarity of
two countries results from the interaction of multiple geographic and cultural factors, and there is no reason a priori to expect the subsets of communist, African, or
French-speaking nations to be either disjoint or properly inclusive.
In this paper we consider the additive clustering (ADCL US) model (Shepard & Arabie, 1979), the simplest instantiation of Tversky 's (1977) general contrast model that
accommodates the arbitrarily overlapping class structures associated with multiple
causes of similarity. Here, the similarity of two stimuli is modeled as a weighted
additive measure of their common clusters:
K

Sij

=

I:

wkfikfJk

+ C,

(1)

k=l

where Sij is the reconstructed similarity of stimuli i and j, the weight Wk captures
the salience of cluster k, and the binary indicator variable fik equals 1 if stimulus i
belongs to cluster k and 0 otherwise. The additive constant c is necessary because the
similarity data are assumed to be on an interval scale. 1 As with conventional clustering models, ADCLUS recovers a system of discrete subsets of stimuli, weighted by
salience, and the similarity of two stimuli increases with the number (and weight)
of their common subsets. ADCLUS, however, makes none of the structural assumptions (e.g. that any two clusters are disjoint or properly inclusive) which limit the
applicability of conventional set-theoretic models. Unfortunately this flexibility also
makes the problem of fitting the ADCL US model to an observed similarity matrix
exceedingly difficult.
Previous attempts to fit the model have followed a heuristic strategy to minimize a
squared-error energy function ,

E

= I:(Sij - Sij)2 = I:(Sij itj

itj

I:

wklikfJk)2,

(2)

k

by alternately solving for the best cluster configurations fik given the current weights
Wk and solving for the best weights given the current clusters (Shepard & Arabie,
1979; Arabie & Carroll, 1980). This strategy is appealing because given the cluster configuration, finding the optimal weights becomes a simple linear least-squares
problem.2 However, finding good cluster configurations is a difficult problem in combinatorial optimization, and this step has always been the weak point in previous
work . The original ADCLUS (Shepard & Arabie, 1979) and later MAPCLUS (Arabie & Carroll, 1980) algorithms employ ad hoc techniques of combinatorial optimization that sometimes yield unexpected or uninterpretable final results. Certainly, no
rigorous theory exists that would explain why these approaches fail to discover the
underlying structure of a stimulus set when they do.
Essentially, the ADCL US model is so challenging to fit because it generates similarities from the interaction of many independent underlying causes . Viewed this way,
modeling the structure of similarity looks very similar to the multiple-cause learning
In the remainder of this paper, we absorb c into the sum over k, taking the sum over
== c, and fixing !iO = 1, (Vi) .
2Strictly speaking, because the weights are typically constrained to be nonnegative, more
elaborate techniques than standard linear least-squares procedures may be required.
1

k

= 0, ... , K , defining Wo

5

Learning the Structure of Similarity

problems that are currently a major focus of study in the neural computation literature (Ghahramani, 1995; Hinton, Dayan, et al., 1995; Saund, 1995; Neal, 1992). Here
we propose a novel approach to additive clustering, inspired by the progress and
promise of work on multiple-cause learning within the Expectation-Maximization
(EM) framework (Ghahramani, 1995; Neal, 1992). Our BM approach still makes
use of the basic insight behind earlier approaches, that finding {wd given {lid is
easy, but obtains better performance from treating the unknown cluster memberships
probabilistically as hidden variables (rather than parameters of the model), and perhaps more importantly, provides a rigorous and well-understood theory. Indeed, it
is natural to consider {/ik} as ""unobserved"" features of the stimuli, complementing the observed data {Sij} in the similarity matrix. Moreover, in some experimental
paradigms, one or more of these features may be considered observed data, if subjects
report using (or are requested to use) certain criteria in their similarity judgments.

2

ALGORITHM

2.1

Maximum likelihood formulation

We begin by formulating the additive clustering problem in terms of maximum likelihood estimation with unobserved data. Treating the cluster weights w
{Wk}
as model parameters and the unobserved cluster memberships I = {lik} as hidden
causes for the observed similarities S {Sij}, it is natural to consider a hierarchical
generative model for the ""complete data"" (including observed and unobserved components) of the form p(s, Ilw) = p(sl/, w)p(flw). In the spirit of earlier approaches
to ADCLUS that seek to minimize a squared-error energy function, we take p(sl/, w)
to be gaussian with common variance u 2 :

=

=

p(sl/, w) ex: exp{ -~ 'L:(Sij - Sij )2} = exp{ -~ 'L:(Sij 2u itj
2u itj

'L: wklik/ik)2}.

(3)

k

Note that logp(sl/, w) is equivalent to -E/(2u 2 ) (ignoring an additive constant),
where E is the energy defined above. In general, priors p(flw) over the cluster
configurations may be useful to favor larger or smaller clusters, induce a dependence
between cluster size and cluster weight, or bias particular kinds of class structures,
but only uniform priors are considered here. In this case -E /(2u 2 ) also gives the
""complete data"" loglikelihood logp(s, Ilw).

2.2

The EM algorithm for additive clustering

Given this probabilistic model, we can now appeal to the EM algorithm as the basis
for a new additive clustering technique. EM calls for iterating the following twostep procedure, in order to obtain successive estimates of the parameters w that are
guaranteed never to decrease in likelihood (Dempster et al., 1977). In the E-step, we
calculate

Q(wlw(n)) =

L,: p(f' Is, wen)) logp(s,f/lw) =
l'

2 \ (-E}3,w(n).

(4)

u

Q(wlw(n) is equivalent to the expected value of E as a function of w, averaged over

all possible configurations I' of the N K binary cluster memberships, given the observed data s and the current parameter estimates wen). In the M-step, we maximize
Q(wlw(n) with respect to w to obtain w(n+l).
Each cluster configuration I' contributes to the mean energy in proportion to its
probability under the gaussian generative model in (3). Thus the number of configurations making significant contributions depends on the model variance u 2 . For large

6

J. B. TENENBAUM

the probability is spread over many configurations. In the limiting case u 2 ---+ 0,
only the most likely configuration contributes, making EM effectively equivalent to
the original approaches presented in Section 1 that use only the single best cluster
configuration to solve for the best cluster weights at each iteration.
U2 ,

In line with the basic insight embodied less rigorously in these earlier algorithms, the
M-step still reduces to a simple (constrained) linear least-squares problem, because
the mean energy (E} = L:i#j (srj - 2Sij L:k Wk(fik!ik} + L:kl WkWl(fik!jk!il!il}) ,
like the energy E, is quadratic in the weights Wk. The E-step, which amounts to
computing the expectations mijk = (fik!ik} and mijkl = (fik !ik!il/j I} , is much
more involved , because the required sums over all possible cluster configurations f'
are intractable for any realistic case. We approximate these calculations using Gibbs
sampling, a Monte Carlo method that has been successfully applied to learning similar
generative models with hidden variables (Ghahramani, 1995; Neal 1992).3
Finally, the algorithm should produce not only estimates of the cluster weights, but
also a final cluster configuration that may be interpreted as the psychologically natural
features or classes of the relevant domain. Consider the expected cluster memberships
Pik = (fik}$ w(n) , which give the probability that stimulus i belongs to cluster k, given
the observed similarity matrix and the current estimates of the weights. Only when
all Pik are close to 0 or 1, i.e. when u 2 is small enough that all the probability becomes
concentrated on the most likely cluster configuration and its neighbors, can we fairly
assert which stimuli belong to which classes.
2.3

Simulated annealing

Two major computational bottlenecks hamper the efficiency of the algorithm as described so far. First, Gibbs sampling may take a very long time to converge to the
equilibrium distribution, particularly when u 2 is small relative to the typical energy
difference between neighboring cluster configurations. Second, the likelihood surfaces
for realistic data sets are typically riddled with local maxima. We solve both problems
by annealing on the variance. That is, we run Gibbs sampling using an effective variance
initially much greater than the assumed model variance 2 , and decrease
towards u 2 according to the following two-level scheme. We anneal within the
nth iteration of EM to speed the convergence of the Gibbs sampling E-step (Neal,
1993) , by lowering u;jJ from some high starting value down to a target U~arg(n) for
the nth EM iteration . We also anneal between iterations of EM to avoid local maxima
(Ros~ et al., 1990), by intializing U~arg(o) at a high value and taking U~arg(n) ---+ u 2
as n Increases.

u;""

3

u;""

u

RESULTS

In all of the examples below, one run of the algorithm consisted of 100-200 iterations
of EM, annealed both within and between iterations. Within each E-step, 10-100
cycles of Gibbs sampling were carried out at the target temperature UTarg while the
statistics for mik and mijk were recorded. These recorded cycles were preceeded
by 20-200 unrecorded cycles, during which the system was annealed from a higher
temperature (e.g. 8u~arg) down to U~arg, to ensure that statistics were collected as
close to equilibrium as possible. The precise numbers of recorded and unrecorded
iterations were chosen as a compromise between the need for longer samples as the
3We generally also approximate
sults with much greater efficiency.

miJkl

~

miJkmi;""l,

which usually yields satisfactory re-

7

Learning the Structure of Similarity

Table 1: Classes and weights recovered for the integers 0-9.
Rank
1
2
3
4
5
6
7
8

Weight
.444
.345
.331
.291
.255
.216
.214
.172

Variance accounted for

Stimuli in class
2
4

8

012

3

9
6
6 789
2 345 6
1
3
5
7
9
1 2 3 4
4 5 6 7 8

= 90.9% with

Interpretation
powers of two
small numbers
multiples of three
large numbers
middle numbers
odd numbers
smallish numbers
largish numbers

8 clusters (additive constant

= .148).

number of hidden variables is increased and the need to keep computation times
practical.
3.1

Artificial data

We first report results with artificial data, for which the true cluster memberships and
weights are known, to verify that the algorithm does in fact find the desired structure.
We generated 10 data sets by randomly assigning each of 12 stimuli independently
and with probability 1/2 to each of 8 classes, and choosing random weights for the
classes uniformly from [0.1,0.6]. These numbers are grossly typical of the real data
sets we examine later in this section. We then calculated the observed similarities
from (1), added a small amount of random noise (with standard deviation equal to
5% of the mean noise-free similarity), and symmeterized the similarity matrix.
The crucial free parameter is K, the assumed number of stimulus classes. When the
algorithm was configured with the correct number of clusters (K = 8), the original
classes and weights were recovered during the first run of the algorithm on all 10 data
sets, after an average of 58 EM iterations (low 30, high 92). When the algorithm
was configured with K = 7 clusters, one less than the correct number, the seven
classes with highest weight were recovered on 9/10 first runs. On these runs, the
recovered weights and true weights had a mean correlation of 0.948 (p < .05 on each
run). When configured with K = 5, the first run recovered either four of the top
five classes (6/10 trials) or three of the top five (4/10 trials). When configured with
too many clusters (K = 12), the algorithm typically recovered only 8 clusters with
significantly non-zero weights, corresponding to the 8 correct classes. Comparable
results are not available for ADCLUS or MAPCLUS, but at least we can be satisfied
that our algorithm achieves a basic level of competence and robustness.
3.2

Judged similarities of the integers 0-9

Shepard et al. (1975) had subjects judge the similarities of the integers 0 through
9, in terms of the ""abstract concepts"" of the numbers. We analyzed the similarity
matrix (Shepard, personal communication) obtained by pooling data across subjects
and across three conditions of stimulus presentation (verbal, written-numeral, and
written-dots). We chose this data set because it illustrates the power of additive
clustering to capture a complex, overlapping system of classes, and also because
it serves to compare the performance of our algorithm with the original ADCL US
algorithm. Observe first that two kinds of classes emerge in the solution. Classes
1, 3, and 6 represent familiar arithmetic concepts (e.g. ""multiples of three"", ""odd
numbers""), while the remaining classes correspond to subsets of consecutive integers

8

1. B. TENENBAUM

Table 2: Classes and weights recovered for the 16 consonant phonemes.
Rank
1
2
3
4
5
6
7
8

Weight
.800
.572
.463
.424
.357
.292
.169
.132

Stimuli in class
f 0
dg

p k
b

v {t

pt k
mn
dgvCTz2

ptkfOs

Interpretation
front unvoiced fricatives
back voiced stops
unvoiced stops (omitting t)
front voiced
unvoiced stops
nasals
voiced (omitting b)
unvoiced (omittings)

Variance accounted for = 90.2% with 8 clusters (additive constant = .047).

and thus together represent the dimension of numerical magnitude. In general, both
arithmetic properties and numerical magnitude contribute to judged similarity, as
every number has features of both types (e.g. 9 is a ""large"" ""odd"" ""multiple of three""),
except for 0, whose only property is ""small."" Clearly an overlapping clustering model
is necessary here to accomodate the multiple causes of similarity.
The best solution reported for these data using the original ADCLUS algorithm
consisted of 10 classes, accounting for 83.1% of the variance of the data (Shepard &
Arabie, 1979).4 Several of the clusters in this solution differed by only one or two
members (e.g. three of the clusters were {0,1}, {0,1,2}, and {0,1,2,3,4}), which led
us to suspect that a better fit might be obtained with fewer than 10 classes. Table 2
shows the best solution found in five runs of our algorithm, accounting for 90.9% of
the variance with eight classes. Compared with our solution, the original ADCLUS
solution leaves almost twice as much residual variance unaccounted for, and with 10
classes, is also less parsimonious.

3.3

Confusions between 16 consonant phonemes

Finally, we examine Miller & Nicely's (1955) classic data on the confusability of 16
consonant phonemes, collected under varying signal/noise conditions with the original intent of identifying the features of English phonology (compiled and reprinted
in Carroll & Wish, 1974). Note that the recovered classes have reasonably natural
interpretations in terms of the basic features of phonological theory, and a very different overall structure from the classes recovered in the previous example. Quite
significantly, the classes respect a hierarchical structure almost perfectly, with class
3 included in class 5, classes 1 and 5 included in class 8, and so on. Only the absence
of /b / in class 7 violates the strict hierarchy.
These data also provide the only convenient oppportunity to compare our algorithm
with the MAPCLUS approach to additive clustering (Arabie & Carroll, 1980). The
published MAPCLUS solution accounts for 88.3% of the variance in this data, using
eight clusters. Arabie & Carroll (1980) report being ""substantively pe...turbed"" (p.
232) that their algorithm does not recover a distinct cluster for the nasals /m n/,
which have been considered a very salient subset in both traditional phonology (Miller
& Nicely, 1955) and other clustering models (Shepard, 1980). Table 3 presents our
eight-cluster solution, accounting for 90.2% of the variance. While this represents
only a marginal improvement, our solution does contain a cluster for the nasals, as
expected on theoretical grounds.
4Variance accounted for = 1- Ej Ei#j(SiJ - 8)2, where

s is

the mea.n of the set {Sij}.

Learning the Structure of Similarity

3.4

9

Conclusion

These examples show that ADCLUS can discover meaningful representations of stimuli with arbitrarily overlapping class structures (arithmetic properties), as well as dimensional structure (numerical magnitude) or hierarchical structure (phoneme families) when appropriate. We have argued that modeling similarity should be a natural
application of learning generative models with multiple hidden causes, and in that
spirit, presented a new probabilistic formulation of the ADCLUS model and an algorithm based on EM that promises better results than previous approaches. We
are currently pursuing several extensions: enriching the generative model, e.g. by
incorporating significant prior structure, and improving the fitting process, e.g. by
developing efficient and accurate mean field approximations . More generally, we hope
this work illustrates how sophisticated techniques of computational learning can be
brought to bear on foundational problems of structure discovery in cognitive science.
Acknowledgements
I thank P. Dayan, W. Richards, S. Gilbert, Y. Weiss, A. Hershowitz, and M. Bernstein
for many helpful discussions, and Roger Shepard for generously supplying inspiration and
unpublished data. The author is a Howard Hughes Medical Institute Predoctoral Fellow.

References
Arabie, P. & Carroll, J. D. (1980). MAPCLUS: A mathematical programming approach to
fitting the ADCLUS model. Psychometrika 45, 211-235.
Carroll, J. D. & Wish, M. (1974) Multidimensional perceptual models and measurement
methods. In Handbook of Perception, Vol. 2. New York: Academic Press, 391-447.
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood estimation from
incomplete data via the EM Algorithm (with discussion). J. Roy. Stat. Soc. B39, 1-38.
Ghahramani, Z. (1995). Factorial learning and the EM algorithm. In G. Tesauro, D. S.
Touretzky, & T . K. Leen (eds.), Advances in Neural Information Processing Systems 7.
Cambridge, MA: MIT Press, 617-624.
Hinton, G. E., Dayan, P., Frey, B. J., & Neal, R. M. (1995) The ((wake-sleep"" algorithm for
unsupervised neural networks. Science 268, 1158-1161.
Miller, G. A. & Nicely, P. E. (1955). An analysis of perceptual confusions among some
English consonants. J. Ac. Soc. Am. 27, 338-352.
Neal, R . M. (1992). Connectionist learning of belief networks. Arti/. Intell. 56, 71-113.
Neal, R. M. (1993). Probabilistic inference using Markov chain Monte Carlo methods.
Technical Report CRG-TR-93-1, Dept. of Computer Science, U. of Toronto.
Rose, K., Gurewitz, F., & Fox, G. (1990). Statistical mechanics and phase transitions in
clustering. Physical Review Letters 65, 945-948.
Saund, E. (1995). A multiple cause mixture model for unsupervised learning. Neural Computation 7, 51-71.
Shepard, R. N. & Arabie, P. (1979). Additive clustering: Representation of similarities as
combinations of discrete overlapping properties. Psychological Review 86, 87-123.
Shepard, R. N., Kilpatric, D. W., & Cunningham, J. P., (1975). The internal representation
of numbers. Cognitive Psychology 7, 82-138.
Shepard, R. N. (1980) . Multidimensional scaling, tree-fitting, and clustering. Science 210,
390-398.
Tversky, A. (1977). Features of similarity. Psychological Review 84, 327-352.

"
1053,1995,Reorganisation of Somatosensory Cortex after Tactile Training,,1053-reorganisation-of-somatosensory-cortex-after-tactile-training.pdf,Abstract Missing,"Reorganisation of Somatosensory Cortex after
Tactile Training

Rasmus S. Petersen
John G. Taylor
Centre for Neural Networks, King's College London
Strand, London WC2R 2LS, UK

Abstract
Topographic maps in primary areas of mammalian cerebral cortex reorganise as a result of behavioural training. The nature of this reorganisation seems consistent with the behaviour of competitive neural networks, as has been demonstrated in the past by computer simulation.
We model tactile training on the hand representation in primate somatosensory cortex, using the Neural Field Theory of Amari and his colleagues. Expressions for changes in both receptive field size and magnification factor are derived, which are consistent with owl monkey experiments and make a prediction which goes beyond them.

1. INTRODUCTION
The primary cortical areas of mammals are now known to be plastic throughout life; reviewed recently by Kaas(1995). The problem of how and why the underlying learning
processes work is an exciting one, for which neural network modelling appears well
suited. In this contribution, we model the long-term effects of tactile training (Jenkins et
ai, 1990) on the functional organisation of monkey primary somatosensory cortex, by
perturbing a topographic net (Takeuchi and Amari, 1979).

1.1 ADAPTATION IN ADULT SOMATOSENSORY CORTEX
Light touch activates skin receptors which in primates are mapped, largely topographically, in area 3b. In a series of papers, Merzenich and colleagues describe how area 3b
becomes reorganised following peripheral nerve damage (Merzenich et ai, 1983a; 1983b)
or digit amputation (Merzenich et ai, 1984). The underlying learning processes may also
explain the phenomenon of phantom limb ""telescoping"" (Haber, 1955). Recent advances
in brain scanning are beginning to make them observable even in the human brain
(Mogilner et ai, 1993).

1.2 ADAPTATION ASSOCIATED WITH TACTILE TRAINING
Jenkins et al trained owl monkeys to maintain contact with a rotating disk. The apparatus
was arranged so that success eventually involved touching the disk with only the digit
tips. Hence these regions received selective stimulation. Some time after training had
been completed electro-physiological recordings were made from area 3b. These revealed an increase in Magnification Factor (MF) for the stimulated skin and a decrease in

83

Reorganization of Somatosensory Cortex after Tactile Training

the size of Receptive Fields (RFs) for that region. The net territory gained for light touch
of the digit tips came from area 3a and/or the face region of area 3b, but details of any
changes in these representations were not reported.

2. THEORETICAL FRAMEWORK
2.1 PREVIOUS WORK
Takeuchi and Amari(1979), Ritter and Schulten(1986), Pearson et al(1987) and Grajski
and Merzenich( 1990) have all modelled amputationldenervation by computer simulation
of competitive neural networks with various Hebbian weight dynamics. Grajski and
Merzenich(1990) also modelled the data of Jenkins et al. We build on this research
within the Neural Field Theory framework (Amari, 1977; Takeuchi and Amari, 1979;
Amari, 1980) of the Neural Activity Model of Willshaw and von der Malsburg(1976).

2.2 NEURAL ACTIVITY MODEL
Consider a ""cortical"" network of simple, laterally connected neurons. Neurons sum inputs linearly and output a sigmoidal function of this sum. The lateral connections are
excitatory at short distances and inhibitory at longer ones. Such a network is competitive: the steady state consists of blobs of activity centred around those neurons locally receiving the greatest afferent input (Amari, 1977). The range of the competition is limited
by the range of the lateral inhibition.
Suppose now that the afferent synapses adapt in a Hebbian manner to stimuli that are localised in the sensory array; the lateral ones are fixed. Willshaw and von der Malsburg(1976) showed by computer simulation that this network is able to form a topographic map of the sensory array. Takeuchi and Amari( 1979) amended the WillshawMalsburg model slightly: neurons possess an adaptive firing threshold in order to prevent
synaptic weight explosion, rather than the more usual mechanism of weight normalisation. They proved that a topographic mapping is stable under certain conditions.

2.3 TAKEUCHI-AMARI THEORY
Consider a one-dimensional model. The membrane dynamics are:

au(~y,t) = -u(x,y,t)+ f s(x,y' ,t)a(y- y')dy'-

(1)

f

so(x,t)ao + w(x-x')f[u(x' ,y,t)]dx'-h
Here u(x,y,t) is the membrane potential at time I for point x when a stimulus centred at y is
being presented; h is a positive resting potential; w(z) is the lateral inhibitory weight between two points in the neural field separated by a distance z - positive for small Izl and
negative for larger Izl; s(x,y,t) is the excitatory synaptic weight from y to x at time I and
sr/X,I) is an inhibitory weight from a tonically active inhibitory input aD to x at time t - it is
the adaptive firing threshold . f[u] is a binary threshold function that maps positive membrane potentials to 1 and non-positive ones to O.
Idealised, point-like stimuli are assumed, which ""spread out"" somewhat on the sensory
surface or subcortically. The spreading process is assumed to be independent of y and is
described in the same coordinates. It is represented by the function a(y-y'), which describes the effect of a point input at y spreading to the point y'. This is a decreasing, positive, symmetric function of Iy-y'l. With this type of input, the steady-state activity of the
network is a single blob, localised around the neuron with maximum afferent input.

R. S. PETERSEN, J. O. TAYLOR

84

The afferent synaptic weights adapt in a leaky Hebbian manner but with a time constant
much larger than that of the membrane dynamics (1). Effectively this means that learning
occurs on the steady state of the membrane dynamics. The following averaged weight
dynamics can be justified (Takeuchi and Amari, 1979; Geman 1979):

J) (

as( x,aty, t) =-s(x,y,t)+b p(y' a Y-Y')f [Au(x,y' )]dy'
(2)

aso(~y,t) =-so(x,y,t)+b' aoJ p(y')f[u(x,y')]dy'

where r1(x,y') is the steady-state of the membrane dynamics at x given a stimulus at y' and
p(y') is the probability of a stimulus at y '; b, b' are constants.
Empirically, the ""classical"" Receptive Field (RF) of a neuron is defined as the region of
the input field within which localised stimulation causes change in its activity. This concept can be modelled in neural field theory as: the RF of a neuron at x is the portion of the
input field within which a stimulus evokes a positive membrane potential (inhibitory RFs
are not considered). If the neural field is a continuous map of the sensory surface then the
RF of a neuron is fully described by its two borders rdx), rix), defined formally:
i

= 1,2

(3)

which are illustrated in figure 1.
Let RF size and RF position be denoted respectively by the functions rex) and m(x), which
represent experimentally measurable quantities. In terms of the border functions they can
be expressed:

r(x) = r2 (x) - r1 (x)

(4)

m(x) =-} (rl {x} + r2 (x))
y

~---------------------------

x

Figure 1:
RF
boundaries as a
function of position
in the neural field,
for a topographically ordered network. Only the region
in-between
rdx) and rix) has
positive
steadystate
membrane
potential
r1(x,y).
rdx) and rix) are
defined
by
the
condition
r1(x,r;(x))=O
for
i=J,2.

Using (1), (2) and the definition (3), Takeuchi and Amari(1979) derived dynamical equations for the change in RF borders due to learning. In the case of uniform stimulus probability, they found solutions for the steady-state RF border functions. With periodic
boundary conditions, the basic solution is a linear map with constant RF size:

85

Reorganization of Somatosensory Cortex after Tactile Training

r(x) = ro = const
m(x) = px ++ro

= px
r~tni (x) = px+ ro

r l uni ( x )

(5)

This means that both RF size and activity blob size are uniform across the network and
that RF position m(x) is a linear function of network location. (The value of p is determined by boundary conditions; ro is then determined from the joint equilibrium of (I),
(2?. The inverse of the RF position function, denoted by m-l(y), is the centre of the cortical active region caused by a stimulus centred at y. The change in m-l(y) over a unit interval in the input field is, by empirical definition, the cortical magnification factor (MF).
Here we model MF as the rate of change of m-l(y). The MF for the system described by
(5) is:
d
_I ( )
-m
y =p -I

(6)

dy

3. ANALYSIS OF TACTILE TRAINING
3.1 TRAINING MODEL AND ASSUMPTIONS
Jenkins et aI's training sessions caused an increase in the relative frequency of stimulation
to the finger tips, and hence a decrease in relative frequency of stimulation elsewhere.
Over a long time, we can express this fact as a localised change in stimulus probability
(figure 2). (This is not sufficient to cause cortical reorganisation - Recanzone et al( 1992)
showed that attention to the stimulation is vital. We consider only attended stimulation in
this model). To account for such data it is clearly necessary to analyse non-uniform
stimulus probabilities, which demands extending the results of Takeuchi and Amari. Unfortunately, it seems to be hard to obtain general results. However, a perturbation analysis around the uniform probability solution (5) is possible.
To proceed in this way, we must be able to assume that the change in the stimulus probability density function away from uniformity is small. This reasoning is expressed by the
following equation:

p(y) = Po + E p(y)

(7)

where pry) is the new stimulus probability in terms of the uniform one and a perturbation
due to training: E is a small constant. The effect of the perturbation is to ease the weight
dynamics (2) away from the solution (5) to a new steady-state. Our goal is to discover the
effect of this on the RF border functions, and hence for RF size and MF.

p(y)

Figure 2: The type
of
change
in
stimulus probability density that we
assume to model
the effects of behavioural training.

o

y

86

R. S. PETERSEN, J. G. TAYLOR

3.2 PERTURBATION ANALYSIS
3.2.1 General Case
For a small enough perturbation, the effect on the RF borders and on the activity blob size
ought also to be small. We consider effects to first order in E, seeking new solutions of
the form:
i

= 1,2

,{x} = r; {x} - ~ {x}
m{x} = +(~ (X}+'2 (x})

(8)

where the superscript peT denotes the new, perturbed equilibrium and uni denotes the unperturbed, uniform probability equilibrium. Using (1) and (2) in (3) for the post-training
RF borders, expanding to first order in E, a pair of difference equations may be obtained
for the changes in RF borders. It is convenient to define the following terms:

J

rt '(x)

o

r,""no (x)

ro

At (x) = p(y+ px)k(y)dy-b' a~
o

Jp(y)dy
r;-n' (x )

Jp(y + px + TO )k(y)dy - b' a~ Jp(y)dy
k(y) = bJa(y - y' )a(y' )dy'

A2 {x} =

(9)

B = b' a~p() -k(ro)po > 0
C=

w(p-tTo)p-t <0

where the signs of Band C arise due to stability conditions (Amari, 1977; Takeuchi and
Amari, 1979). In terms of RF size and RF position (4), the general result is:

= ~(~ + I)At (x) - M2 (x)
BC~2m{X) = (B- C -+ C~)(~+ I}A t (x) + (C- B++(C -2B)~)A2 (x)
B~2 ,(X}

(10)

where ~ is the difference operator:
~ f{ x) = f( x + p - t To) - f( x)

(11 )

3.2.2 Particular Case
The second order difference equations (l0) are rather opaque. This is partly due to coupling in y caused by the auto-correlation function key): (10) simplifies considerably if very
narrow stimuli are assumed - a(y)=O(y) (see also Amari, 1980). For periodic boundary
conditions:

(12)

where:

Reorganization of Somatosensory Cortex after Tactile Training

m -I P(W (y)

87

= m -I pre (y) + Em -I (y)
=p-l(y_+ro)+Em-l(y)

(13)

and we have used the crude approximation:

t;:

d _() 1
(
dx m x """" ~m x -

1

2"" P

_I

ro

)

(14)

which demands smoothness on the scale of 10 . However, for perturbations like that
sketched in figure 2, this is sufficient to tell us about the constant regions of MF. (We
would not expect to be able to model the data in the transition region in any case, as its
form is too dependent upon fine detail of the model).
Our results (12) show that the change in RF size of a neuron is simply minus the total
change in stimulus probability over its RF. Hence RF size decreases where p(y) increases
and vice versa. Conversely, the change in MF at a given stimulus location is roughly the
local average change in stimulus probability there. Note that changes in RF size correlate
inversely with changes in MF. Figure 3 is a sketch of these results for the perturbation of
figure 2.
MF

RF

o

o

y

I

\

I

L.J

Figure 3: Results of perturbation analysis for how behavioural training (figure 2) changes
RF size and MF respectively, in the case where stimulus width can be neglected. For MF
- due to the approximation (14) - predictions do not apply near the transitions.

4. DISCUSSION
Equations (12) are the results of our model for RF size and MF after area 3b has fully
adapted to the behavioural task, in the case where stimulus width can be neglected. They
appear to be fully consistent with the data of Jenkins et al described above: RF size decreases in the region of cortex selective for the stimulated body part and the MF for this
body part increases. Our analysis also makes a specific prediction that goes beyond
Jenkins et aI's data, directly due to the inverse relationship between changes in RF size
and those in MF. Within the regions that surrender territory to the entrained finger tips
(sometimes the face region), for which MF decreases, RF sizes should increase.
Surprisingly perhaps, these changes in RF size are not due to adaptation of the afferent
weights s(x,y). The changes are rather due to the adaptive threshold term six). This
point will be discussed more fully elsewhere.
A limitation of our analysis is the assumption that the change in stimulus probability is in
some sense small. Such an approximation may be reasonable for behavioural training but
seems less so as regards important experimental protocols like amputation or denervation.
Evidently a more general analysis would be highly desirable.

88

R. S. PETERSEN,J. O. TAYLOR

5. CONCLUSION
We have analysed a system with three interacting features: lateral inhibitory interactions;
Hebbian adaptivity of afferent synapses and an adaptive firing threshold. Our results indicate that such a system can account for the data of Jenkins et aI, concerning the response of adult somatosensory cortex to the changing environmental demands imposed by
tactile training. The analysis also brings out a prediction of the model, that may be testable.

Acknowledgements
RSP is very grateful for a travel stipend from the NIPS Foundation and for a Nick
Hughes bursary from the School of Physical Sciences and Engineering, King's College
London, that enabled him to participate in the conference.

References
Amari S. (1977) BioI. Cybern. 2777-87
Amari S. (1980) Bull. Math. Biology 42339-364
Geman S. (1979) SIAM 1. App. Math. 36 86-105
Grajski K.A., Merzenich M.M. (1990) in Neural Information Processing Systems 2
Touretzky D.S. (Ed) 52-59
HaberW.B. (1955)1. Psychol. 40115-123
Jenkins W.M ., Merzenich M.M., Ochs M.T., Allard T., Gufc-Robles E. (1990) 1. Neurophysiol. 63 82-104
Kaas J.H. (1995) in The Cognitive Neurosciences Gazzaniga M.S. (Ed ic) 51-71
Merzenich M.M., Kaas J.H., Wall J.T., Nelson R.J., Sur M., Felleman DJ. (1983a) Neuroscience 8 35-55
Merzenich M .M., Kaas J.H., Wall J.T., Sur M., Nelson R.I., Felleman DJ . (1983b) Neuroscience 10639-665
Merzenich M.M., Nelson R.I., Stryker M.P., Cynader M.S., Schoppmann A., Zook J.M.
(1984) 1. Compo Neural. 224591-605
Mogilner A., Grossman A.T., Ribrary V., Joliot M., Vol mann J., Rapaport D., Beasley R.,
L1inas R. (1993) Proc. Natl. Acad. Sci. USA 90 3593-3597
Pearson J.e., Finkel L.H., Edelman G.M. (1987) 1. Neurosci. 124209-4223
Recanzone G.H., Merzenich M.M., Jenkins W.M., Grajski K.A., Dinse H.R. (1992) 1.
Neurophysiol. 67 1031-1056
Ritter H., Schulten K. (1986) BioI. Cybern. 5499-106
Takeuchi A., Amari S. (1979) BioI. Cybern. 35 63-72
Willshaw DJ., von der Malsburg e. (1976) Proc. R. Soc. Lond. B194 203-243

"
1054,1995,Implementation Issues in the Fourier Transform Algorithm,,1054-implementation-issues-in-the-fourier-transform-algorithm.pdf,Abstract Missing,"Implementation Issues in the Fourier
Transform Algorithm

Yishay Mansour"" Sigal Sahar t
Computer Science Dept.
Tel-Aviv University
Tel-Aviv, ISRAEL

Abstract
The Fourier transform of boolean functions has come to play an
important role in proving many important learnability results. We
aim to demonstrate that the Fourier transform techniques are also
a useful and practical algorithm in addition to being a powerful
theoretical tool. We describe the more prominent changes we have
introduced to the algorithm, ones that were crucial and without
which the performance of the algorithm would severely deteriorate. One of the benefits we present is the confidence level for each
prediction which measures the likelihood the prediction is correct.

1

INTRODUCTION

Over the last few years the Fourier Transform (FT) representation of boolean functions has been an instrumental tool in the computational learning theory community. It has been used mainly to demonstrate the learnability of various classes of
functions with respect to the uniform distribution . The first connection between the
Fourier representation and learnability of boolean functions was established in [6]
where the class ACo was learned (using its FT representation) in O(nPoly-log(n))
time. The work of [5] developed a very powerful algorithmic procedure: given a
function and a threshold parameter it finds in polynomial time all the Fourier coefficients of the function larger than the threshold. Originally the procedure was
used to learn decision trees [5], and in [8, 2, 4] it was used to learn polynomial size
DNF. The FT technique applies naturally to the uniform distribution, though some
of the learnability results were extended to product distribution [1, 3] .
.. e-mail: manSQur@cs.tau.ac.il
t e-mail: gales@cs.tau .ac.il

Implementation Issues in the Fourier Transform Algorithm

261

A great advantage of the FT algorithm is that it does not make any assumptions
on the function it is learning. We can apply it to any function and hope to obtain
""large"" Fourier coefficients. The prediction function simply computes the sum of
the coefficients with the corresponding basis functions and compares the sum to
some threshold. The procedure is also immune to some noise and will be able to
operate even if a fraction of the examples are maliciously misclassified. Its drawback
is that it requires to query the target function on randomly selected inputs.
We aim to demonstrate that the FT technique is not only a powerful theoretical
tool, but also a practical one. In the process of implementing the Fourier algorithm
we enhanced it in order to improve the accuracy of the hypothesis we generate while
maintaining a desirable run time. We have added such feartures as the detection
of inaccurate approximations ""on the fly"" and immediate correction of the errors
incurred at a minimal cost. The methods we devised to choose the ""right"" parameters proved to be essential in order to achieve our goals. Furthermore, when making
predictions, it is extremely beneficial to have the prediction algorithm supply an
indicator that provides the confidence level we have in the prediction we made. Our
algorithm provides us naturally with such an indicator as detailed in Section 4.1.
The paper is organized as follows: section 2 briefly defines the FT and describes
the algorithm. In Section 3 we describe the experiments and their outcome and in
Section 4 the enhancements made. We end with our conclusions in Section 5.

2

FOURIER TRANSFORM (FT) THEORY

In this section we briefly introduce the FT theory and algorithm. its connection to
learning and the algorithm that finds the large coefficients. A comprehensive survey
of the theoretical results and proofs can be found in [7].
We consider boolean functions of n variables: f : {O, l}n - t {-I, I}. We define the
inner product: < g, f >= 2- n L::XE{O,l}R f(x)g(x) = E[g . f], where E is the expected value with respect to the uniform distribution . The basis is defined as follows:
for each z E {O,l}n, we define the basis function :\:z(Xl,???,X n ) = (_1)L::~=lx;z ?.
Any function of n boolean inputs can be uniquely expressed as a linear combination
of the basis functions . For a function f, the zth Fourier coefficient of f is denoted
by j(z) , i.e. , f(x) = L::zE{O,l}R j(z)XAx) . The Fourier coefficients are computed
by j(z) =< f, Xz > and we call z the coefficient-name of j(z). We define at-sparse
function to be a function that has at most t non-zero Fourier coefficients.
2.1

PREDICTION

Our aim is to approximate the target function f by a t-sparse function h. In many
cases h will simply include the ""large"" coefficients of f. That is, if A = {Zl' ... , zm}
is the set of z's for which j(Zi) is ""large"", we set hex) = L::z;EA aiXz;(x), where
at is our approximation of j(Zi). The hypothesis we generate using this process,
hex), does not have a boolean output. In order to obtain a boolean prediction
we use Sign(h(x)), i.e., output +1 if hex) 2 0 and -1 if hex) < o. We want to
bound the error we get from approximating f by h using the expected error squared,
E[(J - h )2]. It can be shown that bounding it bounds the boolean prediction error
probability, i.e., Pr[f(x) f. sign(h(x))] ~ E[(J - h)2] . For a given t, the t-sparse

Y. MANSOUR, S. SAHAR

262

hypothesis h that minimizes E[(J - h)2] simply includes the t largest coefficients of
f. Note that the more coefficients we include in our approximation and the better
we approximate their values, the smaller E[(J - h)2] is going to be. This provides
us with the motivation to find the ""large"" coefficients.
2.2

FINDING THE LARGE COEFFICIENTS

The algorithm that finds the ""large"" coefficients receives as inputs a function 1 (a
black-box it can query) and an interest threshold parameter (J > 0. It outputs a list
of coefficient-names that (1) includes all the coefficients-names whose corresponding coefficients are ""large"", i.e., at least (J , and (2) does not include ""too many""
coefficient-names. The algorithm runs in polynomial time in both 1/() and n .
SUBROUTINE search( a)
IF TEST[J, a, II] THEN IF

lal

=n

THEN OUTPUT a
ELSE search(aO); search(al);

Figure 1: Subroutine search
The basic idea of the algorithm is to perform a search in the space of the coefficientnames of I. Throughout the search algorithm (see Figure (1)) we maintain a prefix
of a coefficient-name and try to estimate whether any of its extensions can be
a coefficient-name whose value is ""large"". The algorithm commences by calling
search(A) where A is the empty string. On each invocation it computes the predicate TEST[/, a, (J]. If the predicate is true, it recursively calls search(aO) and
search(al). Note that if TEST is very permissive we may reach all the coefficients, in which case our running time will not be polynomial; its implementation
is therefore of utmost interest. Formally, T EST[J, a, (J] computes whether

E xe {O,l}n-""E;e{O,lP.[J(YX)Xa(Y)] 2: (J2,

where k = Iiali .

(1)

Define la(x) = L:,ae{O,l}n-"" j(aj3)x.,a(x). It can be shown that the expected value
in (1) is exactly the sum of the squares of the coefficients whose prefix is a , i.e.,
E xe {o,l}n-""E;e{o,l}d/(yx)x.a(Y)] = Ex[/~(x)] = L:,ae{o,l}n-"" p(aj3), implying
that if there exists a coefficient Ii( a,8)1 2: (), then E[/;] 2: (J2 . This condition
guarantees the correctness of our algorithm, namely that we reach all the ""large""
coefficients. We would like also to bound the number of recursive calls that search
performs. We can show that for at most 1/(J2 of the prefixes of size k, TEST[!, a , (J]
is true. This bounds the number of recursive calls in our procedure by O(n/(J2).
In TEST we would like to compute the expected value, but in order to do so
efficiently we settle for an approximation of its value. This can be done as follows:
(1) choose ml random Xi E {a, l}n-k, (2) choose m2 random Yi,j E {a, l}k , (3)
query 1 on Yi,jXi (which is why we need the query model-to query f on many
points with the same prefix Xi) and receive I(Yi,j xd, and (4) compute the estimate
as, Ba =

3

';1 L:~\ (~~ L:~l I(Yi,iXdXa(Yi,j)f

. Again , for more details see [7].

EXPERIMENTS

We implemented the FT algorithm (Section 2.2) and went forth to run a series of
experiments. The parameters of each experiment include the target function , (J , ml

Implementation Issues in the Fourier Transform Algorithm

263

and m2. We briefly introduce the parameters here and defer the detailed discussion.
The parameter () determines the threshold between ""small"" and ""large"" coefficients,
thus controlling the number of coefficients we will output. The parameters wI and
w2 determine how accurately we approximate the TEST predicate. Failure to approximate it accurately may yield faulty, even random, results (e.g., for a ludicrous
choice of m1 = 1 and m2 = 1) that may cause the algorithm to fail (as detailed in
Section 4.3). An intelligent choice of m1 and m2 is therefore indispensable. This
issue is discussed in greater detail in Sections 4.3 and 4.4.

Figure 2:

Typical frequency plots and typical errors . Errors occur in two cases: (1) the algorithm
predicts a +1 response when the actual response is -1 (the lightly shaded area), and (2) the algorithm
predicts a -1 response , while the true response is +1 (the darker shaded area) .

Figures (3)-(5) present representative results of our experiments in the form of
graphs that evaluate the output hypothesis of the algorithm on randomly chosen
test points. The target function, I, returns a boolean response, ?1, while the FT
hypothesis returns a real response. We therefore present, for each experiment, a
graph constituting of two curves: the frequency of the values of the hypothesis,
h( x), when I( x) = +1, and the second curve for I( x) = -1. If the two curves
intersect, their intersection represents the inherent error the algorithm makes.

Figure 3: Decision trees of depth 5 and 3 with 41 variables . The 5-deep (3-deep) decision tree
returns -1 about 50% (62.5%) of the time . The results shown above are for values (J
0.03, ml
100
and m2 = 5600 ?(J = 0.06, ml = 100 and m2 = 1300). Both graphs are disjoint, signifying 0% error.

=

4
4.1

=

RESULTS AND ALGORITHM ENHANCEMENTS
CONFIDENCE LEVELS

One of our most consistent and interesting empirical findings was the distribution
of the error versus the value of the algorithm's hypothesis: its shape is always that
of a bell shaped curve. Knowing the error distribution permits us to determine with
a high (often 100%) confidence level the result for most of the instances, yielding
the much sought after confidence level indicator. Though this simple logic thus far
has not been supported by any theoretical result, our experimental results provide
overwhelming evidence that this is indeed the case.
Let us demonstrate the strength of this technique: consider the results of the 16-term
DNF portrayed in Figure (4) . If the algorithm's hypothesis outputs 0.3 (translated

264

Y. MANSOUR, S. SAHAR

Figure 4: 16 terlD DNF. This (randomly generated) DNF of 40 variables returns -1 about 61 % of
the time. The results shown above are for the values of 9
0 .02 , m2
12500 and ml
100. The
hypothesis uses 186 non-zero coefficients . A total of 9 .628% error was detected.

=

=

=

into 1 in boolean terms by the Sign function), we know with an 83% confidence
level that the prediction is correct. If the algorithm outputs -0.9 as its prediction,
we can virtually guarantee that the response is correct. Thus, although the total
error level is over 9% we can supply a confidence level for each prediction. This is
an indispensable tool for practical usage of the hypothesis .

4.2

DETERMINING THE THRESHOLD

Once the list of large coefficients is built and we compute the hypothesis h( x), we
still need to determine the threshold, a, to which we compare hex) (i.e., predict +1
iff hex) > a). In the theoretical work it is assumed that a = 0, since a priori one
cannot make a better guess . We observed that fixing a's value according to our
hypothesis, improves the hypothesis. a is chosen to minimize the error with respect
to a number of random examples.

Figure 5:

8 terlD DNF . This (randomly generated) DNF of 40 variables returns -1 about 43% of the
time. The results shown above are for the values of 9
0 .03, m2
5600 and ml
100. The hypothesis
consists of 112 non-zero coefficients.

=

=

=

For example, when trying to learn an 8-term DNF with the zero threshold we will
receive a total of 1.22% overall error as depicted in Figure (5). However, if we
choose the threshold to be 0.32, we will get a diminished error of 0.068%.

4.3

ERROR DETECTION ON THE FLY - RETRY

During our experimentations we have noticed that at times the estimate Ba for
E[J~] may be inaccurate. A faulty approximation may result in the abortion of the
traversal of ""interesting"" subtreees, thus decreasing the hypothesis' accuracy, or in
traversal of ""uninteresting"" subtrees, thereby needlessly increasing the algorithm's
runtime. Since the properties of the FT guarantee that E[J~] = E[f~o] + E[J~d,
we expect Ba :::::: Bao + Bal . Whenever this is not true, we conclude that at least
one of our approximations is somewhat lacking. We can remedy the situation by

265

Implementation Issues in the Fourier Transform Algorithm

running the search procedure again on the children, i.e., retry node a. This solution increases the probability of finding all the ""large"" coefficients. A brute force
implementation may cost us an inordinate amount of time since we may retraverse
subtrees that we have previously visited. However, since any discrepancies between
the parent and its children are discovered-and corrected-as soon as they appear,
we can circumvent any retraversal. Thus, we correct the errors without any superfluous additions to the run time.

-J:
,-

Figure 6:
and

(J

i\""
o
"" .......

Majority function of 41 variables. The result portrayed are for values m1 = 100 , m2 = 800

=0 .08 . Note the majority-function characteristic distribution of the results 1 .

We demonstrate the usefulness of this approach with an example of learning the
majority function of 41 boolean variables . Without the retry mechanism, 8 (of a
total of 42) large coefficients were missed, giving rise to 13.724% error represented by
the shaded area in Figure (6). With the retries all the correct coefficients were found,
yielding perfect (flawless) results represented in the dotted curve in Figure (6).
4.4

DETERMINING THE PARAMETERS

One of our aims was to determine the values of the different parameters, m1, m2 and
(}. Recall that in our algorithm we calculate B a , the approximation of Ex[f~(x)]
where m1 is the number of times we sample x in order to make this approximation.
We sample Y randomly m2 times to approximate fa(Xi) = Ey[f(YXih:a(Y)), for each
Xi ? This approximation of fa(Xi) has a standard deviation of approximately
Assume that the true value is 13i, i.e. f3i = fa(Xi), then we expect the contribution
of the ith element to Ba to be (13i ? )n;? = 131 ?
+ rr!~. The algorithm tests
Ba =
L 131 ? (}2, therefore, to ensure a low error, based on the above argument,
we choose m2 = (J52 ?

A.

J&;

rr!1

Choosing the right value for m2 is of great importance. We have noticed on more
than one occasion that increasing the value of m2 actually decreases the overall run
time. This is not obvious at first : seemingly, any increase in the number of times we
loop in the algorithm only increases the run time. However, a more accurate value
for m2 means a more accurate approximation of the TEST predicate, and therefore
less chance of redundant recursive calls (the run time is linear in the number of
recursive calls) . We can see this exemplified in Figure (7) where the number of
recursive calls increase drastically as m2 decreases. In order to present Figure (7) ,
1The ""peaked"" distribution of the results is not coincidental. The FT of the majority function has 42 large
equal coefficients, labeled cmaj' one for each singleton (a vector of the form 0 .. 010 .. 0) and one for parity (the
all-ones vector). The zeros of an input vector with z zeros we will contribute ?1(2z - 41). cmajl to the result
and the parity will contribute ?cma ) (depending on whether z is odd or even), so that the total contribution is
an even factor of c ma )' Since c ma ) =
around the peaks is due to the

f~ct

(~g);tcr

- 0 .12, we have peaks around factors of 0.24 . The distribution

we only approximate each coefficient and get a value close to c ma )'

Y. MANSOUR, S. SAHAR

266

we learned the same 3 term DNF always using e = 0.05 and mr
The trials differ in the specific values chosen in each trial for m2.

* m2

100000.

Figure 7: Deter01ining 012' Note that the number of recursive calls grows dramatically as m2 's
value decreases. For example, for m2
400, the number of recursive calls is 14,433 compared with only
1,329 recursive calls for m2
500 .

=

=

SPECIAL CASES: When k = 110'11 is either very small or very large, the values we
choose for ml and m2 can be self-defeating: when k ,..... n we still loop ml (~ 2n - k )
times, though often without gaining additional information. The same holds for very
small values of k, and the corresponding m2 (~ 2k) values. We therefore add the
following feature: for small and large values of k we calculate exactly the expected
value thereby decreasing the run time and increasing accuracy.

5

CONCLUSIONS

In this work we implemented the FT algorithm and showed it to be a useful practical
tool as well as a powerful theoretical technique. We reviewed major enhancements
the algorithm underwent during the process. The algorithm successfully recovers
functions in a reasonable amount of time. Furthermore, we have shown that the
algorithm naturally derives a confidence parameter. This parameter enables the user
in many cases to conclude that the prediction received is accurate with extremely
high probability, even if the overall error probability is not negligible.
Acknowledgements
This research was supported in part by The Israel Science Foundation administered by The Israel
Academy of Science and Humanities and by a grant of the Israeli Ministry of Science and Technology.

References
[1) Mihir Bellare. A technique for upper bounding the spectral norm with applications to learning.
Annual Work&hop on Computational Learning Theory, pages 62-70, July 1992.

In 5 th

(2) Avrim Blum, Merrick Furst, Jeffrey Jackson, Michael Kearns, Yishay Mansour, and Steven Rudich. Weakly
learning DNF and characterizing statistical query learning using fourier analysis. In The 26 th Annual AC M
Sympo&ium on Theory of Computing, pages 253 - 262, 1994 .
(3) Merrick L . Furst , Jeffrey C. Jackson, and Sean W. Smith. Improved learning of AC O functions .
Annual Work&hop on Computational Learning Theory, pages 317-325, August 1991.

In 4th

(4) J. Jackson . An efficient membership-query algorithm for learning DNF with respect to the uniform distribution. In Annual Sympo&ium on Switching and Automata Theory, pages 42 - 53, 1994.
(5) E. Kushilevitz and Y . Mansour. Learning decision trees using the fourier spectrum. SIAM Journal on
Computing 22(6): 1331-1348, 1993.
(6) N. Linial, Y. Mansour, and N . Nisan. Constant depth circuits, fourier transform and learnability.

JACM

40(3):607-620, 1993.

(7) Y. Mansour . Learning Boolean Functions via the Fourier Transform. Advance& in Neural Computation,
edited by V.P. Roychodhury and K-Y. Siu and A. Orlitsky, Kluwer Academic Pub. 1994. Can be accessed
via Up :/ /ftp .math.tau.ac.iJ/pub/mansour/PAPERS/LEARNING/fourier-survey.ps.Z.
(8) Yishay Mansour. An o(nlog log n) learning algorihm for DNF under the uniform distribution . J. of Computer
and Sy&tem Science, 50(3):543-550, 1995.

"
1055,1995,Adaptive Retina with Center-Surround Receptive Field,,1055-adaptive-retina-with-center-surround-receptive-field.pdf,Abstract Missing,"Adaptive Retina with Center-Surround
Receptive Field

Shih-Chii Lin and Kwabena Boahen
Computation and Neural Systems
139-74 California Institute of Technology
Pasadena, CA 91125
shih@pcmp.caltech.edu, buster@pcmp.caltech.edu

Abstract
Both vertebrate and invertebrate retinas are highly efficient in extracting contrast independent of the background intensity over five
or more decades. This efficiency has been rendered possible by
the adaptation of the DC operating point to the background intensity while maintaining high gain transient responses. The centersurround properties of the retina allows the system to extract information at the edges in the image. This silicon retina models the
adaptation properties of the receptors and the antagonistic centersurround properties of the laminar cells of the invertebrate retina
and the outer-plexiform layer of the vertebrate retina. We also illustrate the spatio-temporal responses of the silicon retina on moving
bars. The chip has 59x64 pixels on a 6.9x6.8mm2 die and it is
fabricated in 2 J-tm n-well technology.

1

Introduction

It has been observed previously that the initial layers of the vertebrate and invertebrate retina systems perform very similar processing functions on the incoming
input signal[1]. The response versus log intensity curves of the receptors in invertebrate and vertebrate retinas look similar. The curves show that the receptors
have a larger gain for changes in illumination than to steady illumination, i.e, the
receptors adapt. This adaptation property allows the receptor to respond over a
large input range without saturating.
Anatomically, the eyes of invertebrates differ greatly from that of vertebrates. Ver-

Adaptive Retina with Center-Surround Receptive Field

679

tebrates normally have two simple eyes while insects have compound eyes. Each
compound eye in the fly consists of 3000-4000 ommatidia and each ommatidium
consists of 8 photoreceptors. Six of these receptors (which are also called RI-R6)
are in a single spectral class. The other two receptors, R7 and R8 provide channels
for wavelength discrimination and polarization.
The vertebrate eye is divided into the outer-plexiform layer and the inner-plexiform
layer. The outer-plexiform layer consists of the rods and cones, horizontal cells
and bipolar cells. Invertebrate receptors depolarise in response to an increase in
light, in contrast to vertebrate receptors, which hyperpolarise to an increase in light
intensity. Both vertebrate and invertebrate receptors show light adaptation over at
least five decades of background illumination. This adaptation property allows the
retina to maintain a high transient gain to contrast over a wide range of background
intensities.
The invertebrate receptors project to the next layer which is called the lamina layer.
This layer consists primarily of monopolar cells which show a similar response versus log intensity curve to that of vertebrate bipolar cells in the outer-plexiform
layer. Both cells respond with graded potentials to changes in illumination. These
cells also show a high transient gain to changes in illumination while ignoring the
background intensity and they possess center-surround receptive fields. In vertebrates, the cones which are excited by the incoming light, activate the horizontal
cells which in tum inhibit the cones. The horizontal cells thus mediate the lateral
inhibition which produces the center-surround properties. In insects, a possible
process of this lateral inhibition is done by current flow from the photoreceptors
through the epithelial glial cells surrounding an ommatidium or the modulation
of the local field potential in the lamina to influence the transmembrane potential
of the photoreceptor[2]. The center-surround receptive fields allow contrasts to be
accentuated since the surround computes a local mean and subtracts that from the
center signal.
Mahowald[3] previously described a silicon retina with adaptive photoreceptors and
Boahen et al.[4] recently described a compact current-mode analog model of the
outer-plexiform layer of the vertebrate retina and analysed the spatio-temporal
processing properties of this retina[5]. A recent array of photoreceptors from
Delbriick[6] uses an adaptive photoreceptor circuit that adapts its operating point
to the background intensity so that the pixel shows a high transient gain over 5
decades of background illumination. However this retina does not have spatial
coupling between pixels.
The pixels in the silicon retina described here has a compact circuit that incorporates both spatial and temporal filtering with light adaptation over 5 decades
of background intensity. The network exhibits center-surround behavior. Boahen
et al.[4] in their current-mode diffusor retina, draw an analogy between parts of
the diffusor circuit and the different cells in the outer-plexiform layer. While the
same analogy cannot be drawn from this silicon retina to the invertebrate retina
since the function of the cells are not completely understood, the output responses
of the retina circuit are similar to the output responses of the photoreceptor and
monopolar cells in invertebrates.
The circuit details are described in Section 2 and the spatio-temporal processing
performed by the retina on stimulus moving at different speeds is shown in Section

S.-C. LIU, K. BOAHEN

680

3.

2

Circuit
-----VI

Vb

VI

1

p1

1

VI

M4
VI?I

Vh

VI+I

Vh

.1.

.bel

.1.

Vr

---------

MI

(a)

im.l

iia

rrr

iI...I

rrr

rrr

'II

'II

(b)

Figure 1: (a) One-dimensional version of the retina. (b) Small-signal equivalent of
circuit in (a).
A one-dimensional version of the retina is shown in Figure l(a). The retina consists
of an adaptive photoreceptor circuit at each pixel coupled together with diffusors,
controlled by voltages, Vg and Vh. The output of this network can either be obtained
at the voltage output, V, or at the current output, 10 but the outputs have different
properties. Phototransduction is obtained by using a reverse-biased photodiode
which produces current that is proportional to the incident light. The logarithmic
properties are obtained by operating the feedback transistor shown in Figure l(a)
in the subthreshold region. The voltage change at the output photoreceptor, V r , is
proportional to a small contrast since

UT

Vr

UTdI

U

i

T
= -d(logl)
==K,
K,
1
K, h
g

CO:rCd '

where UT is the thermal voltage, K, =
Coz is the oxide capacitance and
Cd is the depletion capacitance of a transistor. The circuit works as follows: If
the photocurrent through the photodiode increases, Vr will be pulled low and the
output voltage at V, increases by VI = AVr where A is the amplifier gain of the
output stage. This output change in V, is coupled into Vel through a capacitor

Adaptive Retina with Center-Surround Receptive Field

681

divider ratio, Cl~2C2. The feedback transistor, M4, operates in the subthreshold
region and supplies the current necessary to offset the photocurrent. The increase
in Vel (i.e. the gate voltage of M4) causes the current supplied by M3 to increase
which pulls the node voltage, Vr , back to the voltage level needed by Ml to sink
the bias current from transistor, M2.

3.5

3.45

...-=

3.4

??c
?
a:?

3.35

0

~

-2

0

Q.

3.3
-1

3.25
0
3.2
0

5

10

15

20

25

Time (Sec)

Figure 2: This figure shows the output response of the receptor to a variation of
about 40% p-p in the intensity of a flickering LED light incident on the chip. The
response shows that the high sensitivity of the receptor to the LED is maintained
over 5 decades of differing background intensities. The numbers on the section of
the curve indicate the log intensity of the mean value. 0 log is the absolute intensity
from the LED.
The adaptive element, M3, has an I-V curve which looks like a hyperbolic sine.
The small slope of the I-V curve in the middle means that for small changes of
voltages across M3, the element looks like an open-circuit. With large changes of
voltage across M3, the current through M3 becomes exponential and Vel is charged
or discharged almost instantaneously.
Figure 2 shows the output response of the photoreceptor to a square-wave variation
of about 40% p-p in the intensity of a red LED (635 nm). The results show that
the circuit is able to discern the small contrast over five decades of background intensity while the steady-state voltage of the photoreceptor output varies only about
15mV. Further details of the photoreceptor circuit and its adaptation properties
are described in Delbriick[6].

3

Spatio-Temporal Response

The spatio-temporal response of the network to different moving stimuli is explored
in this section. The circuit shown in Figure l(a) can be transferred to an equivalent
network of resistors and capacitors as shown in Figure l(b) to obtain the transfer
function of the circuit. The capacitors at each node are necessary to model the

S.-C. LIU, K. BOAHEN

682

8.5

i

1lJ;

~ 7.5

:;
:;

...

o

i

I

~

...
r.

0.4

(a)

0.6

0.8

1.2

1.4

Time (Sec)

3.8 ~_---:-"":--_--::'=_ _':""':-_--::':-_--::,'::-_--.J
0.3
0.4
0.5
0 .6
0.7
0.8

(b)

Time (Sec)

Figure 3: (a) Response of a pixel to a grey strip 2 pixels wide of gray-level ""0.4""
on a dark background of level ""0"" moving past the pixel at different speeds. (b)
Response of a pixel to a dark strip of gray-level ""0.6"" on a white background of level
""1"" moving past the pixel at different speeds. The voltage shown on these curves is
not the direct measurement of the voltage at V, but rather V, drives a current-sensing
transistor and this current is then sensed by an offchip current sense-amplifier.

Adaptive Retina with Center-Surround Receptive Field

683

temporal responses of the circuit.
The chip results from the experiments below illustrate the center-surround properties of the network and the difference in time-constants between the surround and
center.
3.1

Chip Results

Data from the 2D chip is shown in the next few figures. In these experiments, we
are only looking at one pixel of the 2D array. A rotating circular fly-wheel stimulus
with strips of alternating contrasts is mounted above the chip. The stimulus was
created using Mathematica. Figure 3a shows the spati~temporal impulse response
of one pixel measured at V, with a small strip at level ""0.4"" on a dark background of
level ""0"" moving past the pixels on the row. At slow speeds, the impulse response
shows a center-surround behavior where the pixel first receives inhibition from the
preceding pixels which are excited by the stimulus. When the stimulus moves by
the pixel of interest, it is excited and then it is inhibited by the subsequent pixels
seeing the stimulus.

I
o

f

I
i

Tim. (Sec)

Figure 4: Response of a pixel to a strip of varying contrasts on a dark background
moving past the pixel at a constant speed.
At faster speeds, the initial inhibition in the response grows smaller until at some
even faster speed, the initial inhibition is no longer observed. This response comes
about because the inhibition from the surround has a longer-time constant than the
center. When the stimulus moves past the pixel of interest, the inhibition from the
preceding pixels excited by the stimulus does not have time to inhibit the pixel of
interest. Hence the excitation is seen first and then the inhibition comes into place
when the stimulus passes by. Note that in these figures (Figures 3-4), the curves
have been displaced to show the pixel response at different speeds of the moving
stimulus. The voltage shown on these curves is not the direct measurement of the
voltage at V, but rather V, drives a current-sensing transistor and this current is
then sensed by an off-chip current sense-amplifier.
Figure 3b shows the

spati~temporal

impulse response of one pixel with a similar

s.-c. LlU, K. BOAHEN

684

size strip of level ""0.6"" on a light background of level ""1"" moving past the row of
pixels. The same inhibition behavior is seen for increasing stimulus speeds. Figure 4 shows the output response at V, for the same stimulus of gray-levels varying
from ""0.2"" to ""0.8"" on a dark background of level ""0"" moving at one speed. The
peak excitation response is plotted against the contrast in Figure 5. A level of ""0.2""
corresponds to a irradiance of 15mW/m2 while a level of ""0.8"" corresponds to a irradiance of 37.4mW/m2. These measurements are done with a photometer mounted
about 1.5in above a piece of paper with the contrast which is being measured. The
irradiance varies exponentially with increasing level.

4

Conclusion

In this paper, we described an adaptive retina with a center-surround receptive
field. The system properties of this retina allows it to model functionally either the
responses of the laminar cells in the invertebrate retina or the outer-plexiform layer
of vertebrate retina. We show that the circuit shows adaptation to changes over
5 decades of background intensities. The center-surround property of the network
can be seen from its spatio-temporal response to different stimulus speeds. This
property serves to remove redundancy in space and time of the input signal.
Acknowledgements

We thank Carver Mead for his support and encouragement. SC Liu is supported by
an NIMH fellowship and K Boahen is supported by a Sloan fellowship. We thank
Tobias Delbriick for the inspiration and help in testing the design. We also thank
Rahul Sarpeshkar and Bradley Minch for comments. Fabrication was provided by
MOSIS.

References
[1] S. B. Laughlin, ""Coding efficiency and design in retinal processing"", In: Facets
of Vision (D. G. Stavenga and R. C. Hardie, eds) pp. 213-234. Springer, Berlin,
1989.
[2] S. R. Shaw, ""Retinal resistance barriers and electrica1lateral inhibition"", Nature, Lond.255,: 480-483, 1975.
[3] M. A. Mahowald, ""Silicon Retina with Adaptive Photoreceptors"" in
SPIE/SPSE Symposium on Electronic Science and Technology: From Neurons
to Chips. Orlando, FL, April 1991.
[4] K. A. Boahen and A. G. Andreou, ""A Contrast Sensitive Silicon Retina with
Reciprocal Synapses"", In D. S. Touretzky (ed.), Advances in Neural Information Processing Systems 4, 764-772. San Mateo, CA: Morgan Kaufmann, 1992.
[5] K. A. Boahen, ""Spatiotemporal sensitivity of the retina: A physical model"",
CNS Memo CNS-TR-91-06, California Institute of Technology, Pasadena, CA
91125, June 1991.
[6] T. Delbriick, ""Analog VLSI Phototransduction by continous-time, adaptive,
logarithmic photoreceptor circuits"", CNS Memo No.30, California Institute of
Technology, Pasadena, CA 91125, 1994.

"
1056,1995,Forward-backward retraining of recurrent neural networks,,1056-forward-backward-retraining-of-recurrent-neural-networks.pdf,Abstract Missing,"Forward-backward retraining of recurrent
neural networks
Andrew Senior ?
Tony Robinson
Cambridge University Engineering Department
Trumpington Street, Cambridge, England

Abstract
This paper describes the training of a recurrent neural network
as the letter posterior probability estimator for a hidden Markov
model, off-line handwriting recognition system. The network estimates posterior distributions for each of a series of frames representing sections of a handwritten word. The supervised training
algorithm, backpropagation through time, requires target outputs
to be provided for each frame. Three methods for deriving these
targets are presented. A novel method based upon the forwardbackward algorithm is found to result in the recognizer with the
lowest error rate.

1

Introduction

In the field of off-line handwriting recognition, the goal is to read a handwritten
document and produce a machine transcription. Such a system could be used
for a variety of purposes, from cheque processing and postal sorting to personal
correspondence reading for the blind or historical document reading. In a previous
publication (Senior 1994) we have described a system based on a recurrent neural
network (Robinson 1994) which can transcribe a handwritten document.
The recurrent neural network is used to estimate posterior probabilities for character classes, given frames of data which represent the handwritten word. These
probabilities are combined in a hidden Markov model framework, using the Viterbi
algorithm to find the most probable state sequence.
To train the network, a series of targets must be given. This paper describes three
methods that have been used to derive these probabilities. The first is a naive bootstrap method, allocating equal lengths to all characters, used to start the training
procedure. The second is a simple Viterbi-style segmentation method that assigns a
single class label to each of the frames of data. Such a scheme has been used before
in speech recognition using recurrent networks (Robinson 1994). This representation, is found to inadequately represent some frames which can represent two letters,
or the ligatures between letters. Thus, by analogy with the forward-backward algorithm (Rabiner and Juang 1986) for HMM speech recognizers, we have developed a
?Now at IDM T .J.Watson Research Center, Yorktown Heights NYI0598, USA.

744

A. SENIOR, T. ROBINSON

forward-backward method for retraining the recurrent neural network. This assigns
a probability distribution across the output classes for each frame of training data,
and training on these 'soft labels' results in improved performance of the recognition
system.
This paper is organized in four sections. The following section outlines the system
in which the neural network is used, then section 3 describes the recurrent network
in more detail. Section 4 explains the different methods of target estimation and
presents the results of experiments before conclusions are presented in the final
section.

2

System background

The recurrent network is the central part of the handwriting recognition system.
The other parts are summarized here and described in more detail in another publication (Senior 1994). The first stage of processing converts the raw data into
an invariant representation used as an input to the neural network. The network
outputs are used to calculate word probabilities in a hidden Markov model.
First, the scanned page image is automatically segmented into words and then normalized. Normalization removes variations in the word appearance that do not
affect its identity, such as rotation, scale, slant, slope and stroke thickness. The
height of the letters forming the words is estimated, and magnifications, shear and
thinning transforms are applied, resulting in a more robust representation of the
word. The normalized word is represented in a compact canonical form encoding
both the shape and salient features. All those features falling within a narrow vertical strip across the word are termed a frame. The representation derived consists
of around 80 values for each of the frames, denoted Xt. The T frames (Xl,' .. , x r )
for a whole word are written xl' Five frames would typically be enough to represent a single character. The recurrent network takes these frames sequentially and
estimates the posterior character probability distribution given the data: P(Ai IxD,
for each of the letters, a, .. ,z, denoted Ao, ... , A25 ? These posterior probabilities are
scaled by the prior class probabilities, and are treated as the emission probabilities
in a hidden Markov model.
A separate model is created for each word in the vocabulary, with one state per
letter. Transitions are allowed only from a state to itself or to the next letter in the
word. The set of states in the models is denoted Q = {ql, ... , qN} and the letter
represented by qi is given by L(qi), L : Q 1-+ Ao, ... , A 25 ?
Word error rates are presented for experiments on a single-writer task tested with
a 1330 word vocabulary!. Statistical significance of the results is evaluated using
Student's t-test, comparing word recognition rates taken from a number of networks
trained under the same conditions but with different random initializations. The
results of the t-test are written: T( degrees of freedom) and the tabulated values:
tsignificance (degrees of freedom).

3

Recurrent networks

This section describes the recurrent error propagation network which has been used
as the probability distribution estimator for the handwriting recognition system.
Recurrent networks have been successfully applied to speech recognition (Robinson 1994) but have not previously been used for handwriting recognition, on-line
or off-line. Here a left-to-right scanning process is adopted to map the frames of
a word into a sequence, so adjacent frames are considered in consecutive instants.
lThe experimental data are available in ftp:/ /svr-ftp.eng.cam.ac.uk/pub/data

Forward-backward Retraining of Recurrent Neural Networks

745

A recurrent network is well suited to the recognition of patterns occurring in a
time-series because series of arbitrary length can be processed, with the same processing being performed on each section of the input stream. Thus a letter 'a'
can be recognized by the same process, wherever it occurs in a word. In addition, internal 'state' units are available to encode multi-frame context information
so letters spread over several frames can be recognized. The recurrent network
Input Frames

_

IT

:J ._- ---.

TT,

Network

,;

,

-- JD
:!
,

Output
(Characlcrprobabllllles )

( .. vv le e W WW II ... )

i
:
f
---l-- _. ;---------r

inpu tiOulpul Uni ts

-ie~~;k-Um, s

Untt Time Iklay

Figure 1: A schematic of the recurrent error propagation network.
For clarity only a few of the units and links are shown.
architecture used here is a single layer of standard perceptrons with nonlinear activation functions. The output 0 i of a unit i is a function of the inputs aj and
the network parameters, which are the weights of the links Wij with a bias bi :

bi + Lakwik.
(2)
The network is fully connected - that is, each input is connected to every output. However, some of the input units receive no external input and are connected one-to-one to corresponding output units through a unit time-delay (figure 1). The remaining input units accept a single frame of parametrized input and the remaining 26 output units estimate letter probabilities for the 26
character classes. The feedback units have a standard sigmoid activation function (3), but the character outputs have a 'softmax' activation function (4).
0i

!i({O""j}),

(1)

O""i

eO' ?

(3)

L: j

eO"" ?

(4)

During recognition ('forward propagation'), the first frame is presented at the input
and the feedback units are initialized to activations of 0.5. The outputs are calculated (1 and 2) and read off for use in the Markov model. In the next iteration, the
outputs of the feedback units are copied to the feedback inputs, and the next frame
presented to the inputs. Outputs are again calculated, and the cycle is repeated for
each frame of input, with a probability distribution being generated for each frame.
To allow the network to assimilate context information, several frames of data are
passed through the network before the probabilities for the first frame are read
off, previous output probabilities being discarded. This input/output latency is
maintained throughout the input sequence, with extra, empty frames of inputs
being presented at the end to give probability distributions for the last frames of
true inputs. A latency of two frames has been found to be most satisfactory in
experiments to date.

3.1

Training

To be able to train the network the target values (j (t) desired for the outputs
= 0, ... ,25 for frame Xt must be specified. The target specification is dealt

OJ (Xt) j

746

A. SENIOR. T. ROBINSON

with in the next section. It is the discrepancy between the actual outputs and these
targets which make up the objective function to be maximized by adjusting the
internal weights of the network. The usual objective function is the mean squared
error, but here the relative entropy, G, of the target and output distributions is
used:
G

(j(t)-)'
- ""
L- ""
L- (j (t) log -.-(
t

j

(5)

oJ Xt

At the end of a word, the errors between the network's outputs and the targets
are propagated back using the generalized delta rule (Rumelhart et al. 1986) and
changes to the network weights are calculated. The network at successive time
steps is treated as adjacent layers of a multi-layer network. This process is generally known as 'back-propagation through time' (Werbos 1990). After processing T
frames of data with an input/output latency, the network is equivalent to a (T +
latency) layer perceptron sharing weights between layers. For a detailed description
of the training procedure, the reader is referred elsewhere (Rumelhart et al. 1986;
Robinson 1994).

4

Target re-estimation

The data used for training are only labelled by word. That is, each image represents
a single word, whose identity is known, but the frames representing that word are
not labelled to indicate which part of the word they represent. To train the network,
a label for each frame's identity must be provided. Labels are indicated by the state
St E Q and the corresponding letter L(St) of which a frame Xt is part.
4.1

A simple solution

To bootstrap the network, a naive method was used, which simply divided the word
up into sections of equal length, one for each letter in the word. Thus, for an Nletter word of T frames, xI, the first letter was assumed to be represented by frames

xr, the next by k+
as follows:
..

2r

x

1

and so on. The segmentation is mapped into a set of targets

I'J.(t)
{ 1 if L(St) = Aj
(6)
..
0 otherwise.
Figure 2a shows such a segmentation for a single word. Each line, representing
(j(t) for some j, has a broad peak for the frames representing letter Aj. Such a
segmentation is inaccurate, but can be improved by adding prior knowledge. It
is clear that some letters are generally longer than others, and some shorter. By
weighting letters according to their a priori lengths it is possible to give a better,
but still very simple, segmentation. The letters Ii, I' are given a length of and
'm, w' a length ~ relative to other letters. Thus in the word 'wig', the first half
of the frames would be assigned the label 'w', the next sixth Ii' and the last third
the label 'g'. While this segmentation is constructed with no regard for the data
being segmented, it is found to provide a good initial approximation from which it
is possible to train the network to recognize words, albeit with high error rates.

!

4.2

Viterbi re-estimation

Having trained the network to some accuracy, it can be used to calculate a good
estimate of the probability of each frame belonging to any letter. The probability
of any state sequence can then be calculated in the hidden Markov model, and
the most likely state sequence through the correct word S* found using dynamic
programming. This best state sequence S* represents a new segmentation giving a
label for each frame. For a network which models the probability distributions well,
this segmentation will be better than the automatic segmentation of section 4.1

Forward-backward Retraining of Recurrent Neural Networks

747

"" Each line represents
Figure 2: Segmentations of the word 'butler'.
P(St = AilS) for one letter ~ and is high for framet when S; = Ai.
(a) is the equal-length segmentation discussed in section 4.1 (b) is
a segmentation of an untrained network. (c) is the segmentation
re-estimated with a trained network.
since it takes the data into account. Finding the most probable state sequence S? is
termed a forced alignment. Since only the correct word model need be considered,
such an alignment is faster than the search through the whole lexicon that is required
for recognition. Training on this automatic segmentation gives a better recognition
rate, but still avoids the necessity of manually segmenting any of the database.
Figure 2 shows two Viterbi segmentations of the word 'butler'. First, figure 2b
shows the segmentation arrived at by taking the most likely state sequence before
training the network. Since the emission probability distributions are random, there
is nothing to distinguish between the state sequences, except slight variations due
to initial asymmetry in the network, so a poor segmentation results. After training the network (2c), the durations deviate from the prior assumed durations to
match the observed data. This re-estimated segmentation represents the data more
accurately, so gives better targets towards which to train. A further improvement
in recognition accuracy can be obtained by using the targets determined by the reestimated segmentation. This cycle can be repeated until the segmentations do not
change and performance ceases to improve. For speed, the network is not trained
to convergence at each iteration.
It can be shown (Santini and Del Bimbo 1995) that, assuming that the network has
enough parameters, the network outputs after convergence will approximate the
posterior probabilities P(~lxD. Further, the approximation P(AilxD ~ P(Adxt)
is made. The posteriors are scaled by the class priors P(Ai) (Bourlard and Morgan
1993), and these scaled posteriors are used in the hidden Markov model in place of
data likelihoods since, by Bayes' rule,

P(XtIAi)

()(

P(~lxt)

P(Ai)?

(7)

Table 1 shows word recognition error rates for three 80-unit networks trained towards fixed targets estimated by another network, and then retrained, re-estimating
the targets at each iteration. The retraining improves the recognition performance
(T(2) = 3.91, t.9s(2) = 2.92).

4.3

Forward-backward re-estimation

The system described above performs well and is the method used in previous recurrent network systems, but examining the speech recognition literature, a potential
method of improvement can be seen. Viterbi frame alignment has so far been used
to determine targets for training. This assigns one class to each frame, based on
the most likely state sequence. A better approach might be to allow a distribution across all the classes indicating which are likely and which are not, avoiding a

748

A. SENIOR, T. ROBINSON

Table 1: Error rates for 3 networks with 80 units trained with fixed
alignments, and retrained with re-estimated alignments.
Training
Error (%)
(7
method
J.I.
Fixed targets 21.2 1.73
17.0 0.68
Retraining
'hard' classification at points where a frame may indeed represent more than one
class (such as where slanting characters overlap), or none (as in a ligature). A 'soft'
classification would give a more accurate portrayal of the frame identities.
<?

Such a distribution, 'Yp(t) = P(St = qplxI, W), can be calculated with the forwardbackward algorithm (Rabiner and Juang 1986). To obtain 'Yp(t), the forward probabilities Ctp(t) = P(St = qp, xD must be combined with the backward probabilities
f3p(t) = P(St = qp, x;+l)' The forward and backward probabilities are calculated
recursively in the same manner.
Ctr(t + 1)
Ctp(t)P(xtIL(qp))ap,r,
(8)

L

/3p(t - 1)

(9)
r

Suitable initial distributions Ctr(O) = 7l'r and f3r(r + 1) = Pr are chosen, e.g. 7l' and
P are one for respectively the first and last character in the word, and zero for the
others. The likelihood of observing the data Xl and being in state qp at time t is
then given by:
e (t) = Ctp(t)/3p(t).
(10)
Then the probabilities 'Yp(t) of being in state qp at time t are obtained by normalization and used as the targets (j (t) for the recurrent network character probability
outputs:
ep(t)
(11)
(j (t)
'Yp(t). (12)

L

l:r er(t)'

p:L(qp)=Aj

Figure 3a shows the initial estimate of the class probabilities for a sample of the
word' butler'. The probabilities shown are those estimated by the forward-backward
algorithm when using an untrained network, for which the P(XtISt = qp) will be
independent of class. Despite the lack of information, the probability distributions
can be seen to take reasonable shapes. The first frame must belong to the first
letter, and the last frame must belong to the last letter, of course, but it can also
be seen that half way through the word, the most likely letters are those in the
middle of the word. Several class probabilities are non-zero at a time, reflecting
the uncertainty caused since the network is untrained. Nevertheless, this limited
information is enough to train a recurrent network, because as the network begins
to approximate these probabilities, the segmentations become more definite. In
contrast, using Viterbi segmentations from an untrained network, the most likely
alignment can be very different from the true alignment (figure 2b). The segmentation is very definite though, and the network is trained towards the incorrect
targets, reinforcing its error. Finally, a trained network gives a much more rigid
segmentation (figure 3b), with most of the probabilities being zero or one, but with
a boundary of uncertainty at the transitions between letters. This uncertainty,
where a frame might truly represent parts of two letters, or a ligature between
two, represents the data better. Just as with Viterbi training, the segmentations
can be re-estimated after training and retraining results in improved performance.
The final probabilistic segmentation can be stored with the data and used when
subsequent networks are trained on the same data. Training is then significantly
quicker than when training towards the approximate bootstrap segmentations and
re-estimating the targets.

Forward-backward Retraining of Recurrent Neural Networks

749

Figure 3: Forward-backward segmentations of the word 'butler'.
(a) is the segmentation of an untrained network with a uniform
class prior. (b) shows the segmentation after training.
The better models obtained with the forward-backward algorithm give improved
recognition results over a network trained with Viterbi alignments. The improvement is shown in table 2. It can be seen that the error rates for the networks
trained with forward-backward targets are lower than those trained on Viterbi targets (T(2) 5.24, t.97S(2) 4.30).

=

=

Table 2: Error rates for networks with
or Forward-Backward alignments.
Training
method
Viterbi
Forward-Backward

5

80 units trained with Viterbi
Error 1%)
J.I.

(7

17.0
15.4

0.68
0.74

Conclusions

This paper has reviewed the training methods used for a recurrent network, applied
to the problem of off-line handwriting recognition. Three methods of deriving target probabilities for the network have been described, and experiments conduded
using all three. The third method is that of the forward-backward procedure, which
has not previously been applied to recurrent neural network training. This method
is found to improve the performance of the network, leading to reduced word error
rates. Other improvements not detailed here (including duration models and stochastic language modelling) allow the error rate for this task to be brought below
10%.
Acknowledgments
The authors would like to thank Mike Hochberg for assistance in preparing this
paper.

References
BOURLARD, H. and MORGAN, N. (1993) Connectionist Speech Recognition: A Hybrid
Approach . Kluwer .
RABINER, L. R. and JUANG, B . H. (1986) An introduction to hidden Markov models.
IEEE ASSP magazine 3 (1): 4-16.
ROBINSON, A. (1994) The application ofrecuIIent nets to phone probability estimation.
IEEE 'lransactions on Neural Networks.
RUMELHART, D. E ., HINTON, G. E. and WILLIAMS, R. J. (1986) Learning internal
representations by eIIor propagation. In Parallel Distributed Processing: Explorations
in the Microstructure of Cognition, ed. by D . E. Rumelhart and J . L. McClelland,
volume 1, chapter 8, PE. 318-362. Bradford Books.
SANTINI, S. and DEL BIMBO, A . (1995) RecuIIent neural networks can be trained to
be maximum a posteriori probability classifiers. Neural Networks 8 (1): 25-29.
SENIOR, A . W ., (1994) Off-line Cursive Handwriting Recognition using Recurrent
Neural Networks. Cambridge University Engineering Department Ph.D. thesis. URL:
~_~: / / svr-ft.p . enK. cam. ac . uk/pub/reports/senioLthesis . ps . gz.
WERBOS, P. J. (1990) Backpropagation through time: What it does and how to do it.
Proceedings of the IEEE 78: 1550-60.

"
1057,1995,When is an Integrate-and-fire Neuron like a Poisson Neuron?,,1057-when-is-an-integrate-and-fire-neuron-like-a-poisson-neuron.pdf,Abstract Missing,"When is an Integrate-and-fire Neuron
like a Poisson Neuron?

Charles F. Stevens
Salk Institute MNL/S
La Jolla, CA 92037
cfs@salk.edu

Anthony Zador
Salk Institute MNL/S
La Jolla, CA 92037
zador@salk.edu

Abstract

In the Poisson neuron model, the output is a rate-modulated Poisson process (Snyder and Miller, 1991); the time varying rate parameter ret) is an instantaneous function G[.] of the stimulus,
ret) = G[s(t)]. In a Poisson neuron, then, ret) gives the instantaneous firing rate-the instantaneous probability of firing at any
instant t-and the output is a stochastic function of the input. In
part because of its great simplicity, this model is widely used (usually with the addition of a refractory period) , especially in in vivo
single unit electrophysiological studies, where set) is usually taken
to be the value of some sensory stimulus. In the integrate-and-fire
neuron model, by contrast, the output is a filtered and thresholded
function of the input: the input is passed through a low-pass filter
(determined by the membrane time constant T) and integrated until the membrane potential vet) reaches threshold 8, at which point
vet) is reset to its initial value. By contrast with the Poisson model,
in the integrate-and-fire model the ouput is a deterministic function
of the input. Although the integrate-and-fire model is a caricature
of real neural dynamics, it captures many of the qualitative features, and is often used as a starting point for conceptualizing the
biophysical behavior of single neurons. Here we show how a slightly
modified Poisson model can be derived from the integrate-and-fire
model with noisy inputs yet) = set) + net). In the modified model,
the transfer function G[.] is a sigmoid (erf) whose shape is determined by the noise variance /T~. Understanding the equivalence
between the dominant in vivo and in vitro simple neuron models
may help forge links between the two levels.

c. F. STEVENS. A. ZADOR

104

1

Introduction

In the Poisson neuron model, the output is a rate-modulated Poisson process; the
time varying rate parameter ret) is an instantaneous function G[.] of the stimulus, ret) = G[s(t)]. In a Poisson neuron, then, ret) gives the instantaneous firing
rate-the instantaneous probability of firing at any instant t-and the output is a
stochastic function of the input. In part because of its great simplicity, this model
is widely used (usually with the addition of a refractory period), especially in in
vivo single unit electrophysiological studies, where set) is usually taken to be the
value of some sensory stimulus.
In the integrate-and-fire neuron model, by contrast, the output is a filtered and
thresholded function of the input: the input is passed through a low-pass filter
(determined by the membrane time constant T) and integrated until the membrane
potential vet) reaches threshold 0, at which point vet) is reset to its initial value.
By contrast with the Poisson model, in the integrate-and-fire model the ouput is
a deterministic function of the input. Although the integrate-and-fire model is a
caricature of real neural dynamics, it captures many of the qualitative features, and
is often used as a starting point for conceptualizing the biophysical behavior of single
neurons (Softky and Koch , 1993; Amit and Tsodyks, 1991; Shadlen and Newsome,
1995; Shadlen and Newsome, 1994; Softky, 1995; DeWeese, 1995; DeWeese, 1996;
Zador and Pearlmutter, 1996).
Here we show how a slightly modified Poisson model can be derived from the
integrate-and-fire model with noisy inputs yet) = set) + net). In the modified
model, the transfer function G[.] is a sigmoid (erf) whose shape is determined by
the noise variance (j~ . Understanding the equivalence between the dominant in vivo
and in vitro simple neuron models may help forge links between the two levels.

2

The integrate-and-fire model

Here we describe the the forgetful leaky integrate-and-fire model. Suppose we add
a signal set) to some noise net),

yet) = net) + set),
and threshold the sum to produce a spike train

z(t) = F[s(t) + net)],
where F is the thresholding functional and z(t) is a list of firing times generated by
the input. Specifically, suppose the voltage vet) of the neuron obeys
vet) = - vet)

+ yet)

(1)

T

where T is the membrane time constant. We assume that the noise net) has O-mean
and is white with variance (j~. Thus yet) can be thought of as a Gaussian white
process with variance (j~ and a time-varying mean set) . If the voltage reaches the
threshold 00 at some time t, the neuron emits a spike at that time and resets to
the initial condition Vo. This is therefore a 5 parameter model: the membrane
time constant T, the mean input signal Il, the variance of the input signal 17 2 , the
threshold 0, and the reset value Vo. Of course, if net) = 0, we recover a purely
deterministic integrate-and-fire model.

When Is an Integrate-and-fire Neuron like a Poisson Neuron?

105

In order to forge the link between the integrate-and-fire neuron dynamics and the
Poisson model, we will treat the firing times T probabilistically. That is, we will
express the output of the neuron to some particular input set) as a conditional
distribution p(Tls(t?, i.e. the probability of obtaining any firing time T given
some particular input set) .
Under these assumptions, peT) is given by the first passage time distribution
(FPTD) of the Ornstein-Uhlenbeck process (Uhlenbeck and Ornstein, 1930; Tuckwell, 1988). This means that the time evolution of the voltage prior to reaching
threshold is given by the Fokker-Planck equation (FPE),

8
8t g(t, v)

u; 8v8

=2

8
vet)
v) - av [(set) - --;:- )g(t, v)],

2

2 get,

where uy = Un and get, v) is the distribution at time t of voltage
Then the first passage time distribution is related to g( v, t) by

peT) = -

81

at

90

-00

(2)
-00

< v ::;

(}o.

(3)

get, v)dv.

The integrand is the fraction of all paths that p.ave not yet crossed threshold. peT)
is therefore just the interspike interval (lSI) distribution for a given signal set). A
general eigenfunction expansion solution for the lSI distribution is known, but it
converges slowly and its terms offer little insight into the behavior (at least to us) .
We now derive an expression for the probability of crossing threshold in some very
short interval ~t, starting at some v. We begin with the ""free"" distribution of g
(Tuckwell, 1988): the probability of the voltage jumping to v' at time t'
t + ~t,
given that it was at v at time t, assuming von Neumann boundary conditions at
plus and minus infinity,

=

1

get', v'lt, v) =

[

J27r q(~t;Uy)

exp -

?)2]

(v' - m( ~t; u y ,
2 q(~t;Uy)

(4)

with
and
m(~t)

= ve- at / + set) * T(l _ e- at /
T

T ),

where * denotes convolution. The free distribution is a Gaussian with a timedependent mean m(~t) and variance q(~t; u y ). This expression is valid for all ~t.
The probability of making a jump
~v

in a short interval

~t ~ T

depends only on

ga(~t, ~v; uy) =
For small

~t,

= v' 1

v

~v

..j27r qa(u y )

and

~t,

exp [_

~~2

)].

2 qa uy

(5)

we expand to get

qa(uy) :::::: 2u;~t,
which is independent of T, showing that the leak can be neglected for short times.

c. F. STEVENS, A. ZADOR

106

Now the probability Pt>, that the voltage exceeds threshold in some short Ilt, given
that it started at v, depends on how far v is from threshold; it is

Pr[v + Ilv

~

0] = Pr[llv

~

0 - v].

Thus

(Xl dvgt>,(llt, v; O""y)

J9-v

1
-erfc
2
1
-erfc
2

(6)

(o-v)
(o-v)
J2qt>,(O""y)

O""yJ21lt

I;

where erfc(x) = 1 - -j;
e-t~ dt goes from [2 : 0]. This then is the key result:
it gives the instantaneous probability of firing as a function of the instantaneous
voltage v. erfc is sigmoidal with a slope determined by O""y, so a smaller noise yields
a steeper (more deterministic) transfer function; in the limit of 0 noise, the transfer
function is a step and we recover a completely deterministic neuron.
Note that Pt>, is actually an instantaneous function of v(t), not the stimulus itself
s(t). If the noise is large compared with s(t) we must consider the distribution
g$ (v, t; O""y) of voltages reached in response to the input s(t):

(7)

Py(t)

3

Ensemble of Signals

What if the inputs s(t) are themselves drawn from an ensemble? If their distribution
is also Gaussian and white with mean Jl and variance
and if the firing rate is
low (E[T] ~ T), then the output spike train is Poisson. Why is firing Poisson only
in the slow firing limit? The reason is that, by assumption, immediately following
a spike the membrane potential resets to 0; it must then rise (assuming Jl > 0) to
some asymptotic level that is independent of the initial conditions. During this rise
the firing rate is lower than the asymptotic rate, because on average the membrane
is farther from threshold, and its variance is lower. The rate at which the asymptote
is achieved depends on T. In the limit as t ~ T, some asymptotic distribution of
voltage qoo(v), is attained. Note that if we make the reset Vo stochastic, with a
distribution given by qoo (v), then the firing probability would be the same even
immediately after spiking, and firing would be Poisson for all firing rates.

0"";,

A Poisson process is characterized by its mean alone. We therefore solve the FPE
(eq. 2) for the steady-state by setting ?tg(t, v) = 0 (we consider only threshold
crossings from initial values t ~ T; negYecting the early events results in only a
small error, since we have assumed E{T} ~ T). Thus with the absorbing boundary

107

When Is an Integrate-and-fire Neuron like a Poisson Neuron?

at 0 the distribution at time t

~ T

(given here for JJ

= 0) is

(8)
g~(Vj uy)= kl (1 - k2 erfi [uyfi]) exp [~i:] ,
where u; = u; + u~, erfi(z) = -ierf(iz), kl determines the normalization (the sign
of kl determines whether the solution extends to positive or negative infinity) and
k2 = l/erfi(O/(uy Vr)) is determined by the boundary. The instantaneous Poisson
rate parameter is then obtained through eq. (7),

(9)

Fig. 1 tests the validity of the exponential approximation. The top graph shows
the lSI distribution near the ""balance point"" , when the excitation is in balance with
the inhibition and the membrane potential hovers just subthreshold. The bottom
curves show the lSI distribution far below the balance point. In both cases, the
exponential distribution provides a good approximation for t ~ T.

4

Discussion

The main point of this paper is to make explicit the relation between the Poisson
and integrate-and-fire models of neuronal acitivity. The key difference between
them is that the former is stochastic while the latter is deterministic. That is, given
exactly the same stimulus, the Poisson neuron produces different spike trains on
different trials, while the integrate-and-fire neuron produces exactly the same spike
train each time. It is therefore clear that if some degree of stochasticity is to be
obtained in the integrate-and-fire model, it must arise from noise in the stimulus
itself.
The relation we have derived here is purely formalj we have intentionally remained
agnostic about the deep issues of what is signal and what is noise in the inputs to a
neuron. We observe nevertheless that although we derive a limit (eq. 9) where the
spike train of an integrate-and-fire neuron is a Poisson process-i.e. the probability
of obtaining a spike in any interval is independent of obtaining a spike in any other
interval (except for very short intervals)-from the point of view of information
processing it is a very different process from the purely stochastic rate-modulated
Poisson neuron. In fact, in this limit the spike train is deterministically Poisson
if u y = u., i. e. when n( t) = OJ in this case the output is a purely deterministic
function of the input, but the lSI distribution is exponential.

108

C. F. STEVENS, A. ZADOR

References
Amit, D. and Tsodyks, M. (1991). Quantitative study of attractor neural network retrieving at low spike rates. i. substrate-spikes, rates and neuronal gain.
Network: Computation in Neural Systems , 2:259-273 .
DeWeese, M. (1995). Optimization principles for the neural code. PhD thesis, Dept
of Physics, Princeton University.
DeWeese, M. (1996). Optimization principles for the neural code. In Hasselmo,
M., editor, Advances in Neural Information Processing Systems, vol. 8. MIT
Press, Cambridge, MA.
Shadlen, M. and Newsome, W. (1994) . Noise, neural codes and cortical organization.
Current Opinion in Neurobiology, 4:569-579.
Shadlen, M. and Newsome, W. (1995) . Is there a signal in the noise? [comment].
Current Opinion in Neurobiology, 5:248-250.
Snyder, D. and Miller, M. (1991). Random Point Processes in Time and Space, 2 nd
edition. Springer-Verlag.
Softky, W. (1995) . Simple codes versus efficient codes. Current Opinion in Neurobiology, 5:239-247 .
Softky, W. and Koch, C. (1993). The highly irregular firing of cortical cells is
inconsistent with temporal integration of random epsps. J. Neuroscience . ,
13:334-350.
Tuckwell, H. (1988). Introduction to theoretical neurobiology (2 vols.). Cambridge.
Uhlenbeck, G. and Ornstein, L. (1930). On the theory of brownian motion. Phys.
Rev., 36:823-84l.
Zador, A. M. and Pearlmutter, B. A. (1996) . VC dimension of an integrate and fire
neuron model. Neural Computation, 8(3) . In press.

When Is an Integrate-and-fire Neuron like a Poisson Neuron?

109

lSI distributions at balance point and the exponential limit
0.02
0.015
.~

15

.8

ea.

0.01
0.005

50

100

150

200

300
250
Time (msec)

350

400

450

500

200

400

600

800

1000 1200
lSI (msec)

1400

1600

1800

2000

2 x 10-3

1.5

~

~

.0

...0a.

1
0.5
0

0

Figure 1: lSI distributions. (A; top) lSI distribution for leaky integrate-and-fire
model at the balance point, where the asymptotic membrane potential is just subthreshold, for two values of the signal variance (1'2 . Increasing (1'2 shifts the distribution to the left . For the left curve, the parameters were chosen so that E{T} ~ T,
giving a nearly exponential distribution; for the right curve, the distribution would
be hard to distinguish experimentally from an exponential distribution with a refractory period. (T = 50 msec; left: E{T} = 166 msec; right: E{T} = 57 msec).
(B; bottom) In the subthreshold regime, the lSI distribution (solid} is nearly exponential (dashed) for intervals greater than the membrane time constant. (T = 50
msec; E{T} = 500 msec)

"
1058,1995,From Isolation to Cooperation: An Alternative View of a System of Experts,,1058-from-isolation-to-cooperation-an-alternative-view-of-a-system-of-experts.pdf,Abstract Missing,"From Isolation to Cooperation:
An Alternative View of a System of Experts
Stefan Schaal:!:*
sschaal@cc.gatech.edu
http://www.cc.gatech.eduifac/Stefan.Schaal

Christopher C. Atkeson:!:
cga@cc.gatech.edu
http://www.cc.gatech.eduifac/Chris.Atkeson

+College of Computing, Georgia Tech, 801 Atlantic Drive, Atlanta, GA 30332-0280

*ATR Human Infonnation Processing, 2-2 Hikaridai, Seiko-cho, Soraku-gun, 619-02 Kyoto

Abstract
We introduce a constructive, incremental learning system for regression
problems that models data by means of locally linear experts. In contrast
to other approaches, the experts are trained independently and do not
compete for data during learning. Only when a prediction for a query is
required do the experts cooperate by blending their individual predictions. Each expert is trained by minimizing a penalized local cross validation error using second order methods. In this way, an expert is able to
find a local distance metric by adjusting the size and shape of the receptive field in which its predictions are valid, and also to detect relevant input features by adjusting its bias on the importance of individual input
dimensions. We derive asymptotic results for our method. In a variety of
simulations the properties of the algorithm are demonstrated with respect
to interference, learning speed, prediction accuracy, feature detection,
and task oriented incremental learning.

1. INTRODUCTION
Distributing a learning task among a set of experts has become a popular method in computationallearning. One approach is to employ several experts, each with a global domain of
expertise (e.g., Wolpert, 1990). When an output for a given input is to be predicted, every
expert gives a prediction together with a confidence measure. The individual predictions
are combined into a single result, for instance, based on a confidence weighted average.
Another approach-the approach pursued in this paper-of employing experts is to create
experts with local domains of expertise. In contrast to the global experts, the local experts
have little overlap or no overlap at all. To assign a local domain of expertise to each expert,
it is necessary to learn an expert selection system in addition to the experts themselves.
This classifier determines which expert models are used in which part of the input space.
For incremental learning, competitive learning methods are usually applied. Here the experts compete for data such that they change their domains of expertise until a stable configuration is achieved (e.g., Jacobs, Jordan, Nowlan, & Hinton, 1991). The advantage of
local experts is that they can have simple parameterizations, such as locally constant or locally linear models. This offers benefits in terms of analyzability, learning speed, and robustness (e.g., Jordan & Jacobs, 1994). For simple experts, however, a large number of experts is necessary to model a function. As a result, the expert selection system has to be
more complicated and, thus, has a higher risk of getting stuck in local minima and/or of
learning rather slowly. In incremental learning, another potential danger arises when the
input distribution of the data changes. The expert selection system usually makes either
implicit or explicit prior assumptions about the input data distribution. For example, in the
classical mixture model (McLachlan & Basford, 1988) which was employed in several local expert approaches, the prior probabilities of each mixture model can be interpreted as

606

S. SCHAAL. C. C. ATKESON

the fraction of data points each expert expects to experience. Therefore, a change in input
distribution will cause all experts to change their domains of expertise in order to fulfill
these prior assumptions. This can lead to catastrophic interference.
In order to avoid these problems and to cope with the interference problems during incremental learning due to changes in input distribution, we suggest eliminating the competition among experts and instead isolating them during learning. Whenever some new data is
experienced which is not accounted for by one of the current experts, a new expert is created. Since the experts do not compete for data with their peers, there is no reason for them
to change the location of their domains of expertise. However, when it comes to making a
prediction at a query point, all the experts cooperate by giving a prediction of the output
together with a confidence measure. A blending of all the predictions of all experts results
in the final prediction. It should be noted that these local experts combine properties of
both the global and local experts mentioned previously. They act like global experts by
learning independently of each other and by blending their predictions, but they act like local experts by confining themselves to a local domain of expertise, i.e., their confidence
measures are large only in a local region.
The topic of data fitting with structurally simple local models (or experts) has received a
great deal of attention in nonparametric statistics (e.g., Nadaraya, 1964; Cleveland, 1979;
Scott, 1992, Hastie & Tibshirani, 1990). In this paper, we will demonstrate how a nonparametric approach can be applied to obtain the isolated expert network (Section 2.1),
how its asymptotic properties can be analyzed (Section 2.2), and what characteristics such
a learning system possesses in terms of the avoidance of interference, feature detection,
dimensionality reduction, and incremental learning of motor control tasks (Section 3).

2. RECEPTIVE FIELD WEIGHTED REGRESSION
This paper focuses on regression problems, i.e., the learning of a map from 9t n ~ 9t m ?
Each expert in our learning method, Receptive Field Weighted Regression (RFWR), consists of two elements, a locally linear model to represent the local functional relationship,
and a receptive field which determines the region in input space in which the expert's
knowledge is valid. As a result, a given data set will be modeled by piecewise linear elements, blended together. For 1000 noisy data points drawn from the unit interval of the
function z == max[exp(-10x 2 ),exp(-50l),1.25exp(-5(x 2 + l)], Figure 1 illustrates an
example of function fitting with RFWR. This function consists of a narrow and a wide
ridge which are perpendicular to each other, and a Gaussian bump at the origin. Figure 1b
shows the receptive fields which the system created during the learning process. Each experts' location is at the center of its receptive field, marked by a $ in Figure 1b. The recep1.5
0 .5
0
-0.5

-1
1.5

0.5
,1

,.,
10. 5%

0
-0.5

0

I
1- 0 .5
1

-1

0

-1.5
-1.5

- 0 .5

(a)

-1

x

(b)

-1

-0.5

o

0.5

1.5

x

Figure 1: (a) result of function approximation with RFWR. (b) contour lines of 0.1 iso-activation of
each expert in input space (the experts' centers are marked by small circles).

From Isolation to Cooperation: An Alternative View of a System of Experts

607

tive fields are modeled by Gaussian functions, and their 0.1 iso-activation lines are shown
in Figure 1b as well. As can be seen, each expert focuses on a certain region of the input
space, and the shape and orientation of this region reflects the function's complexity, or
more precisely, the function's curvature, in this region. It should be noticed that there is a
certain amount of overlap among the experts, and that the placement of experts occurred on
a greedy basis during learning and is not globally optimal. The approximation result
(Figure 1a) is a faithful reconstruction of the real function (MSE = 0.0025 on a test set, 30
epochs training, about 1 minute of computation on a SPARC1O). As a baseline comparison,
a similar result with a sigmoidal 3-layer neural network required about 100 hidden units
and 10000 epochs of annealed standard backpropagation (about 4 hours on a SPARC1O).

2.1 THE ALGORITHM

. .?... '.

~"""" ""

WeighBd' /
Average

Output

li'Iear

~:~~

Galng

Unrt

ConnectIOn

centered at e

y,

Figure 2: The RFWR network

RFWR can be sketched in network form as
shown in Figure 2. All inputs connect to all expert networks, and new experts can be added as
needed. Each expert is an independent entity. It
consists of a two layer linear subnet and a receptive field subnet. The receptive field subnet has a
single unit with a bell-shaped activation profile,
centered at the fixed location c in input space.
The maximal output of this unit is ""I"" at the center, and it decays to zero as a function of the distance from the center. For analytical convenience,
we choose this unit to be Gaussian:
(1)

x is the input vector, and D the distance metric, a positive definite matrix that is generated
from the upper triangular matrix M. The output of the linear subnet is:
(2)
y=x Tb + bo=x-Tf3
A

The connection strengths b of the linear subnet and its bias bO will be denoted by the d-dimensional vector f3 from now on, and the tilde sign will indicate that a vector has been
augmented by a constant ""I"", e.g., i = (x T , Il. In generating the total output, the receptive
field units act as a gating component on the output, such that the total prediction is:
(3)

The parameters f3 and M are the primary quantities which have to be adjusted in the learning process: f3 forms the locally linear model, while M determines the shape and orientation of the receptive fields . Learning is achieved by incrementally minimizing the cost
function:

(4)
The first term of this function is the weighted mean squared cross validation error over all
experienced data points, a local cross validation measure (Schaal & Atkeson, 1994). The
second term is a regularization or penalty term. Local cross validation by itself is consistent, i.e., with an increasing amount of data, the size of the receptive field of an expert
would shrink to zero. This would require the creation of an ever increasing number of experts during the course of learning. The penalty term introduces some non-vanishing bias
in each expert such that its receptive field size does not shrink to zero. By penalizing the
squared coefficients of D, we are essentially penalizing the second derivatives of the function at the site of the expert. This is similar to the approaches taken in spline fitting

608

S. SCHAAL, C. C. A TI(ESON

(deBoor, 1978) and acts as a low-pass filter: the higher the second derivatives, the more
smoothing (and thus bias) will be introduced. This will be analyzed further in Section 2.2.
The update equations for the linear subnet are the standard weighted recursive least squares
equation with forgetting factor A (Ljung & SOderstrom, 1986):

f3 n+1 =f3n+wpn+lxe

1(
wherepn+1 =_ pn_

A

cv'

pn- -Tpn )
xx
ande =(y-x T f3n)
Ajw + xTpnx
cv

(5)

This is a Newton method, and it requires maintaining the matrix P, which is size
0.5d x (d + 1) . The update of the receptive field subnet is a gradient descent in J:

Mn+l=Mn- a dJ!aM

(6)

Due to space limitations, the derivation of the derivative in (6) will not be explained here.
The major ingredient is to take this derivative as in a batch update, and then to reformulate
the result as an iterative scheme. The derivatives in batch mode can be calculated exactly
due to the Sherman-Morrison-Woodbury theorem (Belsley, Kuh, & Welsch, 1980; Atkeson, 1992). The derivative for the incremental update is a very good approximation to
the batch update and realizes incremental local cross validation.
A new expert is initialized with a default M de! and all other variables set to zero, except the
matrix P. P is initialized as a diagonal matrix with elements 11 r/, where the ri are usually
small quantities, e.g., 0.01. The ri are ridge regression parameters. From a probabilistic
view, they are Bayesian priors that the f3 vector is the zero vector. From an algorithmic
view, they are fake data points of the form [x = (0, ... , '12 ,o, ... l,y = 0] (Atkeson, Moore, &
Schaal, submitted). Using the update rule (5), the influence of the ridge regression parameters would fade away due to the forgetting factor A. However, it is useful to make the
ridge regression parameters adjustable. As in (6), rj can be updated by gradient descent:
1'I n+1

= 1'n I

a aJ/ar
I

(7)

There are d ridge regression parameters, one for each diagonal element of the P matrix. In
order to add in the update of the ridge parameters as well as to compensate for the forgetting factor, an iterative procedure based on (5) can be devised which we omit here. The
computational complexity of this update is much reduced in comparison to (5) since many
computations involve multiplications by zero.
In sum, a RFWR expert consists of
three sets of parameters, one for
the locally linear model, one for
end;
the size and shape of the receptive
b)
Ir no expert was activated by more than Wgen :
- create a new expert with c=x
fields,
and one for the bias. The
end;
c)
Ir two experts are acti vated more than W pn..~
linear model parameters are up- erase the expert with the smaller receptive field
dated by a Newton method, while
end;
d)
calculate the mean, err""""an' and standard de viation errslIl of the
the other parameters are updated
incrementally accumulated error er,! of all experts;
by gradient descent. In our implee)
For k.= I to #experts:
Ir (Itrr! - err_I> 9 er'Sld) reinitialize expert k with M = 2 ? Mdef
mentations, we actually use second
end;
end;
order gradient descent based on
Sutton (1992), since, with minor
extra effort, we can obtain estimates of the second derivatives of the cost function with respect to all parameters. Finally, the logic of RFWR becomes as shown in the pseudo-code
above. Point c) and e) of the algorithm introduce a pruning facility. Pruning takes place either when two experts overlap too much, or when an expert has an exceptionally large
mean squared error. The latter method corresponds to a simple form of outlier detection.
Local optimization of a distance metric always has a minimum for a very large receptive
field size. In our case, this would mean that an expert favors global instead of locally linear
regression. Such an expert will accumulate a very large error which can easily be detected
Initialize the RFWR network. with no expert;
For every new training sample (x,y):
a)
For k= I to #experts:
- calculate the activation from (I)
- update the expert's parameters according to (5), (6), and (7)

From Isolation to Cooperation: An Alternative View of a System of Experts

609

in the given way. The mean squared error term, err, on which this outlier detection is
based, is a bias-corrected mean squared error, as will be explained below.
2.2 ASYMPTOTIC BIAS AND PENALTY SELECTION
The penalty term in the cost function (4) introduces bias. In order to assess the asymptotic
value of this bias, the real function f(x) , which is to be learned, is assumed to be represented as a Taylor series expansion at the center of an expert's receptive field. Without loss
of generality, the center is assumed to be at the origin in input space. We furthermore assume that the size and shape of the receptive field are such that terms higher than 0(2) are
negligible. Thus, the cost (4) can be written as:
J

~ (1w(f. +fTX+~XTFX-bo -bTxYdx )/(1wdx )+r~Dnm

(8)

where fo' f, and F denote the constant, linear, and quadratic terms of the Taylor series
expansion, respectively. Inserting Equation (1), the integrals can be solved analytically after the input space is rotated by an orthonormal matrix transforming F to the diagonal matrix F'. Subsequently, bo' b, and D can be determined such that J is minimized:

b~ =fa + bias = fa + ~075 ~ sgn(F:')~IF;,:I,
0.25

(

)

b'

= f,

D::

~
= (2r)2

(9)

This states that the linear model will asymptotically acquire the correct locally linear
model, while the constant term will have bias proportional to the square root of the sum of
the eigenvalues of F, i.e., the n ? The distance metric D, whose diagonalized counterpart
is D', will be a scaled image of the Hessian F with an additional square root distortion.
Thus, the penalty term accomplishes the intended task: it introduces more smoothing the
higher the curvature at an expert's location is, and it prevents the receptive field of an expert shrinking to zero size (which would obviously happen for r ~ 0). Additionally,
Equation (9) shows how to determine rfor a given learning problem from an estimate of
the eigenvalues and a permissible bias. Finally, it is possible to derive estimates of the bias
and the mean squared error of each expert from the current distance metric D:

F:

biasesl = ~0. 5r IJeigenvalues(D)l.; en,,~, = r

Ln.mD;m

(10)

The latter term was incorporated in the mean squared error, err, in Section 2.1. Empirical
evaluations (not shown here) verified the validity of these asymptotic results.

3. SIMULATION RESULTS
This section will demonstrate some of the properties of RFWR. In all simulations, the
threshold parameters of the algorithm were set to = 3.5, w prune = 0.9, and wmin = 0.1.
These quantities determine the overlap of the experts as well as the outlier removal threshold; the results below are not affected by moderate changes in these parameters.

e

3.1 AVOIDING INTERFERENCE
In order to test RFWR's sensitivity with respect to changes in input data distribution, the
data of the example of Figure 1 was partitioned into three separate training sets
1; = {(x, y, z) 1-1.0 < x < -O.2} , 1; = {(x, y, z) 1-0.4 < x < OA}, 1; = {(x, y, z) I 0.2 < x < 1.0} .
These data sets correspond to three overlapping stripes of data, each having about 400 uniformly distributed samples. From scratch, a RFWR network was trained first on I; for 20
epochs, then on T2 for 20 epochs, and finally on 1; for 20 epochs. The penalty was chosen
as in the example of Figure 1 to be r = I.e - 7 , which corresponds to an asymptotic bias of

S. SCHAAL, C. C. ATKESON

610

0.1 at the sharp ridge of the function. The default distance metric D was 50*1, where I is
the identity matrix. Figure 3 shows the results of this experiment. Very little interference
can be found. The MSE on the test set increased from 0.0025 (of the original experiment of
Figure 1) to 0.003, which is still an excellent reconstruction of the real function.
y

0 .5
-0 . 5

-0.5

(a)

(b)

Figure 3: Reconstructed function after training on (a)

(c)

7;, (b) then

-1

~,(c)

and finally

1;.

3.2 LOCAL FEATURE DETECTION
The examples of RFWR given so far did not require ridge regression parameters. Their importance, however, becomes obvious when dealing with locally rank deficient data or with
irrelevant input dimensions. A learning system should be able to recognize irrelevant input
dimensions. It is important to note that this cannot be accomplished by a distance metric.
The distance metric is only able to decide to what spatial extent averaging over data in a
certain dimension should be performed. However, the distance metric has no means to exclude an input dimension. In contrast, bias learning with ridge regression parameters is able
to exclude input dimensions. To demonstrate this, we added 8 purely noisy inputs
(N(0,0.3)) to the data drawn from the function of Figure 1. After 30 epochs of training on a
10000 data point training set, we analyzed histograms of the order of magnitude of the
ridge regression parameters in all 100bias input dimensions over all the 79 experts that had
been generated by the learning algorithm. All experts recognized that the input dimensions
3 to 8 did not contain relevant information, and correctly increased the corresponding ridge
parameters to large values. The effect of a large ridge regression parameter is that the associated regression coefficient becomes zero. In contrast, the ridge parameters of the inputs 1,
2, and the bias input remained very small. The MSE on the test set was 0.0026, basically
identical to the experiment with the original training set.

3.3 LEARNING AN INVERSE DYNAMICS MODEL OF A ROBOT ARM
Robot learning is one of the domains where incremental learning plays an important role. A
real movement system experiences data at a high rate, and it should incorporate this data
immediately to improve its performance. As learning is task oriented, input distributions
will also be task oriented and interference problems can easily arise. Additionally, a real
movement system does not sample data from a training set but rather has to move in order
to receive new data. Thus, training data is always temporally correlated, and learning must
be able to cope with this. An example of such a learning task is given in Figure 4 where a
simulated 2 DOF robot arm has to learn to draw the figure ""8"" in two different regions of
the work space at a moderate speed (1.5 sec duration). In this example, we assume that the
correct movement plan exists, but that the inverse dynamics model which is to be used to
control this movement has not been acquired. The robot is first trained for 10 minutes (real
movement time) in the region of the lower target trajectory where it performs a variety of
rhythmic movements under simple PID control. The initial performance of this controller is
shown in the bottom part of Figure 4a. This training enables the robot to learn the locally
appropriate inverse dynamics model, a ~6 ~ ~2 continuous mapping. Subsequent per-

From Isolation to Cooperation: An Alternative View of a System of Experts
0.5

t

0.'

GralMy

0.'
0.2
0.1

..,.~t
~.

~

8

Z

8

8

?0.4

(b)

(a)

(0)

~.5

0

0.1

0.2

0.3

0.4

0.!5

Figure 4: Learning to draw the figure ""8"" with a 2-joint
arm: (a) Performance of a PID controller before learning (the dimmed lines denote the desired trajectories,
the solid lines the actual performance); (b) Performance after learning using a PD controller with feedforward commands from the learned inverse model; (c)
Performance of the learned controller after training on
the upper ""8"" of (b) (see text for more explanations).

611

formance using this inverse model for
control is depicted in the bottom part
of Figure 4b. Afterwards, the same
training takes place in the region of the
upper target trajectory in order to acquire the inverse model in this part of
the world. The figure ""8"" can then
equally well be drawn there (upper
part of Figure 4a,b). Switching back to
the bottom part of the work space
(Figure 4c), the first task can still be
performed as before. No interference
is recognizable. Thus, the robot could
learn fast and reliably to fulfill the two
tasks. It is important to note that the
data generated by the training movements did not always have locally full
rank. All the parameters of RFWR
were necessary to acquire the local inverse model appropriately. A total of

39 locally linear experts were generated.

4. DISCUSSION
We have introduced an incremental learning algorithm, RFWR, which constructs a network
of isolated experts for supervised learning of regression tasks. Each expert determines a locally linear model, a local distance metric, and local bias parameters by incrementally
minimizing a penalized local cross validation error. Our algorithm differs from other local
learning techniques by entirely avoiding competition among the experts, and by being
based on nonparametric instead of parametric statistics. The resulting properties of RFWR
are a) avoidance of interference in the case of changing input distributions, b) fast incremental learning by means of Newton and second order gradient descent methods, c) analyzable asymptotic properties which facilitate the selection of the fit parameters, and d) local feature detection and dimensionality reduction. The isolated experts are also ideally
suited for parallel implementations. Future work will investigate computationally less
costly delta-rule implementations of RFWR, and how well RFWR scales in higher dimensions.

5. REFERENCES
Atkeson, C. G., Moore, A. W. , & Schaal, S.
(submitted). ""Locally weighted learning."" Artificial Intelligence Review.
Atkeson, C. G. (1992). ""Memory-based approaches to
approximating continuous functions."" In: Casdagli, M.,
& Eubank, S. (Eds.), Nonlinear Modeling and Forecasting, pp.503-521. Addison Wesley.
Belsley, D. A., Kuh, E., & Welsch, R. E. (1980). Regression diagnostics: Identifying influential data and
sources ofcollinearity. New York: Wiley.
Cleveland, W. S. (1979). ""Robust locally weighted regression and smoothing scatterplots."" J. American Stat.
Association, 74, pp.829-836.
de Boor, C. (1978). A practical guide to splines. New
York: Springer.
Hastie, T. J., & Tibshirani, R. J. (1990). Generalized
additive models. London: Chapman and Hall.
Jacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton,
G. E. (1991). ""Adaptive mixtures of local experts.""
Neural Computation, 3, pp.79-87.

Jordan, M. I., & Jacobs, R. (1994). ""Hierarchical mixtures of experts and the EM algorithm."" Neural Computation, 6, pp.79-87.
Ljung, L., & S_derstr_m, T. (1986). Theory and practice of recursive identification. Cambridge, MIT Press.
McLachlan, G. J., & Basford, K. E. (1988). Mixture
models . New York: Marcel Dekker.
Nadaraya, E. A. (1964). ""On estimating regression .""
Theor. Prob. Appl., 9, pp.141-142.
Schaal, S., & Atkeson, C. G. (l994b). ""Assessing the
quality of learned local models."" In: Cowan, J. ,Tesauro, G., & Alspector, J. (Eds.), Advances in Neural
Information Processing Systems 6. Morgan Kaufmann.
Scott, D. W. (1992). Multivariate Density Estimation.
New York: Wiley.
Sutton, R. S. (1992). ""Gain adaptation beats least
squares."" In: Proc. of 7th Yale Workshop on Adaptive
and Learning Systems, New Haven, CT.
Wolpert, D. H. (1990). ""Stacked genealization."" Los
Alamos Technical Report LA-UR-90-3460.

"
1059,1995,Boosting Decision Trees,,1059-boosting-decision-trees.pdf,Abstract Missing,"From Isolation to Cooperation:
An Alternative View of a System of Experts
Stefan Schaal:!:*
sschaal@cc.gatech.edu
http://www.cc.gatech.eduifac/Stefan.Schaal

Christopher C. Atkeson:!:
cga@cc.gatech.edu
http://www.cc.gatech.eduifac/Chris.Atkeson

+College of Computing, Georgia Tech, 801 Atlantic Drive, Atlanta, GA 30332-0280

*ATR Human Infonnation Processing, 2-2 Hikaridai, Seiko-cho, Soraku-gun, 619-02 Kyoto

Abstract
We introduce a constructive, incremental learning system for regression
problems that models data by means of locally linear experts. In contrast
to other approaches, the experts are trained independently and do not
compete for data during learning. Only when a prediction for a query is
required do the experts cooperate by blending their individual predictions. Each expert is trained by minimizing a penalized local cross validation error using second order methods. In this way, an expert is able to
find a local distance metric by adjusting the size and shape of the receptive field in which its predictions are valid, and also to detect relevant input features by adjusting its bias on the importance of individual input
dimensions. We derive asymptotic results for our method. In a variety of
simulations the properties of the algorithm are demonstrated with respect
to interference, learning speed, prediction accuracy, feature detection,
and task oriented incremental learning.

1. INTRODUCTION
Distributing a learning task among a set of experts has become a popular method in computationallearning. One approach is to employ several experts, each with a global domain of
expertise (e.g., Wolpert, 1990). When an output for a given input is to be predicted, every
expert gives a prediction together with a confidence measure. The individual predictions
are combined into a single result, for instance, based on a confidence weighted average.
Another approach-the approach pursued in this paper-of employing experts is to create
experts with local domains of expertise. In contrast to the global experts, the local experts
have little overlap or no overlap at all. To assign a local domain of expertise to each expert,
it is necessary to learn an expert selection system in addition to the experts themselves.
This classifier determines which expert models are used in which part of the input space.
For incremental learning, competitive learning methods are usually applied. Here the experts compete for data such that they change their domains of expertise until a stable configuration is achieved (e.g., Jacobs, Jordan, Nowlan, & Hinton, 1991). The advantage of
local experts is that they can have simple parameterizations, such as locally constant or locally linear models. This offers benefits in terms of analyzability, learning speed, and robustness (e.g., Jordan & Jacobs, 1994). For simple experts, however, a large number of experts is necessary to model a function. As a result, the expert selection system has to be
more complicated and, thus, has a higher risk of getting stuck in local minima and/or of
learning rather slowly. In incremental learning, another potential danger arises when the
input distribution of the data changes. The expert selection system usually makes either
implicit or explicit prior assumptions about the input data distribution. For example, in the
classical mixture model (McLachlan & Basford, 1988) which was employed in several local expert approaches, the prior probabilities of each mixture model can be interpreted as

606

S. SCHAAL. C. C. ATKESON

the fraction of data points each expert expects to experience. Therefore, a change in input
distribution will cause all experts to change their domains of expertise in order to fulfill
these prior assumptions. This can lead to catastrophic interference.
In order to avoid these problems and to cope with the interference problems during incremental learning due to changes in input distribution, we suggest eliminating the competition among experts and instead isolating them during learning. Whenever some new data is
experienced which is not accounted for by one of the current experts, a new expert is created. Since the experts do not compete for data with their peers, there is no reason for them
to change the location of their domains of expertise. However, when it comes to making a
prediction at a query point, all the experts cooperate by giving a prediction of the output
together with a confidence measure. A blending of all the predictions of all experts results
in the final prediction. It should be noted that these local experts combine properties of
both the global and local experts mentioned previously. They act like global experts by
learning independently of each other and by blending their predictions, but they act like local experts by confining themselves to a local domain of expertise, i.e., their confidence
measures are large only in a local region.
The topic of data fitting with structurally simple local models (or experts) has received a
great deal of attention in nonparametric statistics (e.g., Nadaraya, 1964; Cleveland, 1979;
Scott, 1992, Hastie & Tibshirani, 1990). In this paper, we will demonstrate how a nonparametric approach can be applied to obtain the isolated expert network (Section 2.1),
how its asymptotic properties can be analyzed (Section 2.2), and what characteristics such
a learning system possesses in terms of the avoidance of interference, feature detection,
dimensionality reduction, and incremental learning of motor control tasks (Section 3).

2. RECEPTIVE FIELD WEIGHTED REGRESSION
This paper focuses on regression problems, i.e., the learning of a map from 9t n ~ 9t m ?
Each expert in our learning method, Receptive Field Weighted Regression (RFWR), consists of two elements, a locally linear model to represent the local functional relationship,
and a receptive field which determines the region in input space in which the expert's
knowledge is valid. As a result, a given data set will be modeled by piecewise linear elements, blended together. For 1000 noisy data points drawn from the unit interval of the
function z == max[exp(-10x 2 ),exp(-50l),1.25exp(-5(x 2 + l)], Figure 1 illustrates an
example of function fitting with RFWR. This function consists of a narrow and a wide
ridge which are perpendicular to each other, and a Gaussian bump at the origin. Figure 1b
shows the receptive fields which the system created during the learning process. Each experts' location is at the center of its receptive field, marked by a $ in Figure 1b. The recep1.5
0 .5
0
-0.5

-1
1.5

0.5
,1

,.,
10. 5%

0
-0.5

0

I
1- 0 .5
1

-1

0

-1.5
-1.5

- 0 .5

(a)

-1

x

(b)

-1

-0.5

o

0.5

1.5

x

Figure 1: (a) result of function approximation with RFWR. (b) contour lines of 0.1 iso-activation of
each expert in input space (the experts' centers are marked by small circles).

From Isolation to Cooperation: An Alternative View of a System of Experts

607

tive fields are modeled by Gaussian functions, and their 0.1 iso-activation lines are shown
in Figure 1b as well. As can be seen, each expert focuses on a certain region of the input
space, and the shape and orientation of this region reflects the function's complexity, or
more precisely, the function's curvature, in this region. It should be noticed that there is a
certain amount of overlap among the experts, and that the placement of experts occurred on
a greedy basis during learning and is not globally optimal. The approximation result
(Figure 1a) is a faithful reconstruction of the real function (MSE = 0.0025 on a test set, 30
epochs training, about 1 minute of computation on a SPARC1O). As a baseline comparison,
a similar result with a sigmoidal 3-layer neural network required about 100 hidden units
and 10000 epochs of annealed standard backpropagation (about 4 hours on a SPARC1O).

2.1 THE ALGORITHM

. .?... '.

~"""" ""

WeighBd' /
Average

Output

li'Iear

~:~~

Galng

Unrt

ConnectIOn

centered at e

y,

Figure 2: The RFWR network

RFWR can be sketched in network form as
shown in Figure 2. All inputs connect to all expert networks, and new experts can be added as
needed. Each expert is an independent entity. It
consists of a two layer linear subnet and a receptive field subnet. The receptive field subnet has a
single unit with a bell-shaped activation profile,
centered at the fixed location c in input space.
The maximal output of this unit is ""I"" at the center, and it decays to zero as a function of the distance from the center. For analytical convenience,
we choose this unit to be Gaussian:
(1)

x is the input vector, and D the distance metric, a positive definite matrix that is generated
from the upper triangular matrix M. The output of the linear subnet is:
(2)
y=x Tb + bo=x-Tf3
A

The connection strengths b of the linear subnet and its bias bO will be denoted by the d-dimensional vector f3 from now on, and the tilde sign will indicate that a vector has been
augmented by a constant ""I"", e.g., i = (x T , Il. In generating the total output, the receptive
field units act as a gating component on the output, such that the total prediction is:
(3)

The parameters f3 and M are the primary quantities which have to be adjusted in the learning process: f3 forms the locally linear model, while M determines the shape and orientation of the receptive fields . Learning is achieved by incrementally minimizing the cost
function:

(4)
The first term of this function is the weighted mean squared cross validation error over all
experienced data points, a local cross validation measure (Schaal & Atkeson, 1994). The
second term is a regularization or penalty term. Local cross validation by itself is consistent, i.e., with an increasing amount of data, the size of the receptive field of an expert
would shrink to zero. This would require the creation of an ever increasing number of experts during the course of learning. The penalty term introduces some non-vanishing bias
in each expert such that its receptive field size does not shrink to zero. By penalizing the
squared coefficients of D, we are essentially penalizing the second derivatives of the function at the site of the expert. This is similar to the approaches taken in spline fitting

608

S. SCHAAL, C. C. A TI(ESON

(deBoor, 1978) and acts as a low-pass filter: the higher the second derivatives, the more
smoothing (and thus bias) will be introduced. This will be analyzed further in Section 2.2.
The update equations for the linear subnet are the standard weighted recursive least squares
equation with forgetting factor A (Ljung & SOderstrom, 1986):

f3 n+1 =f3n+wpn+lxe

1(
wherepn+1 =_ pn_

A

cv'

pn- -Tpn )
xx
ande =(y-x T f3n)
Ajw + xTpnx
cv

(5)

This is a Newton method, and it requires maintaining the matrix P, which is size
0.5d x (d + 1) . The update of the receptive field subnet is a gradient descent in J:

Mn+l=Mn- a dJ!aM

(6)

Due to space limitations, the derivation of the derivative in (6) will not be explained here.
The major ingredient is to take this derivative as in a batch update, and then to reformulate
the result as an iterative scheme. The derivatives in batch mode can be calculated exactly
due to the Sherman-Morrison-Woodbury theorem (Belsley, Kuh, & Welsch, 1980; Atkeson, 1992). The derivative for the incremental update is a very good approximation to
the batch update and realizes incremental local cross validation.
A new expert is initialized with a default M de! and all other variables set to zero, except the
matrix P. P is initialized as a diagonal matrix with elements 11 r/, where the ri are usually
small quantities, e.g., 0.01. The ri are ridge regression parameters. From a probabilistic
view, they are Bayesian priors that the f3 vector is the zero vector. From an algorithmic
view, they are fake data points of the form [x = (0, ... , '12 ,o, ... l,y = 0] (Atkeson, Moore, &
Schaal, submitted). Using the update rule (5), the influence of the ridge regression parameters would fade away due to the forgetting factor A. However, it is useful to make the
ridge regression parameters adjustable. As in (6), rj can be updated by gradient descent:
1'I n+1

= 1'n I

a aJ/ar
I

(7)

There are d ridge regression parameters, one for each diagonal element of the P matrix. In
order to add in the update of the ridge parameters as well as to compensate for the forgetting factor, an iterative procedure based on (5) can be devised which we omit here. The
computational complexity of this update is much reduced in comparison to (5) since many
computations involve multiplications by zero.
In sum, a RFWR expert consists of
three sets of parameters, one for
the locally linear model, one for
end;
the size and shape of the receptive
b)
Ir no expert was activated by more than Wgen :
- create a new expert with c=x
fields,
and one for the bias. The
end;
c)
Ir two experts are acti vated more than W pn..~
linear model parameters are up- erase the expert with the smaller receptive field
dated by a Newton method, while
end;
d)
calculate the mean, err""""an' and standard de viation errslIl of the
the other parameters are updated
incrementally accumulated error er,! of all experts;
by gradient descent. In our implee)
For k.= I to #experts:
Ir (Itrr! - err_I> 9 er'Sld) reinitialize expert k with M = 2 ? Mdef
mentations, we actually use second
end;
end;
order gradient descent based on
Sutton (1992), since, with minor
extra effort, we can obtain estimates of the second derivatives of the cost function with respect to all parameters. Finally, the logic of RFWR becomes as shown in the pseudo-code
above. Point c) and e) of the algorithm introduce a pruning facility. Pruning takes place either when two experts overlap too much, or when an expert has an exceptionally large
mean squared error. The latter method corresponds to a simple form of outlier detection.
Local optimization of a distance metric always has a minimum for a very large receptive
field size. In our case, this would mean that an expert favors global instead of locally linear
regression. Such an expert will accumulate a very large error which can easily be detected
Initialize the RFWR network. with no expert;
For every new training sample (x,y):
a)
For k= I to #experts:
- calculate the activation from (I)
- update the expert's parameters according to (5), (6), and (7)

From Isolation to Cooperation: An Alternative View of a System of Experts

609

in the given way. The mean squared error term, err, on which this outlier detection is
based, is a bias-corrected mean squared error, as will be explained below.
2.2 ASYMPTOTIC BIAS AND PENALTY SELECTION
The penalty term in the cost function (4) introduces bias. In order to assess the asymptotic
value of this bias, the real function f(x) , which is to be learned, is assumed to be represented as a Taylor series expansion at the center of an expert's receptive field. Without loss
of generality, the center is assumed to be at the origin in input space. We furthermore assume that the size and shape of the receptive field are such that terms higher than 0(2) are
negligible. Thus, the cost (4) can be written as:
J

~ (1w(f. +fTX+~XTFX-bo -bTxYdx )/(1wdx )+r~Dnm

(8)

where fo' f, and F denote the constant, linear, and quadratic terms of the Taylor series
expansion, respectively. Inserting Equation (1), the integrals can be solved analytically after the input space is rotated by an orthonormal matrix transforming F to the diagonal matrix F'. Subsequently, bo' b, and D can be determined such that J is minimized:

b~ =fa + bias = fa + ~075 ~ sgn(F:')~IF;,:I,
0.25

(

)

b'

= f,

D::

~
= (2r)2

(9)

This states that the linear model will asymptotically acquire the correct locally linear
model, while the constant term will have bias proportional to the square root of the sum of
the eigenvalues of F, i.e., the n ? The distance metric D, whose diagonalized counterpart
is D', will be a scaled image of the Hessian F with an additional square root distortion.
Thus, the penalty term accomplishes the intended task: it introduces more smoothing the
higher the curvature at an expert's location is, and it prevents the receptive field of an expert shrinking to zero size (which would obviously happen for r ~ 0). Additionally,
Equation (9) shows how to determine rfor a given learning problem from an estimate of
the eigenvalues and a permissible bias. Finally, it is possible to derive estimates of the bias
and the mean squared error of each expert from the current distance metric D:

F:

biasesl = ~0. 5r IJeigenvalues(D)l.; en,,~, = r

Ln.mD;m

(10)

The latter term was incorporated in the mean squared error, err, in Section 2.1. Empirical
evaluations (not shown here) verified the validity of these asymptotic results.

3. SIMULATION RESULTS
This section will demonstrate some of the properties of RFWR. In all simulations, the
threshold parameters of the algorithm were set to = 3.5, w prune = 0.9, and wmin = 0.1.
These quantities determine the overlap of the experts as well as the outlier removal threshold; the results below are not affected by moderate changes in these parameters.

e

3.1 AVOIDING INTERFERENCE
In order to test RFWR's sensitivity with respect to changes in input data distribution, the
data of the example of Figure 1 was partitioned into three separate training sets
1; = {(x, y, z) 1-1.0 < x < -O.2} , 1; = {(x, y, z) 1-0.4 < x < OA}, 1; = {(x, y, z) I 0.2 < x < 1.0} .
These data sets correspond to three overlapping stripes of data, each having about 400 uniformly distributed samples. From scratch, a RFWR network was trained first on I; for 20
epochs, then on T2 for 20 epochs, and finally on 1; for 20 epochs. The penalty was chosen
as in the example of Figure 1 to be r = I.e - 7 , which corresponds to an asymptotic bias of

S. SCHAAL, C. C. ATKESON

610

0.1 at the sharp ridge of the function. The default distance metric D was 50*1, where I is
the identity matrix. Figure 3 shows the results of this experiment. Very little interference
can be found. The MSE on the test set increased from 0.0025 (of the original experiment of
Figure 1) to 0.003, which is still an excellent reconstruction of the real function.
y

0 .5
-0 . 5

-0.5

(a)

(b)

Figure 3: Reconstructed function after training on (a)

(c)

7;, (b) then

-1

~,(c)

and finally

1;.

3.2 LOCAL FEATURE DETECTION
The examples of RFWR given so far did not require ridge regression parameters. Their importance, however, becomes obvious when dealing with locally rank deficient data or with
irrelevant input dimensions. A learning system should be able to recognize irrelevant input
dimensions. It is important to note that this cannot be accomplished by a distance metric.
The distance metric is only able to decide to what spatial extent averaging over data in a
certain dimension should be performed. However, the distance metric has no means to exclude an input dimension. In contrast, bias learning with ridge regression parameters is able
to exclude input dimensions. To demonstrate this, we added 8 purely noisy inputs
(N(0,0.3)) to the data drawn from the function of Figure 1. After 30 epochs of training on a
10000 data point training set, we analyzed histograms of the order of magnitude of the
ridge regression parameters in all 100bias input dimensions over all the 79 experts that had
been generated by the learning algorithm. All experts recognized that the input dimensions
3 to 8 did not contain relevant information, and correctly increased the corresponding ridge
parameters to large values. The effect of a large ridge regression parameter is that the associated regression coefficient becomes zero. In contrast, the ridge parameters of the inputs 1,
2, and the bias input remained very small. The MSE on the test set was 0.0026, basically
identical to the experiment with the original training set.

3.3 LEARNING AN INVERSE DYNAMICS MODEL OF A ROBOT ARM
Robot learning is one of the domains where incremental learning plays an important role. A
real movement system experiences data at a high rate, and it should incorporate this data
immediately to improve its performance. As learning is task oriented, input distributions
will also be task oriented and interference problems can easily arise. Additionally, a real
movement system does not sample data from a training set but rather has to move in order
to receive new data. Thus, training data is always temporally correlated, and learning must
be able to cope with this. An example of such a learning task is given in Figure 4 where a
simulated 2 DOF robot arm has to learn to draw the figure ""8"" in two different regions of
the work space at a moderate speed (1.5 sec duration). In this example, we assume that the
correct movement plan exists, but that the inverse dynamics model which is to be used to
control this movement has not been acquired. The robot is first trained for 10 minutes (real
movement time) in the region of the lower target trajectory where it performs a variety of
rhythmic movements under simple PID control. The initial performance of this controller is
shown in the bottom part of Figure 4a. This training enables the robot to learn the locally
appropriate inverse dynamics model, a ~6 ~ ~2 continuous mapping. Subsequent per-

From Isolation to Cooperation: An Alternative View of a System of Experts
0.5

t

0.'

GralMy

0.'
0.2
0.1

..,.~t
~.

~

8

Z

8

8

?0.4

(b)

(a)

(0)

~.5

0

0.1

0.2

0.3

0.4

0.!5

Figure 4: Learning to draw the figure ""8"" with a 2-joint
arm: (a) Performance of a PID controller before learning (the dimmed lines denote the desired trajectories,
the solid lines the actual performance); (b) Performance after learning using a PD controller with feedforward commands from the learned inverse model; (c)
Performance of the learned controller after training on
the upper ""8"" of (b) (see text for more explanations).

611

formance using this inverse model for
control is depicted in the bottom part
of Figure 4b. Afterwards, the same
training takes place in the region of the
upper target trajectory in order to acquire the inverse model in this part of
the world. The figure ""8"" can then
equally well be drawn there (upper
part of Figure 4a,b). Switching back to
the bottom part of the work space
(Figure 4c), the first task can still be
performed as before. No interference
is recognizable. Thus, the robot could
learn fast and reliably to fulfill the two
tasks. It is important to note that the
data generated by the training movements did not always have locally full
rank. All the parameters of RFWR
were necessary to acquire the local inverse model appropriately. A total of

39 locally linear experts were generated.

4. DISCUSSION
We have introduced an incremental learning algorithm, RFWR, which constructs a network
of isolated experts for supervised learning of regression tasks. Each expert determines a locally linear model, a local distance metric, and local bias parameters by incrementally
minimizing a penalized local cross validation error. Our algorithm differs from other local
learning techniques by entirely avoiding competition among the experts, and by being
based on nonparametric instead of parametric statistics. The resulting properties of RFWR
are a) avoidance of interference in the case of changing input distributions, b) fast incremental learning by means of Newton and second order gradient descent methods, c) analyzable asymptotic properties which facilitate the selection of the fit parameters, and d) local feature detection and dimensionality reduction. The isolated experts are also ideally
suited for parallel implementations. Future work will investigate computationally less
costly delta-rule implementations of RFWR, and how well RFWR scales in higher dimensions.

5. REFERENCES
Atkeson, C. G., Moore, A. W. , & Schaal, S.
(submitted). ""Locally weighted learning."" Artificial Intelligence Review.
Atkeson, C. G. (1992). ""Memory-based approaches to
approximating continuous functions."" In: Casdagli, M.,
& Eubank, S. (Eds.), Nonlinear Modeling and Forecasting, pp.503-521. Addison Wesley.
Belsley, D. A., Kuh, E., & Welsch, R. E. (1980). Regression diagnostics: Identifying influential data and
sources ofcollinearity. New York: Wiley.
Cleveland, W. S. (1979). ""Robust locally weighted regression and smoothing scatterplots."" J. American Stat.
Association, 74, pp.829-836.
de Boor, C. (1978). A practical guide to splines. New
York: Springer.
Hastie, T. J., & Tibshirani, R. J. (1990). Generalized
additive models. London: Chapman and Hall.
Jacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton,
G. E. (1991). ""Adaptive mixtures of local experts.""
Neural Computation, 3, pp.79-87.

Jordan, M. I., & Jacobs, R. (1994). ""Hierarchical mixtures of experts and the EM algorithm."" Neural Computation, 6, pp.79-87.
Ljung, L., & S_derstr_m, T. (1986). Theory and practice of recursive identification. Cambridge, MIT Press.
McLachlan, G. J., & Basford, K. E. (1988). Mixture
models . New York: Marcel Dekker.
Nadaraya, E. A. (1964). ""On estimating regression .""
Theor. Prob. Appl., 9, pp.141-142.
Schaal, S., & Atkeson, C. G. (l994b). ""Assessing the
quality of learned local models."" In: Cowan, J. ,Tesauro, G., & Alspector, J. (Eds.), Advances in Neural
Information Processing Systems 6. Morgan Kaufmann.
Scott, D. W. (1992). Multivariate Density Estimation.
New York: Wiley.
Sutton, R. S. (1992). ""Gain adaptation beats least
squares."" In: Proc. of 7th Yale Workshop on Adaptive
and Learning Systems, New Haven, CT.
Wolpert, D. H. (1990). ""Stacked genealization."" Los
Alamos Technical Report LA-UR-90-3460.

Boosting Decision Trees

Harris Drucker
AT&T Bell Laboratories
Holmdel, New Jersey 07733

Corinna Cortes
AT&T Bell Laboratories
Murray Hill, New Jersey 07974

Abstract
A new boosting algorithm of Freund and Schapire is used to improve
the performance of decision trees which are constructed using the
information ratio criterion of Quinlan's C4.5 algorithm. This boosting
algorithm iteratively constructs a series of decision trees, each decision
tree being trained and pruned on examples that have been filLered by
previously trained trees. Examples that have been incorrectly classified
by the previous trees in the ensemble are resampled with higher
probability to give a new probability distribution for the next tree in the
ensemble to train on. Results from optical character recognition
(OCR), and knowledge discovery and data mining problems show that
in comparison to single trees, or to trees trained independently, or to
trees trained on subsets of the feature space, the boosting ensemble is
much better.
1 INTRODUCTION

A new boosting algorithm termed AdaBoost by their inventors (Freund and Schapire,
1995) has advantages over the original boosting algorithm (Schapire, 1990) and a second
version (Freund, 1990). The implications of a boosting algorithm is that one can take a
series of learning machines (termed weak learners) each having a poor error rate (but no
worse than .5-y, where y is some small positive number) and combine them to give an
ensemble that has very good performance (termed a strong learner). The first practical
implementation of boosting was in OCR (Drucker, 1993, 1994) using neural networks as
the weak learners. In a series of comparisons (Bottou, 1994) boosting was shown to be
superior to other techniques on a large OCR problem.
The general configuration of AdaBoost is shown in Figure 1. Each box is a decision tree
built using Quinlans C4.5 algorithm (Quinlan, 1993) The key idea is that each weak
learner is trained sequentially. The first weak learner is trained on a set of patterns picked
randomly (with replacement) from a training set. After training and pruning, the training
patterns are passed through this first decision tree. In the two class case the hypothesis hi
is either class 0 or class 1. Some of the patterns will be in error. The training set for the

480

H. DRUCKER. C. CORTES

INPUT FEATURES

#1

#2

h1

~l

T

#3

h
~2

2

h
~3

hT

~T

3

~)t log (11 ~t )

FIGURE 1. BOOSTING ENSEMBLE

.5
WEAK LEARNER WEIGHTED
TRAINING ERROR RATE

w

~
a:
a:
oa:
a:

ENSEMBLE TEST ERROR RATE

w

ENSEMBLE
TRAINING
ERROR RATE
NUMBER OF WEAK LEARNERS

FIGURE 2. INDIVIDUAL WEAK LEARNER ERROR RATE
AND ENSEMBLE TRAINING AND TEST ERROR RATES

Boosting Decision Trees

481

second weak learner will consist of patterns picked from the training set with higher
probability assigned to those patterns the first weak learner classifies incorrectly. Since
patterns are picked with replacement, difficult patterns are more likely to occur multiple
times in the training set. Thus as we proceed to build each member of the ensemble,
patterns which are more difficult to classify correctly appear more and more likely. The
training error rate of an individual weak learner tends to grow as we increase the number
of weak learners because each weak learner is asked to classify progressively more
difficult patterns. However the boosting algorithm shows us that the ensemble training
and test error rate decrease as we increase the number of weak learners. The ensemble
output is determined by weighting the hypotheses with the log of (l!~i) where ~ is
proportional to the weak learner error rate. If the weak learner has good error rate
performance, it will contribute significantly to the output, because then 1/ ~ will be large.
Figure 2 shows the general shape of the curves we would expect. Say we have
constructed N weak learners where N is a large number (right hand side of the graph).
The N'th weak learner (top curve) will have a training error rate that approaches .5
because it is trained on difficult patterns and can do only sightly better than guessing.
The bottom two curves show the test and training error rates of the ensemble using all N
weak learners. which decrease as weak learners are added to the ensemble.
2 BOOSTING
Boosting arises from the PAC (probably approximately correct) learning model which
has as one of its primary interests the efficiency of learning. Schapire was the first one to
show that a series of weak learners could be converted to a strong learner. The detailed
algorithm is show in Figure 3. Let us call the set of N 1 distinct examples the original
training set. We distinguish the original training set from what we will call the filtered
training set which consists of N 1 examples picked with replacement from the original
training set. Basically each of N 1 original examples is assigned a weight which is
proportional to the probability that the example will appear in the filtered training set
(these weights have nothing to do with the weights usually associated with neural
networks). Initially all examples are assigned a weight of unity so that all the examples
are equally likely to show up in the initial set of training examples. However, the weights
are altered at each state of boosting (Step 5 of Figure 3) and if the weights are high we
may have multiple copies of some of the original examples appearing in the filtered
training set. In step three of this algorithm, we calculate what is called the weighted
training error and this is the error rate over all the original N 1 training examples
weighted by their current respective probabilities. The algorithms terminates if this error
rate is .5 (no better than guessing) or zero (then the weights of step 5 do not change).
Although not called for in the original C4.5 algorithm, we also have an original set of
pruning examples which also are assigned weights to form a filtered pruning set and used
to prune the classification trees constructed using the filtered training set. It is known
(Mingers, 1989a) that reducing the size of the tree (pruning) improves generalization.
3 DECISION TREES
For our implementation of decision trees, we have a set of features (attributes) that
specifies an example along with their classification (we discuss the two-class problem
primarily). We pick a feature that based on some criterion, best splits the examples into
two subsets. Each of these two subsets will usually not contain examples of just one
class, so we recursively divide the subsets until the final subsets each contain examples of
just one class. Thus, each internal node specifies a feature and a value for that feature that
determines whether one should take the left or right branch emanating from that node. At
terminal nodes, we make the final decision, class 0 or 1. Thus, in decision trees one
starts at a root node and progressively traverses the tree from the root node to one of the

H. DRUCKER,C. CORTES

482

Inputs: N I training paUans. N 2 pruning paUems. N 3 test paUans

laitialize the weight veco of the N I training pattems: wI = 1 for i = 1"